Atributos de generator_llm:
{'cache': None,
 'is_finished_parser': None,
 'langchain_llm': OllamaLLM(model='mistral-f16'),
 'multiple_completion_supported': False,
 'run_config': RunConfig(timeout=180,
                         max_retries=10,
                         max_wait=60,
                         max_workers=16,
                         exception_types=(<class 'Exception'>,),
                         log_tenacity=False,
                         seed=42)}

Atributos de generator_llm.langchain_llm:
{'base_url': None,
 'cache': None,
 'callback_manager': None,
 'callbacks': None,
 'client_kwargs': {},
 'custom_get_token_ids': None,
 'format': '',
 'keep_alive': None,
 'metadata': None,
 'mirostat': None,
 'mirostat_eta': None,
 'mirostat_tau': None,
 'model': 'mistral-f16',
 'name': None,
 'num_ctx': None,
 'num_gpu': None,
 'num_predict': None,
 'num_thread': None,
 'repeat_last_n': None,
 'repeat_penalty': None,
 'stop': None,
 'tags': None,
 'temperature': None,
 'tfs_z': None,
 'top_k': None,
 'top_p': None,
 'verbose': False}
