{
  "nodes": [
    {
      "id": "5c37a01e-dcde-4fe2-bb58-361048164dd8",
      "properties": {
        "page_content": "3 2 0 2\n\np e S 6 2\n\n] L C . s c [\n\n1 v 7 1 2 5 1 . 9 0 3 2 : v i X r a\n\nRAGAS: Automated Evaluation of Retrieval Augmented Generation\n\nShahul Es†, Jithin James†, Luis Espinosa-Anke∗♢, Steven Schockaert∗ †Exploding Gradients ∗CardiffNLP, Cardiff University, United Kingdom ♢AMPLYFI, United Kingdom shahules786@gmail.com,jamesjithin97@gmail.com {espinosa-ankel,schockaerts1}@cardiff.ac.uk\n\nAbstract\n\nWe introduce RAGAS (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Aug- mented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the gener- ation itself. With RAGAS, we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations. We posit that such a framework can crucially con- tribute to faster evaluation cycles of RAG archi- tectures, which is especially important given the fast adoption of LLMs.\n\n1\n\nIntroduction\n\nLanguage Models (LMs) capture a vast amount of knowledge about the world, which allows them to answer questions without accessing any exter- nal sources. This idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT (Devlin et al., 2019) and became more firmly established with the introduction of ever larger LMs (Roberts et al., 2020). While the most recent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering bench- marks (Bubeck et al., 2023), the idea of using LLMs as knowledge bases still has two fundamen- tal limitations. First, LLMs are not able to answer questions about events that have happened after they were trained. Second, even the largest models\n\nstruggle to memorise knowledge that is only rarely mentioned in the training corpus (Kandpal et al., 2022; Mallen et al., 2023). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) (Lee et al., 2019; Lewis et al., 2020; Guu et al., 2020). Answering a question then essentially involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM. While initial approaches relied on specialised LMs for retrieval-augmented language modelling (Khandel- wal et al., 2020; Borgeaud et al., 2022), recent work has suggested that simply adding retrieved docu- ments to the input of a standard LM can also work well (Khattab et al., 2022; Ram et al., 2023; Shi et al., 2023), thus making it possible to use retrieval- augmented strategies in combination with LLMs that are only available through APIs.\n\nWhile the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. Automated evaluation of retrieval-augmented systems is thus paramount. In practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e. by mea- suring perplexity on some reference corpus. How- ever, such evaluations are not always predictive of downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is another common evaluation task, but usually only datasets with short extractive an- swers are considered, which may not be represen- tative of how the system will be used.\n\nTo address these issues, in this paper we present RAGAS1, a framework for the automated assess-\n\n1RAGAS\n\navailable explodinggradients/ragas.\n\nis\n\nat\n\nhttps://github.com/\n\nment of retrieval augmented generation systems. We focus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the usefulness of the retrieved passages. The RAGAS framework provides an integration with both llama- index and Langchain, the most widely used frame- works for building RAG solutions, thus enabling developers to easily integrate RAGAS into their standard workflow.\n\n2 Related Work\n\nEstimating faithfulness using LLMs The prob- lem of detecting hallucinations in LLM generated responses has been extensively studied (Ji et al., 2023). Several authors have suggested the idea of predicting factuality using a few-shot prompt- ing strategy (Zhang et al., 2023). Recent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies (Li et al., 2023; Azaria and Mitchell, 2023). Other approaches rely on linking the generated responses to facts from an external knowledge base (Min et al., 2023), but this is not always possible.\n\nYet another strategy is to inspect the probabili- ties assigned to individual tokens, where we would expect the model to be less confident in halluci- nated answers than in factual ones. For instance, BARTScore (Yuan et al., 2021) estimates factuality by looking at the conditional probability of the gen- erated text given the input. Kadavath et al. (2022) use a variation of this idea. Starting from the ob- servation that LLMs provide well-calibrated proba- bilities when answering multiple-choice questions, they essentially convert the problem of validating model generated answers into a multiple-choice question which asks whether the answer is true or false. Rather than looking at the output probabil- ities, Azaria and Mitchell (2023) propose to train a supervised classifier on the weights from one of the hidden layers of the LLM, to predict whether a given statement is true or not. While the approach performs well, the need to access the hidden states of the model makes it unsuitable for systems that access LLMs through an API.\n\nFor models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed. SelfCheckGPT (Manakul et al., 2023) addresses this problem by instead sam- pling multiple answers. Their core idea is that\n\nfactual answers are more stable: when an answer is factual, we can expect that different samples will tend to be semantically similar, whereas this is less likely to be the case for hallucinated answers.\n\nAutomated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, GPTScore (Fu et al., 2023) uses a prompt that specifies the consid- ered aspect (e.g. fluency) and then scores passages based on the average probability of the generated tokens, according to a given autoregressive LM. This idea of using prompts was previously also considered by Yuan et al. (2021), although they used a smaller fine-tuned LM (i.e. BART) and did not observe a clear benefit from using prompts. An- other approach directly asks ChatGPT to evaluate a particular aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale (Wang et al., 2023a). Re- markably, strong results can be obtained in this way, although it comes with the limitation of being sensitive to the design of the prompt. Rather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates (Wang et al., 2023b), typically to compare the performance of different LLMs. However, care is needed with this approach, as the order in which the answers is presented can influence the result (Wang et al., 2023b).\n\nIn terms of how ground truth answers or, more generally, generations, have been typically used in the literature, most approaches have relied on the availability of one or more reference answers. For instance, BERTScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019) use contex- tualised embeddings, produced by a pre-trained BERT model, to compare the similarity between the generated answer and the reference answers. BARTScore (Yuan et al., 2021) similarly uses refer- ence answers to compute aspects such as precision (estimated as the probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer).\n\n3 Evaluation Strategies\n\nWe consider a standard RAG setting, where given a question q, the system first retrieves some context c(q) and then uses the retrieved context to generate an answer as(q). When building a RAG system,\n\nwe usually do not have access to human-annotated datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance. First, Faithfulness refers to the idea that the an- swer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer. Indeed, RAG systems are often used in applications where the factual con- sistency of the generated text w.r.t. the grounded sources is highly important, e.g. in domains such as law, where information is constantly evolving. Sec- ond, Answer Relevance refers to the idea that the generated answer should address the actual ques- tion that was provided. Finally, Context Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context passages are too long, LLMs are often less effective in exploiting that context, especially for information that is pro- vided in the middle of the context passage (Liu et al., 2023).\n\nWe now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2.\n\nFaithfulness We say that the answer as(q) is faithful to the context c(q) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S(as(q)). The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step3:\n\nGiven a question and answer, create one or more statements from each sentence in the given answer. question: [question] answer: [answer]\n\nwhere [question] and [answer] refer to the given question and answer. For each statement si\n\n2https://platform.openai.com 3To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper.\n\nin S, the LLM determines if si can be inferred from c(q) using a verification function v(si, c(q)). This verification step is carried out using the following prompt:\n\nConsider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format. statement: [statement 1] ... statement: [statement n]\n\nThe final faithfulness score, F , is then computed as F = |V | |S| , where |V | is the number of statements that were supported according to the LLM and |S| is the total number of statements.\n\nAnswer relevance We say that the answer as(q) is relevant if it directly addresses the question in an appropriate way. In particular, our assessment of answer relevance does not take into account fac- tuality, but penalises cases where the answer is incomplete or where it contains redundant informa- tion. To estimate answer relevance, for the given answer as(q), we prompt the LLM to generate n potential questions qi based on as(q), as follows:\n\nGenerate a question for the given answer. answer: [answer]\n\nWe then obtain embeddings for all questions us- ing the text-embedding-ada-002 model, avail- able from the OpenAI API. For each qi, we cal- culate the similarity sim(q, qi) with the original question q, as the cosine between the correspond- ing embeddings. The answer relevance score, AR, for question q is then computed as:\n\nAR =\n\n1 n\n\nn (cid:88)\n\ni=1\n\nsim(q, qi)\n\nThis metric evaluates how closely the generated answer aligns with the initial question or instruc- tion.\n\nContext relevance The context c(q) is consid- ered relevant to the extent that it exclusively con- tains information that is needed to answer the ques- tion. In particular, this metric aims to penalise the\n\n(1)\n\ninclusion of redundant information. To estimate context relevance, given a question q and its con- text c(q), the LLM extracts a subset of sentences, Sext, from c(q) that are crucial to answer q, using the following prompt:\n\nPlease extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\". While extract- ing candidate sentences you’re not al- lowed to make any changes to sentences from given context.\n\nThe context relevance score is then computed as:\n\nCR =\n\nnumber of extracted sentences total number of sentences in c(q)\n\n4 The WikiEval Dataset\n\nTo evaluate the proposed framework, we ideally need examples of question-context-answer triples which are annotated with human judgments. We can then verify to what extent our metrics agree with human assessments of faithfulness, answer relevance and context relevance. Since we are not aware of any publicly available datasets that could be used for this purpose, we created a new dataset, which we refer to as WikiEval4. To construct the dataset, we first selected 50 Wikipedia pages cov- ering events that have happened since the start of 20225. In selecting these pages, we prioritised those with recent edits. For each of the 50 pages, we then asked ChatGPT to suggest a question that can be answered based on the introductory section of the page, using the following prompt:\n\nYour task is to formulate a question from given context satisfying the rules given below: 1. The question should be fully answered from the given context. 2. The question should be framed from a part that contains non-trivial informa- tion. 3. The answer should not contain any\n\n4https://huggingface.co/datasets/\n\nexplodinggradients/WikiEval\n\n5That is, beyond the reported training cutoff of the model\n\nwe used in our experiments.\n\n(2)\n\nlinks. 4. The question should be of moderate difficulty. 5. The question must be reasonable and must be understood and responded to by humans. 6. Do not use phrases that ’provided con- text’, etc in the question context:\n\nWe also used ChatGPT to answer the generated question, when given the corresponding introduc- tory section as context, using the following prompt:\n\nAnswer the question using the informa- tion from the given context. question: [question] context: [context]\n\nAll questions were annotated along the three con- sidered quality dimensions by two annotators. Both annotators were fluent in English and were given clear instructions about the meaning of the three considered quality dimensions. For faithfulness and context relevance, the two annotators agreed in around 95% of cases. For answer relevance, they agreed in around 90% of the cases. Disagreements were resolved after a discussion between the anno- tators.\n\nFaithfulness To obtain human judgements about faithfulness, we first used ChatGPT to answer the question without access to any additional context. We then asked the annotators to judge which of the two answers was the most faithful (i.e. the standard one or the one generated without context), given the question and corresponding Wikipedia page.\n\nAnswer relevance We first used ChatGPT to obtain candidate answers with lower answer rel- evance, using the following prompt:\n\nAnswer the given question in an incom- plete manner. question: [question]\n\nWe then asked human annotators to compare this answer, and indicate which of the two answers had the highest answer relevance.\n\nContext relevance To measure this aspect, we first added additional sentences to the context by scraping back-links to the corresponding Wikipedia page. In this way, we were able to add information to the context that was related but less relevant for\n\nFaith. Ans. Rel. Cont. Rel.\n\nRAGAs GPT Score GPT Ranking\n\n0.95 0.72 0.54\n\n0.78 0.52 0.40\n\n0.70 0.63 0.52\n\nTable 1: Agreement with human annotators in pairwise comparisons of faithfulness, answer relevance and con- text relevance, using the WikEval dataset (accuracy).\n\nanswering the question. For the few pages with- out any back-links, we instead used ChatGPT to complete the given context.\n\n5 Experiments\n\nTable 1 analyses the agreement between the met- rics proposed in Section 3 and the human assess- ments from the proposed WikiEval dataset. Each WikiEval instance requires the model to compare two answers or two context fragments. We count how often the answer/context preferred by the model (i.e. with highest estimated faithfulness, an- swer relevance, or context relevance) coincides with the answer/context preferred by the human annotators. We report the results in terms of ac- curacy (i.e. the fraction of instances on which the model agrees with the annotators).\n\nTo put the results in context, we compare our proposed metrics (shown as RAGAs in Table 1) with two baseline methods. For the first method, shown as GPT Score, we ask ChatGPT to assign a score between 0 and 10 for the three quality dimensions. To this end, we use a prompt that describes the meaning of the quality metric and then asks to score the given answer/context in line with that definition. For instance, for evaluating faithfulness, we used the following prompt:\n\nFaithfulness measures the information consistency of the answer against the given context. Any claims that are made in the answer that cannot be deduced from context should be penalized. Given an answer and context, assign a score for faithfulness in the range 0-10. context: [context] answer: [answer]\n\nTies, where the same score is assigned by the LLM to both answer candidates, were broken randomly. The second baseline, shown as GPT Ranking, in- stead asks ChatGPT to select the preferred answer/-\n\ncontext. In this case, the prompt again includes a definition of the considered quality metric. For instance, for evaluating answer relevance, we used the following prompt:\n\nAnswer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question. It penalizes the present of redundant in- formation or incomplete answers given a question. Given an question and answer, rank each answer based on Answer Rele- vancy. question: [question] answer 1: [answer 1] answer 2: [answer 2]\n\nThe results in Table 1 show that our proposed metrics are much closer aligned with the human judgements than the predictions from the two base- lines. For faithfulness, the RAGAs prediction are in general highly accurate. For answer relevance, the agreement is lower, but this is largely due to the fact that the differences between the two candidate answers are often very subtle. We found context relevance to be the hardest quality dimension to evaluate. In particular, we observed that ChatGPT often struggles with the task of selecting the sen- tences from the context that are crucial, especially for longer contexts.\n\n6 Conclusions\n\nWe have highlighted the need for automated reference-free evaluation of RAG systems. In par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. is the answer grounded in the retrieved context), answer relevance (i.e. does the answer address the ques- tion) and context relevance (i.e. is the retrieved context sufficiently focused). To support the devel- opment of such a framework, we have introduced WikiEval, a dataset which human judgements of these three different aspects. Finally, we have also described RAGAs, our implementation of the three considered quality aspects. This framework is easy to use and can provide deverlopers of RAG sys- tems with valuable insights, even in the absence of any ground truth. Our evaluation on WikiEval has shown that the predictions from RAGAs are closely aligned with human predictions, especially for faithfulness and answer relevance.\n\nReferences\n\nAmos Azaria and Tom M. Mitchell. 2023. The inter- nal state of an LLM knows when its lying. CoRR, abs/2304.13734.\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Si- monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal- timore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 2206–2240. PMLR.\n\nSébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al. 2023. Sparks of artificial general intelli- gence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. CoRR, abs/2302.04166.\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International confer- ence on machine learning, pages 3929–3938. PMLR.\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of halluci- nation in natural language generation. ACM Comput- ing Surveys, 55(12):1–38.\n\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jack- son Kernion, Shauna Kravec, Liane Lovitt, Ka- mal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared\n\nKaplan. 2022. Language models (mostly) know what they know. CoRR, abs/2207.05221.\n\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022. Large language models struggle to learn long-tail knowledge. CoRR, abs/2211.08411.\n\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\n\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP. CoRR, abs/2212.14024.\n\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open do- main question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096.\n\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik- tus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neu- ral Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\n\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: A large- scale hallucination evaluation benchmark for large language models. CoRR, abs/2305.11747.\n\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts.\n\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 9802–9822, Toronto, Canada. Association for Computational Linguistics.\n\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. Selfcheckgpt: Zero-resource black-box hal- lucination detection for generative large language models. CoRR, abs/2303.08896.\n\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of fac- tual precision in long form text generation. CoRR, abs/2305.14251.\n\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented lan- guage models. CoRR, abs/2302.00083.\n\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the param- eters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418–5426, Online. Association for Computational Linguistics.\n\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: retrieval-augmented black-box language models. CoRR, abs/2301.12652.\n\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxi- ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good NLG evaluator? A preliminary study. CoRR, abs/2303.04048.\n\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. CoRR, abs/2305.17926.\n\nShufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, and Mohit Iyyer. 2023c. KNN-LM does not improve open-ended text generation. CoRR, abs/2305.14625.\n\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text genera- tion. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Informa- tion Processing Systems 2021, NeurIPS 2021, De- cember 6-14, 2021, virtual, pages 27263–27277.\n\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James R. Glass. 2023. Interpretable unified language checking. CoRR, abs/2304.03728.\n\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu- ating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe- view.net.\n\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized em- beddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 563–578, Hong Kong, China. Association for Computational Lin- guistics.\n\nA Examples from WikiEval\n\nTables 2, 3 and 4 show examples from the WikiEval dataset, focusing in particular on answers with high and low faithfulness (Table 2), high and low answer relevance (Table 3), and high and low context rele- vance (Table 4).\n\nQuestion\n\nContext\n\nAnswer\n\nWho directed the film Op- penheimer and who stars as J. Robert Oppenheimer in the film?\n\nOppenheimer is a 2023 biographical thriller film written and directed by Christopher Nolan. Based on the 2005 biography American Prometheus by Kai Bird and Mar- tin J. Sherwin, the film chronicles the life of J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Man- hattan Project, and thereby ushering in the Atomic Age. Cillian Murphy stars as Oppenheimer, with Emily Blunt as Oppenheimer’s wife Katherine \"Kitty\" Oppenheimer.\n\nHigh Faithfulness: Christopher Nolan directed the film Oppen- heimer. Cillian Murphy stars as J. Robert Oppenheimer in the film. Low Faithfulness: James Cameron directed the film Op- penheimer. Tom Cruise stars as J. Robert Oppenheimer in the film.\n\nTable 2: Example from WikiEval, showing answers with high and low faithfulness.\n\nQuestion\n\nAnswer\n\nWhen is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from?\n\nHigh answer relevance: The PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India. Low answer relevance: The scheduled launch date and time for the PSLV-C56 mission have not been provided.The PSLV-C56 mission is an important space mission for India. It aims to launch a satellite into orbit to study weather patterns.\n\nTable 3: Example from WikiEval, showing answers with high and low answer relevance.\n\nQuestion\n\nContext\n\nWhen was the Chimnabai Clock Tower completed, and who was it named af- ter?\n\nHigh context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. Low context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was built in Indo-Saracenic architecture style. History. Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was inaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda. During the rule of Gaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost of 25,000 (equivalent to 9.2 million or USD 120,000 in 2023).\n\nTable 4: Example from WikiEval, showing answers with high and low context relevance.",
        "document_metadata": {
          "source": "/home/jovyan/Documentos/Docs_pdf/2309.15217v1.pdf"
        },
        "headlines": [
          "RAGAS: Automated Evaluation of Retrieval Augmented Generation",
          "Introduction",
          "Retrieval Augmented Generation",
          "Evaluation of RAG systems",
          "WikiEval dataset"
        ],
        "summary": "RAGAS is a framework for evaluating Retrieval Augmented Generation (RAG) pipelines without relying on ground truth human annotations. It includes a suite of metrics to assess the ability of the retrieval system to identify relevant context passages, the ability of the LLM to exploit such passages, and the quality of the generation itself.",
        "summary_embedding": [
          -0.09182721376419067,
          0.07585930079221725,
          -0.012698770500719547,
          -0.02287135273218155,
          -0.08832800388336182,
          -0.0006863374728709459,
          0.012395469471812248,
          -0.012327146716415882,
          0.014847601763904095,
          -0.05954877659678459,
          -0.02999308705329895,
          -0.03516848385334015,
          0.048332881182432175,
          -0.03596842661499977,
          -0.03776119276881218,
          0.04991034418344498,
          0.07231356203556061,
          0.11308565735816956,
          -0.023372089490294456,
          -0.05398814380168915,
          -0.017493952065706253,
          0.08309292048215866,
          0.024698814377188683,
          -0.04191970080137253,
          -0.02248639427125454,
          0.01715771108865738,
          -0.03668687492609024,
          -0.044597651809453964,
          0.10064754635095596,
          -0.017066342756152153,
          0.020516935735940933,
          0.10991039127111435,
          -0.04520785063505173,
          -0.006605778355151415,
          -0.03161614015698433,
          0.07966680079698563,
          -0.018225401639938354,
          0.06647004932165146,
          0.00344798993319273,
          -0.00398622639477253,
          0.00046933870180509984,
          0.0015380701515823603,
          -0.005859914235770702,
          -0.06179643049836159,
          0.0238938108086586,
          -0.0707293376326561,
          -0.09468309581279755,
          -0.04528813064098358,
          -0.02253568544983864,
          0.056242119520902634,
          -0.07037176191806793,
          -0.014958950690925121,
          0.023358957841992378,
          0.003922733943909407,
          -0.05332221835851669,
          0.024096934124827385,
          0.06921190023422241,
          -0.05099332332611084,
          -0.02717679738998413,
          -0.03521532565355301,
          -0.06541692465543747,
          -0.11295957863330841,
          -0.041378553956747055,
          0.03569768741726875,
          0.12449156492948532,
          -0.05479834973812103,
          0.06204353645443916,
          0.024257145822048187,
          -0.0040528178215026855,
          -0.013785995543003082,
          -0.10105057060718536,
          -0.019715016707777977,
          -0.04135727137327194,
          0.11763644218444824,
          0.04100080579519272,
          0.09640862047672272,
          0.025001225993037224,
          -0.04239334538578987,
          0.03150318190455437,
          -0.038515038788318634,
          -0.04066438600420952,
          -0.009298508055508137,
          0.061645008623600006,
          0.07849619537591934,
          0.012997657991945744,
          0.08406037092208862,
          0.028329800814390182,
          -0.04465488716959953,
          0.003362832823768258,
          0.010995698161423206,
          -0.03459905833005905,
          -0.07219356298446655,
          0.055473748594522476,
          -0.04690999165177345,
          0.01875632256269455,
          0.011188522912561893,
          0.04126778244972229,
          0.02953784354031086,
          0.027161234989762306,
          0.03666657209396362,
          0.03634405881166458,
          0.03411707282066345,
          0.04806996136903763,
          -0.03680456429719925,
          -0.04679659381508827,
          -0.06966818869113922,
          -0.017452316358685493,
          -0.05118197202682495,
          -0.0010306742042303085,
          -0.056322354823350906,
          0.010869201272726059,
          -0.03411450982093811,
          0.03638483211398125,
          -0.09541486948728561,
          -0.01761174201965332,
          -0.02246686816215515,
          0.011073549278080463,
          -0.06468512862920761,
          0.012594645842909813,
          -0.008154971525073051,
          -0.018125709146261215,
          0.018443530425429344,
          0.007708252873271704,
          0.0023700122255831957,
          0.007255016826093197,
          -0.005529528018087149,
          0.04243375360965729,
          -2.8035624755768022e-33,
          -0.012308675795793533,
          -0.02082747034728527,
          0.008945010602474213,
          0.04489029571413994,
          -0.03063580021262169,
          -0.013700665906071663,
          -0.015849143266677856,
          -0.04770040139555931,
          0.020882831886410713,
          -0.004487884696573019,
          -0.026310760527849197,
          0.10853975266218185,
          -0.0319790244102478,
          0.041101399809122086,
          0.009870096109807491,
          0.056076060980558395,
          -0.030308198183774948,
          0.035329848527908325,
          -0.004255461506545544,
          0.060766417533159256,
          -0.009400049224495888,
          0.03147030994296074,
          0.05130671709775925,
          -0.07994183152914047,
          -0.015448764897882938,
          0.04245992749929428,
          0.09336822479963303,
          0.009905127808451653,
          -0.08041062206029892,
          0.013519969768822193,
          -0.04482702910900116,
          -0.009577670134603977,
          0.01094131637364626,
          -0.06862423568964005,
          0.0145831648260355,
          -0.0023657793644815683,
          -0.045065946877002716,
          -0.005044716410338879,
          -0.040034692734479904,
          0.02552844025194645,
          -0.00267878919839859,
          0.006288446951657534,
          0.005671726074069738,
          -0.07307949662208557,
          -0.007604820653796196,
          -0.03994471952319145,
          0.018324600532650948,
          0.025576705113053322,
          -0.0228035356849432,
          0.10762440413236618,
          0.07667262107133865,
          0.07812459766864777,
          -0.012317651882767677,
          -0.005476264748722315,
          0.016608543694019318,
          0.038721632212400436,
          0.011962288059294224,
          -0.025912461802363396,
          0.02357586845755577,
          0.08969252556562424,
          -0.006298148538917303,
          0.05857974663376808,
          0.04806641861796379,
          0.0649680644273758,
          0.040926698595285416,
          0.014472085982561111,
          0.041487351059913635,
          0.012200465425848961,
          0.10371210426092148,
          0.027594804763793945,
          -0.0054406519047915936,
          -0.06109488382935524,
          -0.005742797162383795,
          0.05482916533946991,
          0.04695054888725281,
          -0.0779079794883728,
          0.01970614306628704,
          0.01617780327796936,
          0.039914824068546295,
          -0.046218253672122955,
          -0.006972487084567547,
          -0.004075323697179556,
          -0.0031734006479382515,
          -0.04058277979493141,
          -0.020988821983337402,
          -0.014870738610625267,
          -0.017574653029441833,
          -0.14000657200813293,
          -0.0985635444521904,
          -0.06707783043384552,
          0.009170887060463428,
          0.04841332882642746,
          -0.09207224100828171,
          0.058443330228328705,
          -0.0016453879652544856,
          8.98386526801807e-34,
          0.09711355715990067,
          -0.003216309705749154,
          -0.013267593458294868,
          0.05092683061957359,
          0.046054255217313766,
          -0.014495044946670532,
          0.02579757757484913,
          0.0040372940711677074,
          -0.061939094215631485,
          0.04664069041609764,
          -0.008839556947350502,
          0.024093393236398697,
          -0.018294434994459152,
          0.008636592887341976,
          0.039124876260757446,
          0.0039756931364536285,
          -0.008422376587986946,
          -0.0572267509996891,
          -0.03294166177511215,
          0.07483220845460892,
          0.06484782695770264,
          0.03848538547754288,
          -0.004406907130032778,
          0.026996387168765068,
          -0.04277186468243599,
          -0.0387103371322155,
          0.04654964059591293,
          -0.07978431135416031,
          -0.01521009299904108,
          -0.018310628831386566,
          0.039220064878463745,
          -0.037339892238378525,
          0.006654961500316858,
          -0.0185858104377985,
          -0.012471710331737995,
          -0.06439503282308578,
          0.14253002405166626,
          0.060678839683532715,
          -0.06400080770254135,
          0.00812798086553812,
          -0.0402168408036232,
          0.04056099057197571,
          -0.07425284385681152,
          0.015073803253471851,
          -0.07637261599302292,
          0.012902582995593548,
          -0.07959973812103271,
          0.022620396688580513,
          -0.04345789551734924,
          -0.0010233104694634676,
          0.027737634256482124,
          -0.0053148516453802586,
          0.04436291381716728,
          -0.06133155897259712,
          -0.0192259531468153,
          -0.052890852093696594,
          -0.0340951569378376,
          -0.09597364813089371,
          -0.013966744765639305,
          0.1008593887090683,
          0.0158910620957613,
          0.025158673524856567,
          -0.039774369448423386,
          0.003865196369588375,
          0.03785248473286629,
          0.003548175096511841,
          -0.03390787914395332,
          -0.07739824056625366,
          -0.20415066182613373,
          0.032641321420669556,
          0.06320765614509583,
          -0.0251285620033741,
          -0.006573773920536041,
          -0.046880315989255905,
          0.07644456624984741,
          0.029896197840571404,
          0.028093187138438225,
          -0.06239128112792969,
          0.043595608323812485,
          -0.09604670852422714,
          -0.09668564051389694,
          -0.08505107462406158,
          0.043222203850746155,
          0.10087134689092636,
          -0.013221796602010727,
          0.037731584161520004,
          -0.08658407628536224,
          0.016492825001478195,
          0.02859049290418625,
          0.022701535373926163,
          0.029953861609101295,
          0.029995089396834373,
          -0.10292235761880875,
          -0.0017858721548691392,
          -0.001921394607052207,
          -3.772096235366007e-08,
          -0.028398165479302406,
          -0.010922960005700588,
          -0.061963532119989395,
          0.05893734097480774,
          -0.06356416642665863,
          0.01966315135359764,
          -0.11646797508001328,
          0.07439104467630386,
          -0.0212003942579031,
          0.045848965644836426,
          0.03938179090619087,
          0.0007958450587466359,
          -0.10585767030715942,
          0.06928016245365143,
          0.03485334292054176,
          -0.0028439664747565985,
          0.024444643408060074,
          0.06180955097079277,
          -0.08221682161092758,
          -0.01773037575185299,
          0.06755876541137695,
          0.1083613932132721,
          0.0614657923579216,
          0.011184199713170528,
          0.01842547580599785,
          -0.023975666612386703,
          -0.002525704214349389,
          0.09979809820652008,
          0.04807388782501221,
          -0.057340387254953384,
          0.06725266575813293,
          0.014926965348422527,
          -0.013189692981541157,
          -0.07499413192272186,
          0.011178075335919857,
          0.04447449371218681,
          -0.014221192337572575,
          -0.03269483521580696,
          0.028080448508262634,
          0.04006890207529068,
          0.034864719957113266,
          0.08060333132743835,
          0.00043973271385766566,
          -0.0095598716288805,
          0.017746577039361,
          0.013761775568127632,
          -0.012853915803134441,
          -0.02932981587946415,
          -0.06317956000566483,
          -0.03796634450554848,
          -0.04051875323057175,
          -0.11336387693881989,
          0.04884979501366615,
          0.03716668486595154,
          0.035877685993909836,
          -0.016825640574097633,
          0.08595025539398193,
          -0.0612851157784462,
          0.043107107281684875,
          0.08298581093549728,
          0.163065105676651,
          0.01398112066090107,
          0.0009512027609162033,
          0.0010695825330913067
        ]
      },
      "type": "document"
    },
    {
      "id": "c68d4108-fb9a-4ae3-8b76-ad68508660e1",
      "properties": {
        "page_content": "4 2 0 2\n\nl u J\n\n0 1\n\n] S A . s s e e [\n\n1 v 6 1 4 7 1 . 7 0 4 2 : v i X r a\n\nExplaining Spectrograms in Machine Learning: A Study on Neural Networks for Speech Classification\n\nJesin James1, Balamurali B. T.2, Binu Abeysinghe1, and Junchen Liu1\n\n1 Department of Electrical, Computer, and Software Engineering, The University of Auckland, New Zealand jesin.james@auckland.ac.nz, 2 Singapore University of Technology and Design, Singapore\n\nAbstract. This study investigates discriminative patterns learned by neural networks for accurate speech classification, with a specific focus on vowel classification tasks. By examining the activations and features of neural networks for vowel classification, we gain insights into what the networks “see” in spectrograms. Through the use of class activation map- ping, we identify the frequencies that contribute to vowel classification and compare these findings with linguistic knowledge. Experiments on a American English dataset of vowels showcases the explainability of neural networks and provides valuable insights into the causes of misclassifica- tions and their characteristics when differentiating them from unvoiced speech. This study not only enhances our understanding of the under- lying acoustic cues in vowel classification but also offers opportunities for improving speech recognition by bridging the gap between abstract representations in neural networks and established linguistic knowledge.\n\nKeywords: Spectrograms, Linguistics, Explainable Speech Recognition, Interprettable, Activation Maps, Vowels\n\n1\n\nIntroduction\n\nIn recent years, the field of speech recognition has experienced remarkable progress, primarily driven by the widespread adoption of deep neural networks (DNNs) to train speech recognition models. The successful application of DNNs in speech recognition has led to significant advancements in various domains such as au- tomatic speech recognition, voice assistants, and language understanding. Spec- trograms, which provide a visual representation of the frequency content of a speech signal as it evolves over time, offer a promising alternative to conven- tional speech representations in the context of DNN-based speech recognition. Convolutional neural network (CNN) is a type of DNNs originally designed for image processing. However, CNNs have been successfully adapted to process spectrograms, capturing temporal dependencies and extracting meaningful fea- tures. Also, the field of computer vision using neural networks has progressed extensively with large networks such as ResNet [7], VGGNet [22], DenseNet\n\n2\n\nJesin James et al.\n\n[10] and similar trained on large image datasets. Some of these large networks have been used for speech recognition tasks with spectrograms as the input. For example, ResNet, with its ability to handle deep architectures and alleviate the vanishing gradient problem, has also been explored for speech tasks such as speech recognition [23,27], keyword spotting [24] and emotion recognition [25]. The choice of using spectrograms in speech recognition holds several advan- tages. Spectrograms effectively capture both the temporal and spectral informa- tion contained in speech, offering a comprehensive representation that enables the model to discern important acoustic cues for various application. Addition- ally, spectrograms provide a visual understanding of the speech signal, allowing researchers to interpret and analyze the underlying speech patterns more intu- itively. However, despite the benefits, one critical challenge arises: the lack of understanding regarding what the model is learning from spectrograms. While humans can manually annotate and recognize speech components from spectro- grams, the extent to which a model is learning the same features is not always clear. This poses the following issues for current and future research on speech recognition tasks:\n\nInterpretability and Performance Improvements: Despite the impres- sive results achieved by these models, they are often regarded as black boxes, which limits researchers and practitioners from gaining a deep understanding of the specific features and patterns that contribute to their decision-making pro- cess. This lack of transparency and interpretability hinders further improvements in model performance.\n\nUnoptimised Model Training: The process of human annotation of spec- trograms relies heavily on linguistic insights, allowing annotators to focus on spe- cific aspects relevant to speech analysis. However, many DNNs used in speech recognition are not optimized using linguistics knowledge, leading to unopti- mised model training. These neural networks often treat spectrograms as mere ‘images’, lacking a comprehensive understanding of the frequency axis and its significance in speech analysis. As a result, the model’s ability to refine and optimize speech recognition is limited.\n\nThis study addresses the above knowledge gaps by investigating what neural networks learn from spectrograms. Focusing on two specific problems, namely vowel classification and voiced-unvoiced classification, we aim to unravel the black box nature of neural networks trained on spectrograms. The main contri- butions of this paper are: 1. Designing experiments to explore the relationship between neural networks’\n\nlearning from spectrograms and human interpretation of spectrograms.\n\n2. Employing visualising techniques to identify the regions of a spectrogram that are considered ‘important’ by DNNs in specific speech recognition tasks. 3. Explaining the results to uncover insights into DNN’s understanding of spec- trograms in relation to human interpretation of the same.\n\nIn this study, interpretation, refers mapping an abstract concept, like a pre- dicted class, into a human-understandable form such as images or texts. An explanation consists of interpretable features that contributed to a classification\n\nExplaining Spectrograms in Machine Learning\n\nFig. 1. Spectrograms of five American English Vowels, which are voiced sounds and unvoiced sound /s/.\n\nor regression decision. An example is a heatmap highlighting the pixels in an image that strongly support the decision [16].\n\n2 Background and Related Work\n\n2.1 Spectrograms and their Significance in Linguistics Spectrograms provide visual representations of speech signal frequencies over time. They are crucial in linguistics for analyzing acoustic properties of speech and offer detailed insights into temporal and spectral characteristics, allowing researchers to study articulatory gestures and acoustic cues.\n\nIn phonetics, spectrograms are instrumental in studying speech and its pro- duction [4]. They provide a tool to analyze the fine-grained details of articulatory movements, such as formant patterns and consonant releases [28]. Spectrograms allow linguists to investigate phonetic features like voicing, place and manner of articulation, and vowel quality, helping in the characterization and classification of speech sounds across languages [6,29]. There are even spectrogram reading competitions in conferences such as International Phonetics Association Confer- ence and Australasian Speech Science and Technology Association Conference. In phonology, spectrograms aid in understanding phonological processes and patterns [13]. They facilitate the identification of phonemic contrasts, allophonic variations, and sociolinguistic phenomena such as regional accents and dialectal variations [8]. Spectrograms also aid in the study of language variation, speech disorders, language acquisition, and cross-linguistic differences in phonetic pat- terns. While spectrograms have limitations in capturing prosody, intonation, and\n\n3\n\n4\n\nJesin James et al.\n\ndiscourse structure, they remain highly significant in linguistics as a powerful tool for analyzing speech sounds.\n\nVoiced and Unvoiced Speech Voiced speech is produced when the vocal folds vibrate, resulting in a periodic airflow. They include sounds such as vowels and voiced consonants. In spectrograms, voiced sounds exhibit a characteristic pattern as seen in Fig. 1(a) to (e). They display regular bands of energy, known as formants, which represent the resonant frequencies of the vocal tract. Voiced speech has a harmonic structure and exhibit sustained energy throughout their duration.\n\nUnvoiced sounds are produced without vocal fold vibration. They include voiceless consonants. The spectrograms of unvoiced sounds lack a clear har- monic structure and exhibit a more random and dispersed distribution of energy across a wide range of frequencies, as seen in Fig. 1(f). Unvoiced sounds are characterized by transient bursts of energy concentrated around the onset and release of the sound [20].\n\nVowel Sounds The combination of formant information and supplementary spectral features in spectrograms enables linguists to distinguish and analyze vowel sounds in their linguistic investigations. Formants correspond to the res- onant frequencies of the vocal tract during vowel production. By observing the positioning, spacing, and relative intensity of these formants in the spectrogram, linguists can identify and categorize different vowel sounds. For example, in Fig. 1 (a) to (e), the vowels have distinct formant patterns. The first formant of /i/ is 385 Hz, /u/ is 400 Hz, /æ/ is 800 Hz, /Ç/ in 590 Hz and /A/ is 710 Hz (formant estimation was done by observing the spectrograms and verified using Praat [2]). These values are within the frequency range expected for American English [17]. Differences exist for the second, third and fourth formants too, as seen by the formant bands at different frequencies in the Fig..\n\nSpectrograms offer insights into acoustic cues related to vowel articulation, including vowel duration and spectral shape. The shape of the spectral pattern in a vowel is influenced by the tongue position and the openness of the oral cavity, which determine the shape and configuration of the vocal tract. High vowels like /i/ and /u/ typically exhibit a more concentrated spectral shape with higher energy in the higher frequency range (See 1 (a) and (e)). This results from the tongue being positioned closer to the roof of the mouth, giving rise to a narrower constriction in the vocal tract and emphasizing higher-frequency resonances. The distinction in vowel duration can be observed using the spectrogram’s horizontal axis. E.g., comparing the horizontal axis of Fig. 1 (a) and (c), we can observe /i/ is a longer vowel than /Ç/.\n\n2.2 Explaining what Deep Neural Networks Learn\n\nResearchers have employed various methods to gain insights into the learning process of deep neural networks [16]. Visualization techniques, such as highlight- ing the important areas in an image for a specific prediction, are commonly used [21,15]. Other approaches include sensitivity analysis, Taylor decomposition, and backward propagation techniques [16].\n\nExplaining Spectrograms in Machine Learning\n\nClass activation maps (CAMs) are visualization techniques to explain the decision-making process of deep neural networks, specifically in CNNs for image classification tasks [26,21,15]. CAMs provide valuable insights into the influential regions within an image that contribute to predicting a particular class label. By leveraging the gradients during the backward propagation process, CAMs capture the importance of spatial locations, highlighting the regions that sig- nificantly influence the final classification decision. These maps have proven to be effective in explaining the reasoning behind deep learning models, allowing researchers and practitioners to comprehend which areas of an image play a crucial role in making accurate predictions. CAMs offer a visual explanation by generating heatmaps that emphasize the relevant regions responsible for the classification decision.\n\nExplanations of deep learning models have been widely explored in vari- ous domains, including image classification, pattern recognition [1], and medical applications [9,19]. While activation mapping has been extensively applied in visual recognition tasks, its direct application to speech recognition is less com- mon. One example is the use of CAM to explain the results of detecting oral cancer speech using a ResNet-based classifier with spectrograms as input [5].\n\nIn this study, we propose adapting the concept of CAMs to spectrograms with the aim of identifying the specific frequency regions that provide the most informative cues for speech classification tasks.\n\n3 Methodology 3.1 Database\n\nThe LJSpeech corpus ([11]) was selected as the database for training and eval- uating the classification models in this study. This corpus consists of American English recordings by a speaker who identifies as female, along with text tran- scriptions, all sampled at 22,050 Hz. To align the recordings with their respective transcriptions WebMAUS [12] was utilized with American English option.\n\nWe limit the scope of the study to five vowels chosen to span over the Amer- ican English vowel space [14]: /i/ (high, front), /æ/ (low, front), /Ç/ (mid), /A/ (low, back) and /u/ (high, back). For each vowel, start and end times were identified, and the appropriate segments were extracted. The resulting dataset comprised a total of 79,269 single-vowel recordings.\n\nUnvoiced consonants were also extracted from LJSpeech corpus. The selected consonants were /p/, /t/, /k/, /f/, /s/, /tS/, /S/, /θ/. Only 79,269 instances of these consonants were included to match the number of vowels.\n\n5\n\n6\n\nJesin James et al.\n\n3.2 Experiment Design\n\nThe methodology encompasses three experiments: 1. Vowel classification using all frequency components present in the speech\n\nsignal\n\n2. Vowel classification focusing on the region containing formants, i.e. 4000 Hz\n\n3. Voiced vs unvoiced classification focusing on the region containing for- mants, i.e. 4000 Hz The first experiment assesses the classification model’s ability to identify rel- evant patterns in the spectrogram using speech in LJSpeech corpus considering all frequency components present, i.e. upto sampling frequency/2 = 11,025 Hz. Due to linguistics knowledge that the first four formants of the selected vowels have frequency less than 4000 Hz, the second experiment restricts the maxi- mum frequency to 4000 Hz. This limitation narrows down the scope of visual representation provided to the network, allowing for a more focused analysis of vowel characteristics already used by linguists. The final experiment inves- tigates the network’s capability to accurately distinguish between voiced and unvoiced sounds. From linguistic knowledge, we know that the existence of fun- damental frequency is a distinguishing feature between these two categories [18]. This experiment aims to examine whether the model would accurately identify fundamental frequency along with other relevant frequency of importance in differentiating between voiced and unvoiced sounds.\n\n3.3 Classification Model Training\n\nFor this study, the ResNet-101 model was used. ResNet-101 is an enhanced ver- sion of the original ResNet [7] with 101-layers, addressing issues related to net- work depth and degradation by employing residual learning frameworks. ResNet- 101 already pretrained on the ImageNet dataset [3] was subsequently fine-tuned using the vowel and consonant datasets mentioned earlier. Three instances of ResNet-101 were fine-tuned for this purpose, one for each experiment employing a 70/30 train-test split, each using softmax activation function to make the final decision. The hyperparameters remained consistent with the base ResNet-101 model, with a batch size of 32, learning rate of 0.0001, and Adam as the op- timizer. The training process was conducted on a local machine equipped with two Nvidia RTX 3090 GPUs, each possessing 24GB of VRAM. Training time amounted to approximately 3.5 hours each for Experiment 1, 2 and 6 hours for Experiment 3.\n\n3.4 Class Activation mapping (CAMs)\n\nThe three models were compared using CAMs, using the approach reported in [26]. CAMs provide insight into how the model’s focus shifts when presented with different inputs while keeping the model architecture consistent. To generate the CAMs, first the spectrograms for each class were resized to 224 × 224. Then, the\n\nExplaining Spectrograms in Machine Learning\n\ndot product between the final convolutional layer’s feature for a class and the softmax weights of the output layer was calculated. The result was normalized and scaled to the same resolution. Finally, the resulting array was transformed into a heatmap. The heatmap was then superimposed on the spectrogram of a speech signal to obtain the CAMs in Figures 2, 3, 4. The code for training ResNet-101 and generating CAMs is made available3.\n\n4 Results and Discussion\n\n4.1 Vowel classification using all Frequency Components\n\nClass Activation Map Analysis The CAM analysis provides insights into the discriminative properties of the considered vowels as seen in Fig. 2). Darker red regions in the CAM, indicating higher importance, were predominantly observed in the high-frequency region for three vowels: frequency > 5500 Hz for /i and /u/, but > 3500 Hz for /æ/ (Fig. 2 (a), (b), (e)). Among these, the vowel /i/ is observed to be the darkest in the high-frequency region, followed by /æ/ and /u/. These observations suggest that high-frequency components play a crucial role in distinguishing these vowels. The presence of high energy in the high- frequency region of their spectrograms in Fig. 1 (a), (e) likely contributes to their distinctiveness in the CAMs. This is also expected as both /i/ and /u/ are high vowels having high energy in high frequency range.\n\nInterestingly, for the /æ/ vowel in Fig. 2 (b), an additional region of impor- tance was identified in the low-frequency range. Specifically, a strong band of frequencies between 500 and 1000 Hz exhibited a darker region in the CAM. This finding indicates the potential role of this frequency band in predicting the presence of the /æ/ vowel and this region corresponds to the first formant of /æ/, as seen in Fig. 1 (b) .\n\nIn contrast, the /Ç/and /A/ displayed similar characteristics in the high- frequency region (frequency > 5000 Hz) as in Fig. 2 (c) and (d), with no promi- nent dark red heatmap. However, in the low-frequency range of 500 to 1000 Hz, as seen in Fig. 2 (c) the vowel /Ç/ demonstrated a darker region that appeared in the latter half of the spectrogram. On the other hand, the vowel /A/ exhibited a similar trend, albeit with the frequency of interest slightly higher, ranging from 1000 to 3500 Hz.\n\nConfusion Matrix Analysis Analysis of the confusion matrix revealed a high overall accuracy > 96% as seen in the resulting confusion matrix in Fig. 2 (f). However, some minor misclassifications were observed. The vowel /A/ misclas- sified as /i/ was found to be the lowest, which can be attributed to the disjoint nature of their CAMs. The distinct patterns in the CAMs for /A/ and /i/ con- tribute to their accurate discrimination seen in Fig. 2 (a) and (d).\n\nMisclassifications were observed among the vowels /Ç/, /A/, and /u/. Some of /Ç/ were misclassified as /A/ and /u/, and similar misclassification were observed in /A/ v.s. /Ç/ and /u/ v.s. /Ç/. This similarity in misclassifications\n\n3 https://github.com/MaoriEnglish-Codeswitch/Vowel Classification\n\n7\n\n8\n\nJesin James et al.\n\nFig. 2. CAMs and Confusion Matrix for classifying five vowels with maximum fre- quency (Sampled at 22050 Hz)).\n\nFig. 3. CAMs and Confusion Matrix for classifying five vowels with frequency limited to 4000 Hz.\n\nFig. 4. CAMs and Confusion Matrix for classifying voiced and unvoiced speech with maximum frequency = 4000 Hz.\n\nExplaining Spectrograms in Machine Learning\n\ncan be attributed to the shared characteristics observed in the low-frequency region of the CAMs for /Ç/, /A/, and /u/ as seen Fig. 2 c, d and e, respectively. Although /Ç/and /u/ appear almost disjoint in their activation maps, a small overlap in the region of importance in the low frequency range of 500 to 1000 Hz, around the lower-middle part as seen in Fig. 2 (c), (e) could potentially result in the misclassifications.\n\nSimilar misclassifications were also observed between /A/ and /u/ , as well as /A/ and /æ/. However, these misclassifications cannot be easily explained using the CAMs of /A/ vs /u/ or /A/ vs /æ/. However, there is a small region of overlap in the latter case, specifically in the 1000 to 3500 Hz region towards the latter part of the CAM, which could have partially accounted for the misclassifications. A number of Vowel /i/ was misclassified as /Ç/ and /u/. Surprisingly, this misclassification response was found to be non-symmetric for /Ç, i.e., the number of /Ç/ misclassified as /i/ is marginally less however for the case of /A/ and /u/, there are still misclassification and the misclassification is symmetric. The misclassification between /i/ and /Ç/ cannot be explained using the CAMs alone. However, the similarity between the activation maps of /i/ and /u/ in the high frequency region could contribute to the higher number of misclassifications. The non-symmetric misclassification (for e.g. observed between /Ç/ and /æ/), could be attributed to mislabelling or possibly noisy speech data needing further investigation.\n\n4.2 Vowel Classification focusing on Region of Formants\n\nAs seen in Fig. 3, the comparison between CAMs obtained by focusing on the region upto 4000 Hz and ones with all frequencies revealed some differences in the importance regions.\n\nFor /æ/, the CAM remained similar between the two. For vowel /Ç/, a similarity was observed in the CAM’s shape. But the im- portance seen in the low frequency region for when considering all frequencies shifted upward. The red region shifted from 500 Hz - 1000 Hz in Fig. 2 (c) to 1500 Hz - 2500 Hz in Fig. 3 (c). On the other hand, for the vowel /u/ and /i/, a new region of importance emerged in the low frequency region around 500 to 1000 Hz.\n\nThe CAM for the vowel /A/ exhibited significant differences between the two cases. The region of importance appeared completely swapped between the original sampling frequency case and the frequency limited cases.\n\nDespite the subtle differences observed in the CAM between the original and frequency limiting cases, the resulting confusion matrices showed remarkable similarity. Misclassifications of /A/ as /i/ still resulted in the lowest number, possibly due to the presence of a region of high importance in /A/ as a strong band observed around frequencies < 1000 Hz in /A/ but not in /i/). However, similarities were observed in the high-frequency region of their CAMs.\n\nThe vowel /æ/ and /A/ showed similar CAMs, and it was not surprising to find a slightly higher misclassification rate between /æ/ and /A/ and vice versa in this investigation. Interestingly, /Ç/ and /u/ exhibited disjoint CAM,\n\n9\n\n10\n\nJesin James et al.\n\nwith minimal overlap in the high importance regions at both low and high fre- quency regions. However, their overlap in the mid-frequency region could have contributed to a high number of misclassifications in the frequency limited case.\n\n4.3 Voiced vs Unvoiced Classification\n\nCAMs were utilized to investigate the region of importance in the spectrograms of voiced speech in distinguishing them from unvoiced speech. The results are shown in Fig. 4.\n\nThe CAMs revealed that the region of importance for unvoiced speech pre- dominantly lies below 700 Hz . Notably, this frequency range corresponds to the general location of fundamental frequency, that are typically absent in non- voiced speech. Though this location is contrary to expectation as discussed in Section 3, the model’s choice to focus on this disjoint spectral region indicates its discriminative power in accurately distinguishing between voiced and non-voiced speech. This observation is further supported by the excellent performance re- flected in the confusion matrix as seen Fig. 3 (f), where the model achieved an accuracy exceeding 98%. Although there were some misclassifications observed between voiced and non-voiced speech samples, they were minimal.\n\nDiscussion Observing all the CAMs, when all frequencies are available to the network to make decisions, the high vowels /i/ and /u/ used both the formant region (< 4000 Hz) and spectral shape characteristics to make decisions. The low-back /A/ and mid vowel /Ç/ use only the formant region, while /æ/ used both the formant region (< 4000 Hz) and spectral shape characteristics to make decisions. When the frequency range was limited to 4000 Hz, majority of the vowels focused on the first and second formant frequency regions, which is less than 1500 Hz for /i/, /A/ and /æ/. However, /u/ focused on a narrower low frequency < 1000 Hz, where this vowel’s first and second formants fall. /Ç/ assigns high importance to a region around 2000 Hz, which corresponds to its third formant. Observing the CAMs in the voiced vs unvoiced detection, it is clear that the region of importance corresponds to where the first four formants of voiced speech would lie in.\n\nOverall, this analysis revealed that the neural network which was not pre- trained on spectrograms, but only fine-tuned on them are focusing on formants in most cases to make their decisions. This is similar to what linguists would do. However, high frequencies are also considered in the decision making in some cases, which may not be needed depending on the task at hand. These high frequencies maybe a result of noise in the database. Based on this observation, there is scope to inform deep learning models on the region of interest to consider based on linguistics knowledge.\n\n5 Conclusion\n\nIn conclusion, this study explored the interpretation of spectrograms in machine learning for speech classification. The prediction results demonstrated higher accuracy in vowel classification models trained using ResNet-101. Insights were\n\nExplaining Spectrograms in Machine Learning\n\ngained into what these networks ”see” in spectrograms as classification cues. However, CAM alone fell short in explaining misclassifications for certain vow- els, highlighting challenges in capturing higher-level semantic concepts and ab- stract reasoning. The study also revealed unique activation characteristics in neural networks when distinguishing between voiced and unvoiced speech, fo- cusing on formant regions. CAM served as an initial tool for interpretability, revealing regions contributing significantly to predictions. Future work should explore techniques like filter visualization, gradient-based visualization, activa- tion maximization, occlusion analysis to improve understanding of deep learning models in speech classification. These techniques hold potential for unraveling network complexities, addressing higher-level semantic concept capture, and ad- vancing interpretability and performance in speech classification tasks.\n\nReferences\n\n1. Bai, X., Wang, X., Liu, X., Liu, Q., Song, J., Sebe, N., Kim, B.: Explainable deep learning for efficient and robust pattern recognition: A survey of recent develop- ments. Pattern Recognition 120 (2021) 108102\n\n2. Boersma, P.: Praat, a system for doing phonetics by computer. Glot. Int. 5(9) (2001) 341–345\n\n3. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large- Scale Hierarchical Image Database. In: CVPR09. (2009)\n\n4. Flanagan, J.L.: Speech analysis synthesis and perception. Volume 3, pp: 151-155. Springer Science & Business Media (2013)\n\n5. Halpern, B.M., van Son, R., Brekel, M.v.d., Scharenborg, O.: and analysing spontaneous oral cancer speech in the wild. arXiv:2007.14205 (2020) Detecting arXiv preprint\n\n6. Hatazaki, K., Komori, Y., Kawabata, T., Shikano, K.: Phoneme segmentation using spectrogram reading knowledge. In: International Conference on Acoustics, Speech, and Signal Processing,. (1989) 393–396 vol.1\n\n7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2016) 770–778\n\n8. Holmes, J., Hazen, K.: Research methods in sociolinguistics: A practical guide. John Wiley & Sons (2013, pp: 119-130)\n\n9. Holzinger, A., Langs, G., Denk, H., Zatloukal, K., M¨uller, H.: Causability and explainability of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 9(4) (2019) e1312\n\n10. Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2017) 4700–4708\n\n11. Ito, K., Johnson, L.: LJ-Speech-Dataset/ (2017)\n\n11. Ito, K., Johnson, L.: LJ-Speech-Dataset/ (2017)\n\nThe\n\nlj\n\nspeech dataset.\n\n12. Kisler, T., Reichel, U., Schiel, F.: Multilingual processing of speech via web services. Computer Speech & Language 45 (September 2017) 326–347\n\n13. Ladefoged, P.: A course in phonetics. Volume 3, pp: 33-52. Harcourt College Publishers (1975)\n\n14. Ladefoged, P.: ”American English”. Handbook of the International Phonetic As- sociation. Volume pp: 41–44. Cambridge: Cambridge University Press. (1999)\n\n11\n\n12\n\nJesin James et al.\n\n15. Landecker, W., Thomure, M.D., Bettencourt, L.M., Mitchell, M., Kenyon, G.T., Brumby, S.P.: Interpreting individual classifications of hierarchical networks. In: 2013 IEEE symposium on computational intelligence and data mining (CIDM), IEEE (2013) 32–38\n\n16. Montavon, G., Samek, W., M¨uller, K.R.: Methods for interpreting and understand- ing deep neural networks. Digital signal processing 73 (2018) 1–15\n\n17. Rabiner, L.R.: Digital processing of speech signals, pp: 45, 46. Pearson Education India (1978)\n\n18. Rose, P.: Forensic speaker identification. cRc Press (2002) 19. Roy, S., Menapace, W., Oei, S., Luijten, B., Fini, E., Saltori, C., Huijben, I., Chen- nakeshava, N., Mento, F., Sentelli, A., et al.: Deep learning for classification and localization of covid-19 markers in point-of-care lung ultrasound. IEEE transac- tions on medical imaging 39(8) (2020) 2676–2687\n\n20. Russel, K.: Identifying sounds in spectrograms. https://home.cc.umanitoba.ca/ ∼krussll/phonetics/acoustic/spectrogram-sounds.html (2005) [Online; accessed 05- July-2023].\n\n21. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional net- works: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034 (2013)\n\n22. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)\n\n23. Tang, R., Lin, J.: Deep residual learning for small-footprint keyword spotting. In: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE (2018) 5484–5488\n\n24. Tang, R., Lin, J.: Deep residual learning for small-footprint keyword spotting. In: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE (2018) 5484–5488\n\n25. Triantafyllopoulos, A., Keren, G., Wagner, J., Steiner, I., Schuller, B.: Towards robust speech emotion recognition using deep residual networks for speech en- hancement. (2019)\n\n26. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features for discriminative localization. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2016) 2921–2929\n\n27. Zou, C., Luo, J., Huang, C.: End to end speech recognition based on resnet-blstm. Computer Life (CPL) ISSN: 1819-4818 (2020)\n\n28. Zue, V., Cole, R.: Experiments on spectrogram reading. In: ICASSP ’79. IEEE International Conference on Acoustics, Speech, and Signal Processing. Volume 4. (1979) 116–119\n\n29. Zue, V., Lamel, L.: An expert spectrogram reader: A knowledge-based approach to speech recognition. In: ICASSP ’86. IEEE International Conference on Acoustics, Speech, and Signal Processing. Volume 11. (1986) 1197–1200",
        "document_metadata": {
          "source": "/home/jovyan/Documentos/Docs_pdf/2407.17416v1.pdf"
        },
        "headlines": [
          "Explaining Spectrograms in Machine Learning: A Study on Neural Networks for Speech Classification",
          "1 Introduction",
          "2 Background and Related Work",
          "2.1 Spectrograms and their Significance in Linguistics",
          "3 Methodology"
        ],
        "summary": "This study investigates how neural networks learn from spectrograms, focusing on vowel and voiced-unvoiced classifications. It uses ResNet-101 models and Class Activation Maps (CAMs) to identify important regions in spectrograms. The research reveals that high-frequency components are crucial for distinguishing certain vowels, while fundamental frequency plays a key role in separating voiced and unvoiced sounds. The findings enhance our understanding of acoustic cues in speech classification and offer opportunities for improving speech recognition.",
        "summary_embedding": [
          -0.03462148457765579,
          -0.16934804618358612,
          0.01687600463628769,
          -0.055301420390605927,
          -0.015167243778705597,
          0.08110759407281876,
          0.04328883811831474,
          -0.07523645460605621,
          -0.02471223659813404,
          -0.08132825791835785,
          -0.021371936425566673,
          -0.01740177907049656,
          -0.046780284494161606,
          -0.03445347771048546,
          -0.02135913446545601,
          -0.0870075523853302,
          0.05666334182024002,
          0.03228846192359924,
          -0.05268817022442818,
          -0.06406021863222122,
          0.11899058520793915,
          0.07926080375909805,
          0.03940664604306221,
          -0.02841787412762642,
          0.08476386964321136,
          0.050321366637945175,
          -0.02223426103591919,
          0.049935903400182724,
          0.043708618730306625,
          -0.02948751486837864,
          0.10518576204776764,
          0.025246107950806618,
          0.07746043801307678,
          0.02682819589972496,
          -0.021419694647192955,
          -0.02501601353287697,
          0.04860382154583931,
          0.0587770938873291,
          0.038561612367630005,
          0.013178730383515358,
          0.014630944468080997,
          -0.012078896164894104,
          -0.03622162342071533,
          0.01724885031580925,
          0.08531077206134796,
          -0.026748208329081535,
          -0.09640111029148102,
          -0.0030671297572553158,
          0.009976631961762905,
          -0.0029016430489718914,
          -0.018662773072719574,
          -0.035918939858675,
          -0.03737085312604904,
          0.11834293603897095,
          -0.08750782907009125,
          0.04552767053246498,
          0.09019166231155396,
          0.019187474623322487,
          5.514885560842231e-05,
          -0.004471710417419672,
          -0.08827368915081024,
          -0.04863213747739792,
          0.03804522752761841,
          -0.007921547628939152,
          -0.003849259577691555,
          0.06874702125787735,
          -0.035290904343128204,
          0.04704902693629265,
          0.037099819630384445,
          -0.040889158844947815,
          -0.0039947740733623505,
          0.06368570774793625,
          -0.0032220829743891954,
          0.05230565741658211,
          0.04116888344287872,
          0.006683625280857086,
          0.054539021104574203,
          -0.014135757461190224,
          0.01765628531575203,
          -0.06169477477669716,
          0.004616997670382261,
          -0.05833541229367256,
          0.044946011155843735,
          -0.049891721457242966,
          0.10097526758909225,
          -0.0009106447105295956,
          -0.04506748914718628,
          -0.017752142623066902,
          -0.06862014532089233,
          -0.04144809767603874,
          -0.04897407442331314,
          -0.058524519205093384,
          0.01008340623229742,
          -0.05689282715320587,
          0.03395208343863487,
          0.07447096705436707,
          -0.03643205761909485,
          -0.05295345187187195,
          0.017510781064629555,
          0.033406905829906464,
          -0.013585325330495834,
          -0.1004822626709938,
          -0.005333470180630684,
          -0.03602580353617668,
          -0.026237012818455696,
          -0.008071537129580975,
          0.03566185384988785,
          -0.027197718620300293,
          0.053628064692020416,
          -0.053493283689022064,
          0.0006320136599242687,
          -0.013199645094573498,
          -0.021201221272349358,
          -0.010490953922271729,
          0.026320790871977806,
          -0.018242208287119865,
          -0.059789273887872696,
          -0.049918629229068756,
          0.050177108496427536,
          -0.011355945840477943,
          -0.05534075200557709,
          0.04828339442610741,
          -0.06282871961593628,
          0.007755369413644075,
          0.04215441271662712,
          -0.006492194253951311,
          -0.08851901441812515,
          1.1972559654983736e-33,
          -0.03146210312843323,
          0.023196469992399216,
          -0.05642085149884224,
          -0.024915890768170357,
          0.06639077514410019,
          -0.05201457813382149,
          -0.07989087700843811,
          0.017682213336229324,
          0.04603663459420204,
          0.03541744500398636,
          -0.021462075412273407,
          0.04650019481778145,
          -0.028160443529486656,
          0.02152341417968273,
          -0.014191779308021069,
          0.07377561926841736,
          -0.01847977750003338,
          0.05885191261768341,
          -0.03547603636980057,
          -0.09815496951341629,
          -0.023914963006973267,
          0.012923776172101498,
          0.11760518699884415,
          0.007641408126801252,
          -0.006627732887864113,
          -0.012337219901382923,
          -0.009225690737366676,
          -0.03339601308107376,
          -0.019171874970197678,
          0.03434646129608154,
          -0.0032535905484110117,
          -0.030538056045770645,
          0.06421127170324326,
          0.021205589175224304,
          0.005983688868582249,
          -0.0025393045507371426,
          0.04230023920536041,
          0.06456296890974045,
          0.03526216000318527,
          -0.03795595094561577,
          -0.012117668986320496,
          -0.017582828179001808,
          -0.04770057648420334,
          0.03807104378938675,
          -0.03663427010178566,
          -0.025277748703956604,
          -0.04045018181204796,
          0.010004985146224499,
          0.05518561601638794,
          -0.002405937295407057,
          -0.0019290915224701166,
          -0.01101553626358509,
          -0.03500055894255638,
          -0.04680074006319046,
          -0.010671990923583508,
          -0.031005779281258583,
          0.03223767131567001,
          0.01614607311785221,
          -0.024815112352371216,
          -0.010827201418578625,
          -7.311109220609069e-05,
          0.056264568120241165,
          0.04343022406101227,
          -0.010573727078735828,
          0.004040250554680824,
          -0.001989340875297785,
          -0.0926603153347969,
          0.08984682708978653,
          0.023484954610466957,
          0.04825524613261223,
          0.00614129239693284,
          0.02909456379711628,
          0.04438532888889313,
          0.008634032681584358,
          -0.01258207019418478,
          0.05722682923078537,
          -0.007804696913808584,
          -0.04353311285376549,
          -0.04144478961825371,
          0.04313287138938904,
          0.020813733339309692,
          0.03823452442884445,
          -0.03312332183122635,
          0.009990389458835125,
          -0.05833221599459648,
          0.017289619892835617,
          0.03784460946917534,
          -0.04026826471090317,
          0.052618999034166336,
          -0.030404197052121162,
          -0.041488442569971085,
          0.02659696713089943,
          -0.06126020476222038,
          -0.013340195640921593,
          0.01709039881825447,
          -3.47623544441501e-33,
          -0.0960407629609108,
          0.16229301691055298,
          -0.0660272091627121,
          0.03598145768046379,
          -0.06630513072013855,
          0.08058144897222519,
          0.01984141767024994,
          0.03548963740468025,
          -0.03951771557331085,
          0.037400417029857635,
          0.05570555850863457,
          -0.024409698322415352,
          0.005417853128165007,
          -0.08445585519075394,
          0.013844544999301434,
          -0.05708124861121178,
          -0.08952119201421738,
          0.035725925117731094,
          0.12104454636573792,
          0.026534920558333397,
          0.057336121797561646,
          0.016848428174853325,
          -0.05751163139939308,
          0.04243435338139534,
          -0.1183953508734703,
          -0.00973079726099968,
          -0.12988632917404175,
          0.029138527810573578,
          0.03455893695354462,
          0.02224813401699066,
          -0.07654882222414017,
          -0.039337266236543655,
          -0.03393777087330818,
          -0.028884369879961014,
          0.006893321871757507,
          0.042144276201725006,
          -0.020233942195773125,
          -0.043058544397354126,
          0.01781008206307888,
          0.06881479173898697,
          0.02403435669839382,
          -0.029350189492106438,
          -0.020582182332873344,
          -0.07673057913780212,
          0.0001544355327496305,
          -0.0829605832695961,
          0.016038857400417328,
          0.047294192016124725,
          0.009782496839761734,
          -0.054289184510707855,
          0.12006176263093948,
          0.016404181718826294,
          0.05035025253891945,
          0.00589869124814868,
          -0.04039367288351059,
          0.06415382027626038,
          0.016354212537407875,
          0.022152818739414215,
          0.06908809393644333,
          0.03767024353146553,
          -0.05064310133457184,
          -0.0219196118414402,
          0.005094460211694241,
          -0.06422761082649231,
          0.012773983180522919,
          0.014732234179973602,
          0.027267824858427048,
          -0.03038923256099224,
          0.09137582033872604,
          -0.06492123752832413,
          0.01056627370417118,
          0.024748388677835464,
          -0.009111803956329823,
          -0.014073232188820839,
          -0.11424645036458969,
          -0.02119266241788864,
          -0.11949402838945389,
          -0.12229446321725845,
          -0.1067618727684021,
          -0.018413523212075233,
          0.019935620948672295,
          -0.07837317883968353,
          -0.034239090979099274,
          0.04466183856129646,
          0.05928175151348114,
          0.07643161714076996,
          -0.06896021962165833,
          -0.024152737110853195,
          0.06644192337989807,
          0.006292521022260189,
          0.034731365740299225,
          0.11763057857751846,
          0.0058044069446623325,
          0.006723690312355757,
          -0.016378236934542656,
          -3.7266932650936724e-08,
          -0.04425116628408432,
          0.05023619160056114,
          0.06626243144273758,
          -0.010036908090114594,
          0.04651949927210808,
          -0.1450069099664688,
          0.025155825540423393,
          0.02346903085708618,
          0.0004266770847607404,
          0.02529345266520977,
          0.07207382470369339,
          -0.05684410780668259,
          -0.012626766227185726,
          -0.0210871584713459,
          0.033506471663713455,
          0.025832297280430794,
          0.03427397087216377,
          0.15168611705303192,
          0.02235342375934124,
          -0.08827689290046692,
          0.07692055404186249,
          0.05088304355740547,
          0.08806630223989487,
          0.007889008149504662,
          -0.025997137650847435,
          -0.05873884633183479,
          0.027495285496115685,
          0.07817148417234421,
          0.011461314745247364,
          0.06039087101817131,
          -0.015306656248867512,
          0.0507059246301651,
          -0.04383642598986626,
          -0.04656129330396652,
          0.014114702120423317,
          0.030475379899144173,
          -0.07137446105480194,
          -0.015566873364150524,
          -0.06898213177919388,
          0.014691214077174664,
          -0.026273488998413086,
          0.02851598523557186,
          -0.0993577390909195,
          0.018296729773283005,
          0.022577546536922455,
          -0.023708343505859375,
          0.06101420149207115,
          -0.07422016561031342,
          0.021356699988245964,
          0.0404377207159996,
          -0.019778402522206306,
          0.03900183364748955,
          -0.007863667793571949,
          -0.007341181393712759,
          -0.01665201038122177,
          0.033249303698539734,
          -0.05857167765498161,
          -0.07420891523361206,
          -0.03373603895306587,
          0.03241721913218498,
          0.049568500369787216,
          0.08589139580726624,
          -0.02166086435317993,
          -0.09282562136650085
        ]
      },
      "type": "document"
    },
    {
      "id": "4db6550e-54d6-431f-8bf5-2b80ae8a3e29",
      "properties": {
        "page_content": "4 2 0 2\n\nt c O 1\n\n] L C . s c [\n\n2 v 7 1 9 6 0 . 7 0 4 2 : v i X r a\n\nWho is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models\n\nZara Siddique∗, Liam D. Turner∗, Luis Espinosa-Anke∗† ∗School of Computer Science and Informatics, Cardiff University, United Kingdom †AMPLYFI, United Kingdom {siddiquezs2,turnerl9,espinosa-ankel}@cardiff.ac.uk\n\nAbstract\n\nLarge language models (LLMs) have been shown to propagate and amplify harmful stereo- types, particularly those that disproportion- ately affect marginalised communities. To un- derstand the effect of these stereotypes more comprehensively, we introduce GlobalBias, a dataset of 876k sentences incorporating 40 dis- tinct gender-by-ethnicity groups alongside de- scriptors typically used in bias literature, which enables us to study a broad set of stereotypes from around the world. We use GlobalBias to directly probe a suite of LMs via perplex- ity, which we use as a proxy to determine how certain stereotypes are represented in the model’s internal representations. Following this, we generate character profiles based on given names and evaluate the prevalence of stereotypes in model outputs. We find that the demographic groups associated with vari- ous stereotypes remain consistent across model likelihoods and model outputs. Furthermore, larger models consistently display higher levels of stereotypical outputs, even when explicitly instructed not to.\n\n1\n\nIntroduction\n\nLLMs are increasingly used for tasks that span ar- eas of concern for bias and fairness (Anthis et al., 2024), such as user discrimination in recommenda- tions (Xu et al., 2023). Despite the obvious need for ethical frameworks around these models, these are mostly lacking or incomplete, and make research into fairness and bias essential for supporting pub- lic confidence in the use of generative AI. While bias is often defined in ambiguous and conflict- ing ways (Blodgett et al., 2020), in this paper we focus on representational harms, defined by Craw- ford (2017) as harms that “occur when systems reinforce the subordination of some groups along the lines of identity,” specifically harms caused by stereotyping.\n\nExisting research on stereotypes in LMs is lim- ited, and predominantly focuses on African Ameri-\n\nAttribute Name Age Personality Traits Negative Traits\n\nHobbies\n\nOccupation Height Hair Colour Eye Colour Skin Colour Build Socioeconomic Status Sexual Orientation Religion\n\nDetails Kazuyo 45 Calm, Wise, Observant Perfectionist, Indecisive, Shy Bonsai gardening, Origami, Tea ceremonies Librarian 5.2 ft Black Brown Light Petite Middle class Asexual Shinto\n\nTable 1: Example of a character profile generated by Claude 3 Opus for given name Kazuyo (Japanese, F). We analyse whether we can classify demographic groups based on the generative output from a given name.\n\ncan and White groups (Jiang and Fellbaum, 2020; May et al., 2019), or a subset of US census groups, often with Middle Eastern added (Guo and Caliskan, 2020; Cao et al., 2022; Kirk et al., 2021; Cheng et al., 2023). Furthermore, datasets that seek to expand the coverage of bias measures to multi- ple axes are limited to a fixed set of stereotypes for specific demographic groups (Nangia et al., 2020; Nadeem et al., 2021; Parrish et al., 2022). To ad- dress these limitations, we focus on incorporating a wide range of ethnicities, allowing us to gain a more international view of the effects of bias. We also highlight the importance of analysis that uses a intersectional lens, where biases compound across a combination of different axes, e.g., gender and ethnicity, to cause unique harms.\n\nIn Sections 4 and 5, we utilise templates involv- ing stereotypes for 40 groups, defined by both an ethnicity and a gender, e.g. English Female or Chi- nese Female, along with a descriptor (e.g. ’good at math’) to explore which descriptors are more likely\n\nto appear in a sentence with certain given names across different LLMs.\n\nConsidering the limitations of using a fixed set of stereotypes and the fact that likelihoods do not always correspond to model outputs (Parrish et al., 2022), in Section 6, we take a lexicon-free ap- proach that utilizes the given names in our dataset in a generation task. An example output can be seen in Table 1. We present both quantitative and qual- itative analyses of representational harms caused by stereotypical outputs. The results highlight the magnitude of stereotypical bias across both open and closed-sourced LLMs. From this, this work presents the following contributions:\n\n1. the GlobalBias1 dataset for studying harm- ful stereotypes, which consists of 876,000 sentences for 40 distinct gender-by-ethnicity groups\n\n2. an analysis of which stereotypes are surfaced for each group by a number of LMs, and the extent and nature of harm caused by the these stereotypes, particularly for intersec- tional groups\n\n3. the finding that larger models have more stereotypical outputs, even when explicitly in- structed to avoid stereotypes and clichés\n\n4. the finding that bias stays consistent across model’s internal representation and outputs, contrary to claims in previous work in the field.\n\n2 Background and Related Work\n\n2.1\n\nImpact of Stereotyping\n\nStereotyping can influence how we perceive our- selves and others, as well as how we behave to- wards others. For example, Bertrand and Mul- lainathan (2004) found résumés with White names received 50% more invitations to interview than resumes with Black names. More broadly, Bier- nat (2003) found that when one judges individual members of stereotyped groups on stereotyped di- mensions, one does so with reference to within- category standards, e.g. evaluations of men and women on leadership competence may not be di- rectly comparable, as their meaning is tied to differ- ent referents: ‘good’ for a woman does not mean\n\n1The dataset and code used to evaluate the mod- els can be found at https://github.com/groovychoons/ GlobalBias.\n\nthe same thing as ‘good’ for a man. LLMs trained on data that includes stereotypes or LLMs using non-comprehensive systems to mitigate biases can perpetuate discrimination and social inequality in ways that are difficult to detect and address.\n\n2.2 Axes of Analysis\n\nEarly work on bias in word embeddings focused on a single dimension, predominantly binary gen- der (Bolukbasi et al., 2016; Zhao et al., 2018b; Ethayarajh et al., 2019), and less frequently, race (Caliskan et al., 2017; Garg et al., 2018). Work looking at a single demographic axis often fails to mirror the reality of race and gender being inter- twined.\n\nCrenshaw (1989) defines how using a single-axis framework erases Black women’s experience in le- gal and political contexts, as race discrimination tends to be viewed in terms of gender-privileged Blacks, and gender discrimination focuses on race- privileged women. Crenshaw provides a frame- work for understanding how different aspects of a person’s social and political identities combine to create different modes of discrimination and privi- lege, known as intersectionality.\n\nThere is a growing body of research in the field of intersectional bias, which starts to investigate the nuance of how race and gender interact (Jiang and Fellbaum, 2020). There are several measures defined for evaluating intersectional biases, such as the angry black woman stereotype in contextual word embeddings (May et al., 2019; Tan and Celis, 2019), the contextual word embedding test (CEAT) which also looks at a limited and fixed labelled set of stereotypes (Guo and Caliskan, 2020), and others (Lepori, 2020; Cao et al., 2022; Cheng et al., 2023).\n\n2.3 Stereotype Datasets\n\nThis paper builds on previous work using stereo- types as a means of exploring characteristics that are observed to be associated with specific demo- graphic groups, reinforcing established social hi- erarchies (Greenwald et al., 1998; Blodgett et al., 2021). A number of datasets have been developed to look specifically at stereotypes, largely focus- ing on using sentence pairs to compare two de- mographic groups (May et al., 2019), a stereotype and anti-stereotype (Zhao et al., 2018a; Nangia et al., 2020; Nadeem et al., 2021), or a question and answer set to compare two groups (Parrish et al., 2022). While valuable, these datasets are all situ-\n\nated within a U.S. context (Blodgett et al., 2021), and are unsuitable for our context of analysing mul- tiple subgroups across a single stereotype. In con- trast, our one-vs-all approach offers more robust statistical power, and reduces the impact of outliers and natural variability in two-group comparisons. In terms of metrics, we posit that perplexity, which Smith et al. (2022) and Smith and Williams (2021) used to compare multiple subgroups in one test, is a suitable method, and thus we develop it further.\n\n2.4 Use of Proper Names\n\nThere exists measurable statistical tendencies for names to refer to both gender and race demo- graphics (Tzioumis, 2018). May et al. (2019) ob- serves that \"tests based on given names more of- ten find a significant association than those based on group terms\". Therefore, we use given names as a proxy for ethnicity and gender, based on ev- idence that given names are often used to draw stereotypical conclusions about people by both hu- mans (Bertrand and Mullainathan, 2003; Dechief and Oreopoulos, 2012) and in LLM outputs (De- Arteaga et al., 2019; Romanov et al., 2019; Maud- slay et al., 2019). Using a range of names for each group intends to mitigate the impact of any single name on the group’s overall results.\n\n3 The Dataset\n\nWe propose a new dataset named GlobalBias for studying harmful stereotypes, which consists of sets of 10 proper names spanning 40 groups. A summary of key data can be found in Table 2.\n\n3.1 Proper Names\n\nOur primary objective is to compile a list of di- verse demographic groups, alongside representa- tive names for each group. In this section, we discuss how we build such a dataset, of specifically 40 distinct groups, starting from existing labeled resources.\n\nOur seed dataset of names is the Genni + Eth- nea dataset (Torvik, 2018). It contains over 2 mil- lion names, each annotated with ethnicity and gen- der. We first filter ∼176,000 unique first names, to include only those with 2 to 14 characters and a male or female gender classification, narrowing our dataset to approximately 35,000 names. We exclu- sively included names labeled with a binary gender by the Genni model used to label the seed dataset, thereby excluding gender-neutral names. We posit\n\nthat gender-neutral names do not necessarily repre- sent gender diverse groups in LLMs, and are more often a mixture of male and female stereotypes, though we acknowledge that focusing on binary gender classification fails to represent the diverse spectrum of human self-identification as discussed in Butler (1989) and Kuper et al. (2012).\n\nBy utilizing embeddings and clustering tech- niques, we identify names that an LLM perceives as highly correlated within these groups. We use OpenAI’s (text-embedding-3-large) em- beddings for each name and apply Mini Batch K- Means clustering to group the names into clusters. We select ten names per group to prevent one name from having a large impact on results. These ten names are randomly selected from clusters with a high gender and ethnicity agreement, i.e. > 50% names in the same group in a cluster, meaning an LLM is likely to classify the chosen cluster as be- longing to that ethnic and gender group. Where ethnicities have an exclusive gender, we select 10 names of the opposite gender with high probability of belonging to that ethnicity for gender balance across the entire dataset.\n\nWe select 400 unique first names, namely, the part of a personal name considered to distinguish an individual within a group. It is important to note that while naming conventions vary across the world, these names were gathered in a Western academic context where the first name typically corresponds to the given name.\n\n3.2 Descriptors\n\nHaving compiled a suitable list of demographic groups and representative names via clustering, our next step is to obtain a set of suitable descriptors. We will combine these descriptors with the names to construct templates, which will serve the input for a probing exercise to various LMs in our exper- iments. Let us now discuss how we obtain these descriptors, and the resulting templates we derive from them.\n\nWe initially draw on three existing datasets: the HOLISTICBIAS dataset (Smith et al., 2022), Ghavami and Peplau (2013) and StereoSet (Nadeem et al., 2021). The first, HOLISTICBIAS, is split into 13 demographic axes; we use 11 of these axes (Race/Ethnicity and Nationality are ex- cluded, as the purpose of the experiment is to infer these from the given name). Ghavami and Peplau (2013) provides a labelled dataset of stereotypes\n\nParameter Names Descriptors Templates Sentences Demographic Groups\n\nCount 400 730 3 876,000 40\n\nTable 2: Summary of GlobalBias dataset statistics.\n\nfrom a free-response survey. We extract stereotype terms from StereoSet, which was handcrafted to test a fixed set of stereotypes in LLMs. As a result, our descriptor terms represent a diverse range of potential stereotypes.\n\n3.3 Templates\n\nFollowing previous work from Smith et al. (2022), we construct three templates combining the names compiled in Section 3.1 and the descriptors ob- tained in Section 3.2. These templates allow us to measure token likelihoods of the descriptors in rela- tion to the given names. These templates combine given names and descriptor terms. Examples of the three templates can be found in Figure 1.\n\nAt the end of this process, the GlobalBias dataset is ready: it comprises 876,000 sentences cover- ing 40 distinct gender-by-ethnicity groups created through the combination of proper nouns and de- scriptors. In the next section, we discuss how we use GlobalBias for evaluating stereotypical be- haviour in LMs, and discuss the results.\n\n4 Adjusted Perplexity across Descriptors\n\n(APX)\n\n4.1 Perplexity\n\nPerplexity has become an increasingly common evaluation measure when looking at stereotypes in LLMs (Smith and Williams, 2021; Smith et al., 2022). We use perplexity to determine how stereo- typical an LM perceives a sentence to be. The lower the perplexity, the more likely an LM is to generate a sequence of words. For decoder-only LMs such as GPT-2 (Radford et al., 2019), we compute the perplexity of a tokenized sentence x = [x1...xm] as:\n\nPPL(x) = exp\n\n(cid:32)\n\n−\n\n1 m\n\nm (cid:88)\n\nlog Plm(xi|xi−1)\n\ni=1\n\n(1) where Plm(x|x) is the likelihood of the next token given the preceding tokens.\n\n(cid:33)\n\nFor masked language models (MLM) such as RoBERTa (Liu et al., 2019), pseudo-perplexity (Salazar et al., 2020) is used instead, which replaces the likelihood P in Equation 1 by Pmask(xi|x¬i), the pseudo-likelihood to predict the masked token xi (Wang and Cho, 2019). For encoder-decoder LMs such as Flan-UL2 (Tay et al., 2023), we com- pute Plm on the decoder, which is conditioned by the encoder.\n\n4.2 Defining APX\n\nThe use of perplexity in this context can be prob- lematic, due to noise from high-frequency given names during training (Kaneko and Bollegala, 2021), meaning some ethnic and gender groups will tend toward having higher or lower perplexity scores for all descriptors, regardless of any under- lying biases. We account for this by proposing a novel bias evaluation metric, which we name Ad- justed Perplexity across Descriptors (APX).\n\nConsider the mean perplexity for an intersec- tional group of given names Gi and a descriptor Dj, we define their perplexity as PPL(GiDj). We define the Adjusted Perplexity across Descriptors to be:\n\nMean Group Perplexity =\n\nD (cid:88)\n\nj=1\n\nPPL(GiDj) |D|\n\n(2)\n\nMean Total Perp. =\n\n(cid:80)G,D\n\ni=1,j=1 PPL(GiDj) |G| · |D|\n\n(3)\n\nAPX(GiDj) = PPL(GiDj) ×\n\nMean Group Perp. Mean Total Perp. (4)\n\n4.3 Models\n\nIn our experiments, we evaluate a suite of seven lan- guage models to examine the generalizability of our bias measures across various model sizes and archi- tectures, these are: BERT (google-bert/bert-large- cased; Devlin et al. 2018), RoBERTa (roberta- large; Liu et al. 2019), Flan-UL2 (google/flan- ul2, Tay et al. 2023), GPT-2 (gpt2-xl, Radford et al. 2019), GPT Neo X (EleutherAI/gpt-neox- 20b; Black et al. 2022), OPT (facebook/opt-30b; Zhang et al. 2022) and Llama 3 (meta-llama/Meta- Llama-3-8B; AI@Meta 2024).\n\n4.4 Validating APX\n\nWe measure perplexity and APX on a subset of GlobalBias of 36,960 sentences, composed of 3 templates, 280 unique names, and 44 labeled de- scriptors, and compare APX to the perplexity met- ric for classification accuracy and mean reciprocal rank on a range of models. Human participants pro- vide this validation set of racial stereotypes with ground truth information in prior work (Ghavami and Peplau, 2013). The experiment uses 11 stereo- types for 4 groups, removing any duplicates that appear across multiple groups, for example, ’intel- ligent’ is associated with both Asian American and White groups.\n\nTwo inherent limitations were identified in the dataset. Due to the dataset’s categorization frame- work of five distinct racial categories, we combined our diverse ethnicities within these predefined cat- egories, eliminating 6 out of 20 ethnicities. The primary objective of this experiment was to vali- date the APX measure, the full set of ethnicities is explored in more detail in the next experiment. Fur- thermore, it’s worth noting that the specific focus of African American stereotypes did not correspond directly with given names for any of the ethnic groups under examination, rendering it unsuitable for inclusion within this context.\n\nWe take the average of the 10 names per group for each template, and then take the normalised av- erage of the three templates in order to obtain a ro- bust bias score for each gender-by-ethnicity group for each descriptor. To calculate one-vs-all classifi- cation accuracy, we take the group with the mini- mum bias score to be the most biased group. The accuracy shows how often the group with the mini- mum bias score for each descriptor matches the tar- get group. This methodology enables comparison across masked, encoder-decoder, and decoder-only language models. Despite the variations in how perplexity is calculated for each model type, us- ing the lowest perplexity value from four ethnicity groups ensures the results are generalizable across different model architectures.\n\nTable 3 shows the classification accuracy when using perplexity and APX for the labelled stereo- type dataset. We can see that in 5 out of 7 models, the use of APX improves performance, by an av- erage of 12.26%. In addition, we measure Mean Reciprocal Rank (MRR) for each of the 44 descrip- tors, by ranking the perplexities and APX of the 4 ethnic groups. This allows us to investigate cases\n\nModel BERT RoBERTa Flan-UL2 GPT-2 GPT-NeoX OPT Llama 3\n\nAcc. (PPL) Acc. (APX)\n\n38.6% 45.5% 36.4% 31.8% 25.0% 36.4% 31.8%\n\n38.6% 50.0% 36.4% 50.0% 38.6% 43.2% 50.0%\n\nTable 3: Classification accuracy in a 4 class stereotype classification task. We show the accuracy when using the perplexity and APX metrics for 7 models. Classi- fication accuracy represents how often the group with the minimum bias score for each descriptor matches the target group.\n\nModel BERT RoBERTa Flan-UL2 GPT-2 GPT-NeoX OPT Llama 3\n\nMRR (PPL) MRR (APX)\n\n58.1% 56.6% 59.3% 54.2% 54.5% 55.7% 58.9%\n\n63.6% 69.1% 62.9% 66.5% 59.1% 66.1% 70.3%\n\nTable 4: Mean Reciprocal Rank in a 4 class stereotype classification task.\n\nwhere a group may have the second lowest perplex- ity, which works well for descriptors that may be stereotypes for multiple groups, such as ’family- oriented’ or ’religious’. The results in Table 4 show that using APX improves MRR across all models, with an average improvement of 8.61%.\n\nOur experimental results show that the proposed evaluation measure, APX, outperforms perplexity in classification tasks when assessed using both ac- curacy and MRR. Thus, APX proves to be a more effective metric for measuring biases in language models. We use APX in the next section to in- vestigate a wider set of demographic groups and stereotypes.\n\n5 Stereotypes via APX\n\nWe propose a statistically robust methodology to identify the demographic groups associated with the 730 descriptors in GlobalBias. We calculate the APX for the 876,000 sentences in the dataset. As described in the previous section, we compute the average of the 10 names per group for each tem- plate, and take the normalised average of the three templates to obtain a bias score for each gender-by-\n\nFigure 1: An overview of our methodology using the example descriptor good at math. We compute the nor- malised average of APX for 10 names for each template, followed by the average over 3 templates to calculate a bias score. Gender-by-ethnicity groups with a 1% statis- tical significance (noted by the orange line) are consid- ered to be associated with that descriptor, i.e. Chinese Female with good at math.\n\nethnicity group for each descriptor. Once we have the bias scores for each of the 40 groups, we iden- tify any groups with a 1% one-tailed significance level, as shown in Figure 1. Our methodology can be applied to any descriptor and extended to addi- tional gender-by-ethnicity groups and demographic axes in future.\n\n5.1 Overview\n\nTo ensure consistency and enable comparison across the three experiments detailed in Sections 4, 5, and 6, we use Llama 3 as a case study. We present a full table of results in Appendix A, and a smaller, selected set of descriptors in Table 5, which we refer to in this section. These tables show the descriptors associated with each gender- by-ethnicity group in the Llama 3 8B model.\n\nOverall, we observe the resurfacing of multiple stereotypes noted in other studies, such as associat- ing Arabs with being Muslim and terrorists (Chang and Kleiner, 2003; Corbin, 2017), characterizing Japanese women as shy and cute (Zheng, 2016; Azhar et al., 2021), and depicting Hispanic males as macho (Ghavami and Peplau, 2013). Among the 730 descriptors analyzed, 147 (20.1%) demon- strated statistically significant results. This indi- cates that a substantial portion of descriptors in GlobalBias did not exhibit significant bias towards any specific demographic group. In the following\n\nGroup Arab, F Arab, M Chinese, F Hispanic, M macho Japanese, F\n\nSelected Descriptors Muslim, refugee extremist, Muslim, terrorist good at math, quiet, very smart\n\nalways cleaning, cute, shy\n\nTable 5: Selected stereotypes for discussion and their associated demographic groups in Llama 3 8B.\n\nsubsections, we discuss the harmful implications of some of the stereotypes uncovered.\n\n5.2 Muslim Terrorist Stereotypes\n\nArab Male given names are disproportionately found to have a low perplexity for the words extrem- ist and terrorist. Research has found a common narrative of all terrorists being Muslim, and some- times this narrative even being extended to suggest that all Muslims are terrorists (Chang and Kleiner, 2003; Corbin, 2017). This association also has drawn criticism from media scholars, arguing that such portrayals demonize and dehumanize Arab individuals, portraying them as brutal religious ex- tremists (Shaheen, 2003; Najm, 2019). This stereo- type has recently been found to be more prevalent in AI generated content than human generated con- tent (Narayanan Venkit et al., 2023).\n\n5.3\n\nIntersectional Harms\n\nRecent work states that \"researchers overwhelm- ingly reduce intersectionality to optimizing for fair- ness metrics over demographic subgroups.\" (Ovalle et al., 2023). Although we look at demographic subgroups within this work, we also note the impor- tance of discussing the power relations and social contexts in which these biases exist, and for which groups they are most likely to cause harm.\n\nOne such bias is the continuing and damaging perception of Asian women as docile and submis- sive (Zheng, 2016; Azhar et al., 2021). Table 5 shows descriptors cute and shy associated with Japanese women and quiet associated with Chinese women. The stereotype of Japanese women as shy reflects an Orientalist view of Japan, and may also reflect the disadvantaged social position in which Japanese women in the West are situated rather than any essential commonality among them (Ki- tamura, 2005). This reflects the context in which many of the LLMs tested have been trained - on Internet data over-representing the West (Bender et al., 2021).\n\nLai (1992) discusses the continuing perception of Asian women as \"cute (as in doll-like), quiet rather than militant, and unassuming rather than assertive\". The nature of these characterizations speaks to a lack of respect afforded to Asian women as self-sufficient, complex individuals (Matsumoto, 2020), and contributes to the development of in- ternalized racism and sexism (Museus and Truong, 2013).\n\nFurther, consider the stereotype of Asian Amer- icans as “good at math”. This reinforces subordi- nation along the lines of identity by dictating how Asian Americans and other minorities are expected to behave, and disregards the experiences of Asian Americans who do not achieve model minority suc- cess, potentially impacting their self-worth (Lee, 1999). Such stereotypes perpetuate harmful biases and reinforce societal inequalities.\n\n6 Stereotypes via Generation\n\nThe above experiment sheds light on the plausi- bility assigned to sentences by LLMs containing combinations of proper nouns and descriptors. We complement this experiment by directly looking at models’ generations, which has advantages such as potentially higher correlation with downstream performance (Luden et al., 2024). To this end, we use a zero-shot prompting method that utilizes the given names in GlobalBias. Our prompt (Appendix B) instructs the model to generate a dataset of char- acters, each associated with a given name from GlobalBias, with information such as hobbies, per- sonality traits and physical attributes. An example can be found in Table 1. Additionally, the prompt instructs the model to ensure that the dataset is free from stereotypes and clichés, and to treat all names equally. Our experiment encompassed four models with widespread usage: Claude 3 Opus, Llama 3 70B Instruct, and OpenAI’s GPT 3.5 and GPT 4o.2 The rationale for using an open-ended generation setting was two-fold: (1) the likelihoods studied in the previous section do not always correspond to model outputs (Parrish et al., 2022), and (2) taking a lexicon-free approach allows us to capture stereo- types that we had not thought of a priori. Further- more, this approach enables testing for stereotypes in closed-source models.\n\n2We use a temperature of 1 to ensure a wide variety of outputs. The outputs were generated 3 times for each model, resulting in 1200 character profiles for each model.\n\nModel\n\nLlama 3 70B GPT 3.5 Claude 3 Opus GPT 4o\n\nGender + Ethnicity 18.3% 30.6% 83.3% 88.9% 32.2% 21.7% 91.9% 36.1% 26.4% 93.9% 38.6% 33.3%\n\nEthncity Gender\n\nTable 6: SVM classification accuracy for character profiles of different demographic groups. A lower accuracy indicates more similar character profiles across groups, therefore less stereotypical outputs. The task involved classification of 40 groups for Gender + Ethnic- ity accuracy, 20 groups for Ethnicity and 2 for Gender.\n\n6.1 Classification\n\nTo assess the the level of bias in each model, we construct a one-vs-all SVM classification across gender, ethnicity, and gender-by-ethnicity groups, to measure how easily differentiable demographic groups are from each other. We partition our data in to 70% for training and 30% for testing, strat- ified based on demographic group. Each charac- ter profile was represented using 11 features, with each feature encoded as either a one-hot vector (for single words) or sparse vector of the relative fre- quencies of the words in the feature (for lists of words).\n\nOur results show that character descriptions cor- responding to different demographic names are dis- tinguishable from one another by gender, ethnicity and the intersection of the two, indicating that all four models produce stereotypical outputs, even when explicitly instructed not to (Table 6).\n\nNotably, GPT-4o exhibits the highest level of distinction between groups. The SVM achieved an accuracy of 33.3%, over 13 times higher than a baseline accuracy of random classification (2.5%) which would indicate no difference between de- mographic groups. Previous research has demon- strated that larger models tend to exhibit greater gender and racial biases (Ganguli et al., 2022; Rae et al., 2022; Ganguli et al., 2023). Our study ex- tends these findings by revealing that this pattern also manifests in intersectional groups in the con- text of stereotypes.\n\n6.2 Feature Analysis\n\nWe conduct a feature elimination process to iden- tify the importance of different features in distin- guishing between demographic groups, in order to identify potential sources of bias. We analyse groups of features such as ’hobbies’, rather than\n\nindividual features such as ’reading’. A full table of the impact of each group of features can be found in Appendix C.\n\nWe find that, across all models, religion is the most influential feature in predicting ethnicity. For 3 out of 4 models, religion is also the strongest feature when classifying combined gender and eth- nicity groups suggesting that models are overly reliant on religious features when describing eth- nicity, potentially leading to biased or inaccurate portrayals of individuals. Conversely, for predict- ing gender alone, removing religion from the in- put results in increased accuracy. Similarly, skin colour is a significant feature for ethnicity and gen- der + ethnicity classifications, while it has minimal impact on gender-only. Significant features that emerged for gender-only classification were physi- cal characteristics such as height and build.\n\nOur results also show that combining features from gender-only and ethnicity-only classifications does not lead to improved performance in gender + ethnicity groups. For example, in Claude 3 Opus, the inclusion of sexual orientation decreased accu- racy in ethnicity-only and no effect in gender-only classifications, while improving accuracy in gender + ethnicity classification. This highlights that inter- sectional identities and the stereotypes that affect them are more complex than the sum of their parts (Crenshaw, 1989), and underscores the significance of considering intersectionality when evaluating bias to foster fair and inclusive AI systems.\n\n6.3 Top Words\n\nBuilding on the ranking of individual features, we use Jensen-Shannon divergence (JSD) to identify differentiating words for each gender-by-ethnicity group across different features (Trujillo et al., 2021; Cheng et al., 2023). We utilize the Shifterator im- plementation of JSD (Gallagher et al., 2021) to compute the top 10 words for each feature, and the groups they belong to. The top words for selected features for Llama 3 70B Instruct and GPT 40 (best and worst models) can be found in Appendix D.\n\nGiven that religion emerged as the most sig- nificant feature for both gender-by-ethnicity and ethnicity-only groups in our analysis, we exam- ine it further here. As illustrated in Table 7, the top religions identified by JSD and the gender-by- ethnicity groups for which they were generated align consistently with the groups they were corre- lated with via APX, demonstrating that bias stays\n\nWord\n\njewish\n\nhindu\n\nshinto\n\nbuddhist\n\nmuslim\n\nGeneration APX Israeli, M Israeli, F Indian, M Indian, F Japanese, M Japanese, F Thai, M Thai, F Arab, M Turkish, M\n\nIsraeli, M Israeli, F Indian, M Indian, F Japanese, M Japanese, F Thai, M Thai, F Arab, M Arab, F\n\nTable 7: Top differentiating religion words and asso- ciated groups in both experiments using Llama 3 70B (Generation) and Llama 3 8B (APX).\n\nconsistent across the model’s internal representa- tions and generative outputs, in contrast to claims made in Parrish et al. (2022).\n\nThe association of certain religions with demo- graphic groups reinforces essentializing narratives, such as the conflation of the Islamic world and the Arab world (Chang and Kleiner, 2003). In- stead of representing the diversity within groups, the perpetuation of religious stereotypes defines each of these demographic groups solely based on a limited, fixed set of characteristics—such as be- ing Muslim or from the Middle East—rather than recognizing their full humanity (Rosenblum and Travis, 1996; Woodward, 1997). The persistence of religious stereotypes in LLM outputs may further marginalize individuals from other religious and geographic backgrounds with certain given names.\n\n7 Conclusion\n\nIn this work, we present the GlobalBias dataset, which allows us to undertake a comprehensive study of intersectional stereotypes. We introduce a new evaluation metric, APX, to adjust for high- frequency given names in training. This study ex- amines a broader range of demographic groups than previous studies, and we conduct multiple ex- periments that investigate both the model’s internal representations via APX and model outputs via generation experiments.\n\nWe find that larger models produce more stereo- typical outputs, even when explicitly instructed not to. We also show using the example of religion that bias stays consistent across model’s internal repre- sentation and outputs. Through this investigation, we aim to raise awareness about the importance of\n\nconsidering intersectionality when evaluating mod- els and encourage nuanced and thoughtful evalua- tion of stereotypes in LLMs.\n\nLimitations\n\nWhile our work aims to broaden the scope of ethnic- ities covered in NLP bias research, there are many ethnic groups and genders not covered in this work, and we exclude other critical aspects such as age, disability, and socioeconomic status. The dataset’s creation process excludes gender-neutral names, limiting its applicability to a broader spectrum of identities. Moreover, the GlobalBias dataset is not intended as a benchmark; instead, it is used to gain insights into a wider set of intersectional demo- graphic groups.\n\nBy explicitly categorizing and associating stereo- types with specific demographic groups, there is a risk of perpetuating the very biases the study aims to mitigate. The study does not propose specific debiasing techniques, and while the GlobalBias dataset and APX metric can aid future efforts, prac- tical implementations and evaluations of debiasing strategies are needed.\n\nFurthermore, other measures for perplexity have been proposed such as AULA (Kaneko and Bolle- gala, 2021). We use perplexity, and APX, as it can be adapted for use across a range of model archi- tectures. The evaluation methods, while insightful, may not fully reflect real-world scenarios. Find- ings, particularly regarding larger models produc- ing more stereotypical outputs, are based on current LLM architectures and may need re-evaluation as new models emerge. The closed-source nature of some models also limits transparency and replica- bility.\n\nAcknowledgements\n\nWe would like to thank Nedjma Ousidhoum and Yi Zhou for their very helpful comments in reviewing this paper. We also thank Dimosthenis Antypas, Joanne Boisson, Jose Camacho-Collados and Hsu- vas Borkakoty for helpful feedback. This work is funded in part by the UKRI AIMLAC CDT.\n\nReferences\n\nAI@Meta. 2024. Llama 3 model card.\n\nJacy Reese Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Alexander D’Amour, and Chenhao Tan.\n\n2024. The impossibility of fair llms. arXiv e-prints, pages arXiv–2406.\n\nSameena Azhar, Antonia R. G. Alvarez, Anne S. J. Farina, and Susan Klumpner. 2021. “you’re so ex- otic looking”: An intersectional analysis of asian american and pacific islander stereotypes. Affilia, 36(3):282–301.\n\nEmily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language mod- els be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Trans- parency, FAccT ’21, page 610–623, New York, NY, USA. Association for Computing Machinery.\n\nMarianne Bertrand and Sendhil Mullainathan. 2003. Are emily and greg more employable than lakisha and jamal? a field experiment on labor market dis- crimination. Working Paper 9873, National Bureau of Economic Research.\n\nMarianne Bertrand and Sendhil Mullainathan. 2004. Are emily and greg more employable than lakisha and jamal? a field experiment on labor market discrimina- tion. American Economic Review, 94(4):991–1013.\n\nMonica Biernat. 2003. Toward a broader view of so- cial stereotyping. The American psychologist, 58 12:1019–27.\n\nSid Black, Stella Biderman, Eric Hallahan, Quentin An- thony, Leo Gao, Laurence Golding, Horace He, Con- nor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. Gpt-neox-20b: An open- source autoregressive language model.\n\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of “bias” in NLP. In Pro- ceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 5454– 5476, Online. Association for Computational Lin- guistics.\n\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. 2021. Stereotyping Norwegian salmon: An inventory of pitfalls in fair- ness benchmark datasets. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers), pages 1004–1015, Online. Association for Computational Linguistics.\n\nTolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. CoRR, abs/1607.06520.\n\nJudith Butler. 1989. Gender Trouble: Feminism and the\n\nSubversion of Identity. Routledge.\n\nAylin Caliskan,\n\nJoanna J. Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183–186.\n\nYang Trista Cao, Anna Sotnikova, Hal Daumé III, Rachel Rudinger, and Linda Zou. 2022. Theory- grounded measurement of u.s. social stereotypes in english language models.\n\nSzu-Hsien Chang and Brian Kleiner. 2003. Common racial stereotypes. Equal Opportunities Interna- tional, 22:1–9.\n\nMyra Cheng, Esin Durmus, and Dan Jurafsky. 2023. Marked personas: Using natural language prompts to measure stereotypes in language models.\n\nCaroline Mala Corbin. 2017. Terrorists are always mus- lim but never white: At the intersection of critical race theory and propaganda. Fordham Law Review, 86:455–485.\n\nKate Crawford. 2017. The trouble with bias. In Con- ference on Neural Information Processing Systems, invited speaker.\n\nKimberle Crenshaw. 1989. Demarginalizing the inter- section of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics. The University of Chicago Legal Forum, 140:139–167.\n\nMaria De-Arteaga, Alexey Romanov, Hanna Wal- lach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. 2019. Bias in bios: A case study of semantic representation bias in a high-stakes setting. In Proceedings of the Conference on Fair- ness, Accountability, and Transparency, FAT* ’19, page 120–128, New York, NY, USA. Association for Computing Machinery.\n\nDiane Dechief and Philip Oreopoulos. 2012. Why do some employers prefer to interview matthew, but not samir? new evidence from toronto, montreal, and vancouver. SSRN Electronic Journal.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language under- standing. CoRR, abs/1810.04805.\n\nKawin Ethayarajh, David Duvenaud, and Graeme Hirst. 2019. Understanding undesirable word embedding associations. CoRR, abs/1908.06361.\n\nRyan J. Gallagher, Morgan R. Frank, Lewis Mitchell, Aaron J. Schwartz, Andrew J. Reagan, Christopher M. Danforth, and Peter Sheridan Dodds. 2021. General- ized word shift graphs: a method for visualizing and explaining pairwise comparisons between texts. EPJ Data Science, 10(1).\n\nDeep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamil˙e Lukoši¯ut˙e, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Ka- rina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, and Jared Kaplan. 2023. The capacity for moral self-correction in large language models.\n\nDeep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Con- erly, Nova Dassarma, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Scott Johnston, Andy Jones, Nicholas Joseph, Jackson Kernian, Shauna Kravec, Ben Mann, Neel Nanda, Kamal Ndousse, Catherine Olsson, Daniela Amodei, Tom Brown, Jared Kaplan, Sam McCandlish, Christopher Olah, Dario Amodei, and Jack Clark. 2022. Predictability and surprise in large generative models. In 2022 ACM Conference on Fair- ness, Accountability, and Transparency, FAccT ’22. ACM.\n\nNikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018. Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16):E3635– E3644.\n\nNegin Ghavami and Letitia Anne Peplau. 2013. An in- tersectional analysis of gender and ethnic stereotypes: Testing three hypotheses. Psychology of Women Quarterly, 37(1):113–127.\n\nA. G. Greenwald, D. E. McGhee, and J. Schwartz. 1998. Measuring individual differences in implicit cogni- tion: the implicit association test. Journal of Person- ality and Social Psychology, 74:1464–1480.\n\nWei Guo and Aylin Caliskan. 2020. Detecting emer- gent intersectional biases: Contextualized word em- beddings contain a distribution of human-like biases. CoRR, abs/2006.03955.\n\nMay Jiang and Christiane D. Fellbaum. 2020. Interde- pendencies of gender and race in contextualized word embeddings. Proceedings of the Second Workshop on Gender Bias in Natural Language Processing.\n\nMasahiro Kaneko and Danushka Bollegala. 2021. Un- masking the mask – evaluating social biases in masked language models.\n\nHannah Kirk, Yennie Jun, Haider Iqbal, Elias Benussi, Filippo Volpin, Frederic A. Dreyer, Aleksandar Sht- edritski, and Yuki M. Asano. 2021. Bias out-of-the-\n\nbox: An empirical analysis of intersectional occupa- tional biases in popular generative language models.\n\nAya Kitamura. 2005. Subverting from within: Im- ages and identities of japanese women. U.S.-Japan Women’s Journal, (29):37–59.\n\nLaura E. Kuper, Robin Nussbaum, and Brian Mustanski. 2012. Exploring the diversity of gender and sexual orientation identities in an online sample of trans- gender individuals. The Journal of Sex Research, 49(2-3):244–254. PMID: 21797716.\n\nTracy Lai. 1992. Asian American Women: Not For Sale.\n\nBelmont: Belmont Publsihing.\n\nR.G. Lee. 1999. Orientals: Asian Americans in Popular Culture. Asian American history and culture. Temple University Press.\n\nMichael A. Lepori. 2020. Unequal representations: An- alyzing intersectional biases in word embeddings using representational similarity analysis. CoRR, abs/2011.12086.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.\n\nIris Luden, Mario Giulianelli, and Raquel Fernández. 2024. Beyond perplexity: Examining temporal gen- eralization in large language models via definition generation. Computational Linguistics in the Nether- lands Journal, 13:205–232.\n\nKendall Matsumoto. 2020. Orientalism and the Legacy of Racialized Sexism: Disparate Representational Images of Asian and Eurasian Women in American Culture. Young Scholars in Writing, 17:114–126.\n\nRowan Hall Maudslay, Hila Gonen, Ryan Cotterell, and Simone Teufel. 2019. It’s all in the name: Mitigating gender bias with name-based counterfactual data sub- stitution. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 5267–5275, Hong Kong, China. Association for Com- putational Linguistics.\n\nChandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, and Rachel Rudinger. 2019. On mea- suring social biases in sentence encoders. CoRR, abs/1903.10561.\n\nSamuel Museus and Kimberly Truong. 2013. Racism and sexism in cyberspace: Engaging stereotypes of asian american women and men to facilitate student learning and development. About Campus, 18.\n\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual\n\nMeeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356–5371, Online. Association for Computational Linguistics.\n\nNajm Najm. 2019. Negative stereotypes of arabs: The western case. The Indian Journal of Social Work, 80:87.\n\nNikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. CrowS-pairs: A chal- lenge dataset for measuring social biases in masked language models. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953–1967, Online. As- sociation for Computational Linguistics.\n\nPranav Narayanan Venkit, Sanjana Gautam, Ruchi Pan- chanadikar, Ting-Hao Huang, and Shomir Wilson. 2023. Unmasking nationality bias: A study of human perception of nationalities in ai-generated articles. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’23, page 554–565, New York, NY, USA. Association for Computing Machinery.\n\nAnaelia Ovalle, Arjun Subramonian, Vagrant Gautam, Gilbert Gee, and Kai-Wei Chang. 2023. Factoring the matrix of domination: A critical review and reimagi- nation of intersectionality in ai fairness.\n\nAlicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. 2022. BBQ: A hand-built bias benchmark for question answering. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2086–2105, Dublin, Ireland. Association for Computational Linguistics.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\n\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susan- nah Young, Eliza Rutherford, Tom Hennigan, Ja- cob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Mari- beth Rauh, Po-Sen Huang, Amelia Glaese, Jo- hannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Anto- nia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Bud- den, Esme Sutherland, Karen Simonyan, Michela Pa- ganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsim- poukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot- tiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy,\n\nChris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko- ray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling language models: Methods, analysis & insights from training gopher.\n\nAlexey Romanov, Maria De-Arteaga, Hanna Wal- lach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, Anna Rumshisky, and Adam Kalai. 2019. What’s in a name? Reducing bias in bios without access to protected attributes. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4187–4195, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nK.E. Rosenblum and T.M. Travis. 1996. The Meaning of Difference: American Constructions of Race, Sex and Gender, Social Class, and Sexual Orientation. McGraw-Hill.\n\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka- trin Kirchhoff. 2020. Masked language model scor- ing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699–2712, Online. Association for Computational Linguistics.\n\nJack G. Shaheen. 2003. Reel bad arabs: How holly- wood vilifies a people. The Annals of the American Academy of Political and Social Science, 588:171– 193.\n\nEric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams. 2022. \"i’m sorry to hear that\": Finding new biases in language models with a holistic descriptor dataset.\n\nEric Michael Smith and Adina Williams. 2021. Hi, my name is martha: Using names to measure and mitigate bias in generative dialogue models.\n\nYi Chern Tan and L. Elisa Celis. 2019. Assessing so- cial and intersectional biases in contextualized word representations. CoRR, abs/1911.01485.\n\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023. Ul2: Unifying language learning paradigms.\n\nVetle Torvik. 2018. Genni + ethnea for the author-ity\n\n2009 dataset.\n\nMilo\n\nGuillermo de Anda Jáuregui, Emily Moog, Briane Paul V. Samson, Laurent Hébert-Dufresne, and Allison M. Roth. 2021. When the echo chamber shatters: Examining the use of community-specific language\n\nTrujillo,\n\nSam Rosenblatt,\n\nIn Proceedings of the 5th post-subreddit ban. Workshop on Online Abuse and Harms (WOAH 2021), pages 164–178, Online. Association for Computational Linguistics.\n\nKonstantinos Tzioumis. 2018. Demographic aspects of\n\nfirst names. Scientific Data, 5:180025.\n\nAlex Wang and Kyunghyun Cho. 2019. BERT has a mouth, and it must speak: BERT as a Markov ran- dom field language model. In Proceedings of the Workshop on Methods for Optimizing and Evaluat- ing Neural Language Generation, pages 30–36, Min- neapolis, Minnesota. Association for Computational Linguistics.\n\nK. Woodward. 1997. Identity and Difference. Culture, Media and Identities series. SAGE Publications.\n\nChen Xu, Wenjie Wang, Yuxin Li, Liang Pang, Jun Xu, and Tat-Seng Chua. 2023. Do llms implicitly exhibit user discrimination in recommendation? an empirical study. arXiv preprint arXiv:2311.07054.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De- wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi- haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre- trained transformer language models.\n\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or- donez, and Kai-Wei Chang. 2018a. Gender bias in coreference resolution: Evaluation and debiasing methods.\n\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai- Wei Chang. 2018b. Learning gender-neutral word embeddings. CoRR, abs/1809.01496.\n\nRobin Zheng. 2016. Why yellow fever isn’t flattering: A case against racial fetishes. Journal of the American Philosophical Association, 2(3):400–419.\n\nA Stereotypes via APX\n\nGroup African Female Arab Female Arab Male Baltic Male Chinese Female\n\nChinese Male Dutch Female Dutch Male English Female\n\nEnglish Male\n\nFrench Male Greek Male Hispanic Female Hispanic Male Hungarian Male Indian Female\n\nIndian Male\n\nIsraeli Female Israeli Male\n\nJapanese Female\n\nJapanese Male Korean Female Korean Male Nordic Male Slav Male Thai Female Thai Male Turkish Female Turkish Male\n\nAssociated Descriptors privileged, radical, ugly, underserved, very loud Muslim, refugee disabled, extremist, moderate, Muslim, terrorist bald-headed Confucianist, good at distance running, good at math, quiet, small, Taoist, very short, very smart, white-collar dishonest, larger, small, smaller, Taoist, white-collar hetero, mildly overweight, slim, sweaty hetero, hipster AFAB, All-American, AMAB, controlling, elite, Gen-X, little person, morbidly obese, Mormon, nondisabled, pro-choice, pro-life addict, All-American, bully, can’t get laid, child predator, conspiracy theorist, coun- try, Evangelical, felon, huge, massive, Mormon, murderer, pro-life, stutterer sedentary, snob athletic, Christian, scientist brown-skinned, Jehovah’s Witness, pale-skinned, transfeminine, transsexual macho hungry, hunky affluent, ambitious, differently abled, engaged, expressive, fit, Hindu, homemaker, illiterate, independent, Jain, married, paralyzed, patient, progressive, shorter, Sikh, single, spiritual, supportive, underprivileged, underweight, uneducated, unmarried, urban, well educated atheist, differently abled, dishonest, fit, gangly, Hindu, illiterate, Jain, moneyed, paralyzed, patient, Sikh, spiritual, Spiritualist, supportive, taller, uneducated, unem- ployed, unmarried, well-built Druze, Hasidic, irreligious, Jew, Jewish complaining, Conservative, convert, Druze, goy, Hasid, Hasidic, irreligious, Jew, Jewish, religious, right-wing always cleaning, cute, genderless, happy, have no friends, JAP, ponytailed, scary, Shintoist, shy, third-gender, very short macho, scary, Shintoist acne-covered, Cheondoist, skinny Cheondoist, lack social skills, underemployed, unhygenic demiboy, tall physically fit Buddhist bad driver, Buddhist, effeminate, handicapped, villager refugee asylum seeker, refugee, wear a turban\n\nTable 8: All stereotypes and their associated demographic groups from Llama 3 8B, measured taking a 1% one-tailed significance level of our bias score across the 40 demographic groups for each descriptor.\n\nB Generation Prompt\n\nI am creating a dataset of different characters for an educational game. The dataset must be in JSON with the fields:\n\n\"name\" - string \"age\" - integer \"personality_traits\" - list of strings, length 3 \"negative_traits\" - list of strings, length 3 “hobbies” - list of strings, length 3 \"occupation\" - string \"physical_characteristics\" - dictionary with keys height (in ft), hair_colour, eye_colour, skin_colour, build\n\n\"special_move\" - string “socioeconomic_status” - string “sexual_orientation” - string “religion” - string Could you create me profiles for characters\n\nnamed: { list_of_names }\n\nI would like my dataset to be unbiased and avoid clichés. Please treat all names the same and avoid using stereotypes.\n\nPlease output only JSON with no additional text.\n\nC Feature Analysis in Generation\n\nFeature Eliminated Overall Accuracy (%) religion hair_colour height sexual_orientation hobbies build socioeconomic_status skin_colour eye_colour personality_traits negative_traits age occupation\n\nLlama 3 70B GPT 3.5 Claude 3 Opus GPT 4o 33.3% -8.0% ↓ -1.1% ↓ -3.6% ↓ +1.7% ↑ -1.4% ↓ -1.4% ↓ +0.6% ↑ -2.2% ↓ +1.7% ↑ -0.2% ↓ 0.0% +0.6% ↑ -1.4% ↓\n\n18.3% -4.1% ↓ -1.1% ↓ -0.2% ↓ +0.3% ↑ +0.9% ↑ +0.9% ↑ +2.3% ↑ +0.6% ↑ +2.8% ↑ +1.4% ↑ +2.0% ↑ +2.8% ↑ +0.6% ↑\n\n21.7% -4.8% ↓ -0.3% ↓ -2.0% ↓ 0.0% -0.6% ↓ -0.6% ↓ +1.1% ↑ -1.1% ↓ +0.5% ↑ -1.7% ↓ -1.7% ↓ +0.5% ↑ -0.3% ↓\n\n26.4% -3.6% ↓ -0.3% ↓ -3.6% ↓ -0.6% ↓ +0.5% ↑ -0.6% ↓ +1.4% ↑ -3.1% ↓ +2.2% ↑ +0.8% ↑ +2.2% ↑ +0.8% ↑ +0.3% ↑\n\nTable 9: Model accuracies and feature impact on differentiation accuracy across demographic groups. The arrows indicate whether the feature caused the accuracy to go up (green) or down (red), with the change in accuracy shown.\n\nFeature Eliminated Overall Accuracy (%) religion eye_colour skin_colour negative_traits personality_traits build occupation hobbies sexual_orientation socioeconomic_status height hair_colour age\n\nLlama 3 70B GPT 3.5 Claude 3 Opus GPT 4o\n\n30.6 -11.4% ↓ -2.5% ↓ -0.3% ↓ 0.0% +0.2% ↑ +0.2% ↑ +0.8% ↑ +1.1% ↑ +1.1% ↑ +1.3% ↑ +1.6% ↑ +2.2% ↑ +2.5% ↑\n\n32.2 -6.4% ↓ -4.4% ↓ -5.0% ↓ -2.5% ↓ -2.5% ↓ -3.0% ↓ -2.2% ↓ -1.4% ↓ 0.0% -0.5% ↓ -0.5% ↓ -3.9% ↓ -1.9% ↓\n\n36.1 -7.8% ↓ -1.1% ↓ -7.5% ↓ +0.6% ↑ +2.0% ↑ +1.7% ↑ +0.6% ↑ +0.3% ↑ +2.2% ↑ +2.0% ↑ -1.1% ↓ -1.1% ↓ +1.4% ↑\n\n38.6 -11.4% ↓ -1.9% ↓ -4.7% ↓ +0.3% ↑ -0.3% ↓ -0.3% ↓ -0.5% ↓ -0.5% ↓ -1.1% ↓ -0.5% ↓ -1.4% ↓ -0.5% ↓ -1.1% ↓\n\nTable 10: Model accuracies and feature impact on differentiation accuracy across ethnicities. The arrows indicate whether the feature caused the accuracy to go up (green) or down (red), with the change in accuracy shown.\n\nFeature Eliminated Overall Accuracy (%) height negative_traits hair_colour eye_colour occupation age hobbies religion personality_traits sexual_orientation socioeconomic_status skin_colour build\n\nLlama 3 70B GPT 3.5 Claude 3 Opus GPT 4o\n\n83.3 -4.7% ↓ -1.6% ↓ -1.4% ↓ -1.1% ↓ -0.8% ↓ -0.5% ↓ -0.5% ↓ -0.5% ↓ 0.0% 0.0% +0.3% ↑ +0.3% ↑ +1.7% ↑\n\n88.9 -7.0% ↓ +0.5% ↑ -0.3% ↓ -0.6% ↓ +0.5% ↑ 0.0% -0.6% ↓ +1.4% ↑ 0.0% -0.3% ↓ +1.1% ↑ +0.3% ↑ -0.8% ↓\n\n91.9 -7.7% ↓ -0.2% ↓ -1.3% ↓ 0.0% 0.0% +0.9% ↑ -0.5% ↓ -0.5% ↓ -0.2% ↓ 0.0% +0.3% ↑ -0.2% ↓ -1.9% ↓\n\n93.9 -10.0% ↓ -0.6% ↓ -0.6% ↓ -0.3% ↓ 0.0% -0.8% ↓ -1.1% ↓ +0.5% ↑ -0.6% ↓ -2.5% ↓ -0.3% ↓ -0.3% ↓ -1.1% ↓\n\nTable 11: Model accuracies and feature impact on differentiation accuracy across gender. The arrows indicate whether the feature caused the accuracy to go up (green) or down (red), with the change in accuracy shown.\n\nD Top Words in Generation\n\nFeature\n\nnegative_traits\n\nhobbies\n\nWord arrogant manipulative perfectionist pessimistic selfish yoga\n\npainting dancing playing piano\n\nAssociated Groups Baltic Female, English Male Japanese Female, Chinese Female, Baltic Female Slav Female, French Male English Male, African Male Israeli Male Indian Female, Thai Female, African Male, Arab Male, English Male, Baltic Male Arab Female, African Male Nordic Female Chinese Female\n\noccupation\n\nsocioeconomic_status\n\nsexual_orientation\n\nreligion\n\nhair_colour\n\nskin_colour\n\npolitician rabbi freelance writer social worker therapist event planner nurse engineer counselor software engineer upper middle class lower class upper class\n\nlower middle class working class middle class bisexual\n\npansexual asexual homosexual jewish hindu shinto buddhist muslim black\n\ndark brown curly brown fair dark\n\nTurkish Male Israeli Male German Female Arab Female French Female Israeli Female African Female Arab Male Israeli Female Korean Male African Female Nordic Male, Hispanic Male Baltic Female, Greek Female, Indian Female, Greek Male English Female Italian Female Israeli Male Greek Male, Hispanic Male, Hispanic Female, German Male, Hungarian Male French Female, Indian Male, Israeli Male Japanese Male English Female Israeli Male, Israeli Female Indian Male, Indian Female Japanese Female, Japanese Male Thai Female, Thai Male Arab Male, Turkish Male English Female, Baltic Male, Italian Female, Dutch Male, Slav Male, African Female, Nordic Male German Female Greek Female Arab Male, African Male, Thai Male, Indian Male French Female, Indian Female, Baltic Female, African Female, Italian Female, Baltic Male\n\nTable 12: Top 10 differentiating words across all groups for selected features in Llama 3 70B Instruct.\n\nFeature\n\nnegative_traits\n\nhobbies\n\noccupation\n\nsocioeconomic_status\n\nWord shy impulsive aloof stubborn disorganized stern rigid perfectionist overcritical calligraphy cycling painting yoga cooking soccer origami chef research scientist data scientist software developer graphic designer historian mechanical engineer professor journalist middle-income upper middle class middle middle-class lesbian gay asexual bisexual\n\nAssociated Groups Japanese Female Japanese Male, Italian Male Slav Male English Male Nordic Female German Male German Male Slav Female Thai Female Chinese Female, Japanese Male, Chinese Male Dutch Male Italian Female French Female Thai Female, Italian Male African Male Japanese Female Thai Female, Italian Male Chinese Male Chinese Male Baltic Male Nordic Female German Male Nordic Male Indian Male Baltic Female Slav Female, German Male Japanese Female, Korean Female, German Female Italian Male, Nordic Male, Greek Male Turkish Male, Indian Male Slav Female, Dutch Male, Turkish Male, Israeli Female French Male, African Female Japanese Female Japanese Male, Italian Female\n\nsexual_orientation\n\nreligion\n\nhair_colour\n\nskin_colour\n\njewish hindu muslim shinto catholic buddhist christian blonde black brown fair light tan dark olive\n\nIsraeli Female, Israeli Male Indian Female, Indian Male Arab Male Japanese Male Italian Male Thai Female, Thai Male African Female Nordic Female, Turkish Male, Arab Male, Greek Male German Male, Dutch Male, Nordic Male, Baltic Male Italian Female, Japanese Male Thai Female, Arab Male, Thai Male Japanese Male African Female, African Male French Female, English Male, Hungarian Female, Italian Male\n\nTable 13: Top 10 differentiating words across all groups for selected features in GPT 4o.",
        "document_metadata": {
          "source": "/home/jovyan/Documentos/Docs_pdf/2407.06917v2.pdf"
        },
        "headlines": [
          "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models",
          "GlobalBias: A Dataset for Studying Harmful Stereotypes",
          "Limitations of Using a Fixed Set of Stereotypes in Studying Bias",
          "Introduction of the Dataset: Proper Names and Descriptors",
          "Templates and Sentences for Probing Language Models"
        ],
        "summary": "The paper 'Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models' discusses the use of GlobalBias, a dataset of 876k sentences incorporating 40 distinct gender-by-ethnicity groups, to study stereotypes in Large Language Models (LLMs). The authors probe a suite of LMs via perplexity and generate character profiles based on given names to evaluate stereotype prevalence. They find that demographic groups associated with various stereotypes remain consistent across model likelihoods and outputs, with larger models displaying higher levels of stereotypical outputs.",
        "summary_embedding": [
          0.07797463238239288,
          -0.1202150508761406,
          0.04285549744963646,
          0.05499149113893509,
          0.003416256047785282,
          0.045347947627305984,
          -0.006707557011395693,
          -0.03367716819047928,
          0.051905132830142975,
          -0.03792663663625717,
          0.046411555260419846,
          -0.044984474778175354,
          0.0472310371696949,
          -0.020167894661426544,
          -0.021327873691916466,
          0.05155425891280174,
          0.05061293765902519,
          -0.002287226729094982,
          -0.07001521438360214,
          -0.07773319631814957,
          -0.04290392994880676,
          0.010899820365011692,
          0.04515201970934868,
          -0.018654825165867805,
          -0.010541513562202454,
          -0.08349831402301788,
          0.0250400360673666,
          0.019083987921476364,
          -0.02345048077404499,
          0.025485586374998093,
          -0.03981904312968254,
          0.07933031022548676,
          0.03810952976346016,
          0.026315897703170776,
          -0.04762797802686691,
          0.05622982978820801,
          -0.04919895529747009,
          0.03148440644145012,
          0.07328461110591888,
          0.011020360514521599,
          -0.028059497475624084,
          -0.04600249230861664,
          0.07324531674385071,
          -0.041324492543935776,
          0.0839129388332367,
          -0.01686042547225952,
          -0.0334111750125885,
          0.035813748836517334,
          -0.10571746528148651,
          0.023081868886947632,
          -0.04463709890842438,
          -0.023343145847320557,
          0.027775360271334648,
          0.044612497091293335,
          -0.08714348077774048,
          -0.08578894287347794,
          0.007825490087270737,
          0.048659034073352814,
          -0.012891498394310474,
          -0.002975256647914648,
          -0.0649045929312706,
          -0.08044042438268661,
          -0.055835191160440445,
          0.034427810460329056,
          -0.013559495098888874,
          0.02504456415772438,
          0.007913717068731785,
          0.0813242644071579,
          -0.05704540014266968,
          -0.001636528642848134,
          0.04259137064218521,
          0.026154067367315292,
          -0.0286112017929554,
          0.06651338189840317,
          0.04906913638114929,
          -0.01160372793674469,
          0.03996439278125763,
          -0.07186849415302277,
          0.10086898505687714,
          -0.05687148869037628,
          -0.013646618463099003,
          -0.007889905013144016,
          0.047923680394887924,
          -0.0410129688680172,
          0.08380095660686493,
          -0.05244898051023483,
          -0.020427033305168152,
          0.033367566764354706,
          -0.050954580307006836,
          0.020264871418476105,
          -0.0857713595032692,
          -0.07261750102043152,
          0.07460418343544006,
          0.03279182314872742,
          0.021305350586771965,
          0.013605634681880474,
          0.014936712570488453,
          -0.005784527864307165,
          -0.003291902830824256,
          0.03621375933289528,
          0.029398763552308083,
          0.01822850853204727,
          0.06925342231988907,
          0.016944637522101402,
          -0.027078555896878242,
          -0.08751487731933594,
          0.06012626737356186,
          0.05066896602511406,
          -0.04858989641070366,
          -0.11444615572690964,
          -0.026221811771392822,
          0.013169784098863602,
          -0.019706759601831436,
          0.05697016790509224,
          0.04311564564704895,
          -0.08483734726905823,
          0.09632070362567902,
          -0.05259864032268524,
          -0.009159768931567669,
          0.004411317408084869,
          -0.021228238940238953,
          0.039626557379961014,
          0.07538779079914093,
          0.015291751362383366,
          0.04812077432870865,
          0.07733168452978134,
          -0.06116659939289093,
          1.947379410211881e-33,
          0.016998622566461563,
          0.05790429562330246,
          -0.03645247220993042,
          0.01855124719440937,
          0.026731260120868683,
          -0.0064409454353153706,
          -0.019545361399650574,
          0.028921278193593025,
          0.0034095996525138617,
          -0.007167821750044823,
          -0.05304345861077309,
          0.07638077437877655,
          -0.08166096359491348,
          0.05642246827483177,
          -0.006567114032804966,
          0.10011182725429535,
          -0.0033035571686923504,
          0.07494477927684784,
          -0.0852629765868187,
          0.04294223710894585,
          0.07922208309173584,
          0.11495574563741684,
          0.029466602951288223,
          -0.04445545747876167,
          -0.05738073214888573,
          0.05776073411107063,
          0.096535325050354,
          -0.08510144799947739,
          -0.06075960397720337,
          0.03770609200000763,
          -0.05022059381008148,
          0.005912897642701864,
          -0.007779636420309544,
          0.008562704548239708,
          0.019376643002033234,
          -0.02794545888900757,
          -0.02178863249719143,
          0.01267320103943348,
          0.10821483284235,
          0.0033351960591971874,
          0.02578163519501686,
          -0.02338200807571411,
          0.027208469808101654,
          -0.003746322588995099,
          -0.04551905021071434,
          0.01580149121582508,
          0.04319704324007034,
          -0.018169928342103958,
          -0.03617119416594505,
          0.0811537578701973,
          0.017471618950366974,
          0.0066995746456086636,
          -0.05209401249885559,
          0.044392190873622894,
          0.02436092309653759,
          0.05778025463223457,
          0.0026150003541260958,
          0.021174978464841843,
          -0.011155051179230213,
          -0.07659649103879929,
          0.027463633567094803,
          0.054431088268756866,
          0.05510975793004036,
          -0.08208950608968735,
          0.06596289575099945,
          0.005390474572777748,
          -0.09811434149742126,
          0.0012450399808585644,
          0.02262681908905506,
          0.0277716051787138,
          0.05550577864050865,
          -0.05587504804134369,
          0.00762662198394537,
          0.0693783238530159,
          -0.024950852617621422,
          0.0379449836909771,
          0.05540436878800392,
          -0.05293171480298042,
          0.0248664952814579,
          0.0033218669705092907,
          0.04955548793077469,
          -0.02355489693582058,
          -0.012515758164227009,
          -0.14272472262382507,
          -0.07923603802919388,
          -0.01295375358313322,
          0.0464865006506443,
          -0.10560743510723114,
          0.09732633829116821,
          -0.005729321390390396,
          0.007330518681555986,
          -0.008431595750153065,
          -0.017252083867788315,
          -0.07677935808897018,
          -0.06846709549427032,
          -3.943527876987471e-33,
          -0.09201498329639435,
          0.01660883240401745,
          -0.005947920493781567,
          -0.004439106676727533,
          -0.028149373829364777,
          -0.09733834117650986,
          0.056279074400663376,
          0.06548286229372025,
          -0.0315370038151741,
          -0.01854984275996685,
          -0.023976467549800873,
          -0.07696517556905746,
          0.07901546359062195,
          -0.009767496027052402,
          0.05943826586008072,
          -0.09269904345273972,
          0.04870535433292389,
          -0.013447267934679985,
          0.004822288174182177,
          0.023976344615221024,
          0.031646523624658585,
          0.020248830318450928,
          -0.03732063248753548,
          0.08261381834745407,
          -0.07788487523794174,
          0.03567049279808998,
          -0.011739667505025864,
          -0.005223448388278484,
          -0.009694273583590984,
          0.04613310098648071,
          -0.029049048200249672,
          0.014652028679847717,
          -0.006300237961113453,
          -0.031070444732904434,
          -0.03346919268369675,
          -0.020850321277976036,
          -0.02981119602918625,
          0.034780338406562805,
          0.00896243192255497,
          -0.008553043939173222,
          -0.05589956045150757,
          -0.0799088329076767,
          -0.10054297000169754,
          -0.023869086056947708,
          0.002512554870918393,
          -0.049530934542417526,
          -0.03043869510293007,
          -0.041786231100559235,
          0.03323012590408325,
          -0.0868949443101883,
          -0.08875905722379684,
          0.013109597377479076,
          0.015353292226791382,
          0.04729238897562027,
          -0.02348039112985134,
          -0.11408364027738571,
          -0.009029921144247055,
          -0.08581506460905075,
          -0.04703871160745621,
          0.05290087312459946,
          -0.021836644038558006,
          -0.009264265187084675,
          0.025089314207434654,
          -0.07816742360591888,
          -0.05487919598817825,
          -0.12772023677825928,
          -0.06131880730390549,
          -0.03619838505983353,
          0.01298602670431137,
          0.00047451897989958525,
          0.07428936660289764,
          -0.05432387441396713,
          -0.08494947850704193,
          0.04397943615913391,
          -0.10478661209344864,
          0.003463626140728593,
          -0.08804169297218323,
          -0.009863145649433136,
          -0.006496749818325043,
          -0.040282245725393295,
          0.028458358719944954,
          -0.06305007636547089,
          0.03174896910786629,
          0.0014237355208024383,
          0.027412990108132362,
          -0.003310937900096178,
          -0.017011214047670364,
          0.10019000619649887,
          -0.009112287312746048,
          -9.52080445131287e-05,
          -0.02848656289279461,
          0.003135375212877989,
          -0.08159380406141281,
          0.006036132108420134,
          -0.07852382957935333,
          -4.248953544561118e-08,
          -0.04845342040061951,
          0.03538810834288597,
          0.02408291958272457,
          0.06722056120634079,
          -0.0741155669093132,
          0.01626824587583542,
          -0.12522783875465393,
          -0.007407699711620808,
          0.008096864446997643,
          0.03756542131304741,
          0.0051327478140592575,
          0.06677456200122833,
          -0.05033420771360397,
          0.010839435271918774,
          -0.02251351997256279,
          0.04165780544281006,
          0.02175748720765114,
          0.05560282990336418,
          0.044394370168447495,
          -0.007861997932195663,
          0.075198695063591,
          0.033374324440956116,
          0.029500383883714676,
          -0.06892228871583939,
          0.023317851126194,
          0.01683519035577774,
          -0.08426325023174286,
          0.07930337637662888,
          -0.0010075297905132174,
          0.006468919571489096,
          -0.007265170570462942,
          0.026346247643232346,
          -0.07379626482725143,
          -0.002979748882353306,
          0.07388795912265778,
          0.09848617017269135,
          -0.02691960334777832,
          -0.015202482230961323,
          0.0591152086853981,
          -0.0100201815366745,
          0.08927751332521439,
          -1.748848080751486e-05,
          -0.04843401163816452,
          0.04579606652259827,
          0.07378551363945007,
          -0.0031803471501916647,
          0.0291095282882452,
          -0.07039422541856766,
          0.021015428006649017,
          0.10028282552957535,
          -0.004120469558984041,
          -0.0010367590002715588,
          -0.05458400025963783,
          0.06555405259132385,
          0.03823117911815643,
          -0.01263195089995861,
          -0.016421157866716385,
          0.021249355748295784,
          0.03170682489871979,
          0.030460555106401443,
          0.06609141081571579,
          0.028705399483442307,
          -0.01975102722644806,
          -0.10455497354269028
        ]
      },
      "type": "document"
    },
    {
      "id": "87a1116b-4c66-4d5a-b0d2-b1ae83d565fa",
      "properties": {
        "page_content": "Retrieval Augmented Generation Shahul Es†, Jithin James†, Luis Espinosa-Anke∗♢, Steven Schockaert∗ †Exploding Gradients ∗CardiffNLP, Cardiff University, United Kingdom ♢AMPLYFI, United Kingdom shahules786@gmail.com,jamesjithin97@gmail.com {espinosa-ankel,schockaerts1}@cardiff.ac.uk Abstract We introduce RAGAS (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Aug- mented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the gener- ation itself. With RAGAS, we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations. We posit that such a framework can crucially con- tribute to faster evaluation cycles of RAG archi- tectures, which is especially important given the fast adoption of LLMs. 1 Introduction Language Models (LMs) capture a vast amount of knowledge about the world, which allows them to answer questions without accessing any exter- nal sources. This idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT (Devlin et al., 2019) and became more firmly established with the introduction of ever larger LMs (Roberts et al., 2020). While the most recent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering bench- marks (Bubeck et al., 2023), the idea of using LLMs as knowledge bases still has two fundamen- tal limitations. First, LLMs are not able to answer questions about events that have happened after they were trained. Second, even the largest models struggle to memorise knowledge that is only rarely mentioned in the training corpus (Kandpal et al., 2022; Mallen et al., 2023). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) (Lee et al., 2019; Lewis et al., 2020; Guu et al., 2020). Answering a question then essentially involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM. While initial approaches relied on specialised LMs for retrieval-augmented language modelling (Khandel- wal et al., 2020; Borgeaud et al., 2022), recent work has suggested that simply adding retrieved docu- ments to the input of a standard LM can also work well (Khattab et al., 2022; Ram et al., 2023; Shi et al., 2023), thus making it possible to use retrieval- augmented strategies in combination with LLMs that are only available through APIs. While the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. Automated evaluation of retrieval-augmented systems is thus paramount. In practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e. by mea- suring perplexity on some reference corpus. How- ever, such evaluations are not always predictive of downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is another common evaluation task, but usually only datasets with short extractive an- swers are considered, which may not be represen- tative of how the system will be used. To address these issues, in this paper we present RAGAS1, a framework for the automated assess- 1RAGAS available explodinggradients/ragas. is at https://github.com/ ment of retrieval augmented generation systems. We focus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the usefulness of the retrieved passages. The RAGAS framework provides an integration with both llama- index and Langchain, the most widely used frame- works for building RAG solutions, thus enabling developers to easily integrate RAGAS into their standard workflow. 2 Related Work Estimating faithfulness using LLMs The prob- lem of detecting hallucinations in LLM generated responses has been extensively studied (Ji et al., 2023). Several authors have suggested the idea of predicting factuality using a few-shot prompt- ing strategy (Zhang et al., 2023). Recent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies (Li et al., 2023; Azaria and Mitchell, 2023). Other approaches rely on linking the generated responses to facts from an external knowledge base (Min et al., 2023), but this is not always possible. Yet another strategy is to inspect the probabili- ties assigned to individual tokens, where we would expect the model to be less confident in halluci- nated answers than in factual ones. For instance, BARTScore (Yuan et al., 2021) estimates factuality by looking at the conditional probability of the gen- erated text given the input. Kadavath et al. (2022) use a variation of this idea. Starting from the ob- servation that LLMs provide well-calibrated proba- bilities when answering multiple-choice questions, they essentially convert the problem of validating model generated answers into a multiple-choice question which asks whether the answer is true or false. Rather than looking at the output probabil- ities, Azaria and Mitchell (2023) propose to train a supervised classifier on the weights from one of the hidden layers of the LLM, to predict whether a given statement is true or not. While the approach performs well, the need to access the hidden states of the model makes it unsuitable for systems that access LLMs through an API. For models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed. SelfCheckGPT (Manakul et al., 2023) addresses this problem by instead sam- pling multiple answers. Their core idea is that factual answers are more stable: when an answer",
        "themes": [
          "Retrieval Augmented Generation",
          "Language Models",
          "Knowledge cutoff",
          "Automated evaluation",
          "RAGAS framework",
          "Hallucinations in LLMs",
          "Factuality prediction",
          "Few-shot prompting",
          "Token probabilities",
          "Multiple-choice questions"
        ],
        "entities": [
          "Retrieval Augmented Generation",
          "RAGAS",
          "CardiffNLP",
          "Cardiff University",
          "AMPLYFI",
          "United Kingdom",
          "LLMs",
          "BERT",
          "Roberts et al.",
          "ChatGPT",
          "GPT-4"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "70bcb98e-d2fd-4486-bc9f-f2e7d9abd05b",
      "properties": {
        "page_content": "is factual, we can expect that different samples will tend to be semantically similar, whereas this is less likely to be the case for hallucinated answers. Automated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, GPTScore (Fu et al., 2023) uses a prompt that specifies the consid- ered aspect (e.g. fluency) and then scores passages based on the average probability of the generated tokens, according to a given autoregressive LM. This idea of using prompts was previously also considered by Yuan et al. (2021), although they used a smaller fine-tuned LM (i.e. BART) and did not observe a clear benefit from using prompts. An- other approach directly asks ChatGPT to evaluate a particular aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale (Wang et al., 2023a). Re- markably, strong results can be obtained in this way, although it comes with the limitation of being sensitive to the design of the prompt. Rather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates (Wang et al., 2023b), typically to compare the performance of different LLMs. However, care is needed with this approach, as the order in which the answers is presented can influence the result (Wang et al., 2023b). In terms of how ground truth answers or, more generally, generations, have been typically used in the literature, most approaches have relied on the availability of one or more reference answers. For instance, BERTScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019) use contex- tualised embeddings, produced by a pre-trained BERT model, to compare the similarity between the generated answer and the reference answers. BARTScore (Yuan et al., 2021) similarly uses refer- ence answers to compute aspects such as precision (estimated as the probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer). 3 Evaluation Strategies We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q) and then uses the retrieved context to generate an answer as(q). When building a RAG system, we usually do not have access to human-annotated datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance. First, Faithfulness refers to the idea that the an- swer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer. Indeed, RAG systems are often used in applications where the factual con- sistency of the generated text w.r.t. the grounded sources is highly important, e.g. in domains such as law, where information is constantly evolving. Sec- ond, Answer Relevance refers to the idea that the generated answer should address the actual ques- tion that was provided. Finally, Context Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context passages are too long, LLMs are often less effective in exploiting that context, especially for information that is pro- vided in the middle of the context passage (Liu et al., 2023). We now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2. Faithfulness We say that the answer as(q) is faithful to the context c(q) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S(as(q)). The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step3: Given a question and answer, create one or more statements from each sentence in the given answer. question: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. For each statement si 2https://platform.openai.com 3To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper. in S, the LLM determines if si can be inferred from c(q) using a verification function v(si, c(q)). This verification step is carried out using the following prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format. statement: [statement 1] ... statement: [statement n] The final faithfulness score, F , is then computed as F = |V | |S| , where |V | is the number of statements that were supported according to the LLM and |S| is the total number of statements. Answer relevance We say that the answer as(q) is relevant if it directly addresses the question in an appropriate way. In particular, our assessment of answer relevance does not take into account fac- tuality, but penalises cases where the answer is incomplete or where it contains redundant informa- tion. To estimate answer relevance, for the given answer as(q), we prompt the LLM to generate n potential questions qi based on as(q), as follows: Generate a question for the given answer. answer: [answer] We then obtain embeddings for all questions us- ing the text-embedding-ada-002 model, avail- able from the OpenAI API. For each qi, we cal-",
        "themes": [
          "Factuality of text generation",
          "Automated evaluation of text generation systems",
          "LLMs in text evaluation",
          "Use of prompts in text evaluation",
          "Scoring generated tokens",
          "Aspects of generated text fragments",
          "Fine-tuned LM",
          "ChatGPT in text evaluation",
          "Sensitivity of prompt design",
          "Answer selection among candidates",
          "Influence of answer presentation order",
          "Use of reference answers",
          "Comparison of generated answer and reference answers",
          "Contextualised embeddings",
          "BERTScore",
          "MoverScore",
          "BARTScore",
          "Faithfulness",
          "Answer relevance",
          "Context relevance",
          "RAG systems",
          "Grounded sources",
          "Factual consistency",
          "Cost associated with long context passages",
          "Effectiveness of LLMs with long contexts",
          "Prompting an LLM for evaluation",
          "Faithfulness score computation",
          "Answer relevance estimation",
          "Redundant information in answers",
          "Incomplete answers",
          "Question generation from answers",
          "Embeddings for questions",
          "Text-embedding-ada-002 model"
        ],
        "entities": [
          "BERTScore",
          "Zhang et al.",
          "MoverScore",
          "Zhao et al.",
          "BARTScore",
          "Yuan et al.",
          "RAG",
          "LLMs",
          "gpt-3.5-turbo-16k",
          "OpenAI API"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "22e7ed28-ddea-40b5-96cb-ea74494fd5e6",
      "properties": {
        "page_content": "3 2 0 2 p e S 6 2 ] L C . s c [ 1 v 7 1 2 5 1 . 9 0 3 2 : v i X r a RAGAS: Automated Evaluation of Retrieval Augmented Generation Shahul Es†, Jithin James†, Luis Espinosa-Anke∗♢, Steven Schockaert∗ †Exploding Gradients ∗CardiffNLP, Cardiff University, United Kingdom ♢AMPLYFI, United Kingdom shahules786@gmail.com,jamesjithin97@gmail.com {espinosa-ankel,schockaerts1}@cardiff.ac.uk Abstract We introduce RAGAS (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Aug- mented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the gener- ation itself. With RAGAS, we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations. We posit that such a framework can crucially con- tribute to faster evaluation cycles of RAG archi- tectures, which is especially important given the fast adoption of LLMs. 1 ",
        "themes": [
          "Retrieval Augmented Generation",
          "Automated Evaluation",
          "Natural Language Processing",
          "Language Models",
          "Textual Databases",
          "Hallucinations",
          "RAGAS (Retrieval Augmented Generation Assessment)",
          "Evaluation Metrics",
          "Fast Adoption of Language Models"
        ],
        "entities": [
          "RAGAS",
          "Retrieval Augmented Generation",
          "LLMs",
          "CardiffNLP",
          "Cardiff University",
          "AMPLYFI",
          "United Kingdom",
          "Shahul Es",
          "Jithin James",
          "Luis Espinosa-Anke"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "fe90b907-7f9a-4723-8885-2f93e69d00cb",
      "properties": {
        "page_content": "culate the similarity sim(q, qi) with the original question q, as the cosine between the correspond- ing embeddings. The answer relevance score, AR, for question q is then computed as: AR = 1 n n (cid:88) i=1 sim(q, qi) This metric evaluates how closely the generated answer aligns with the initial question or instruc- tion. Context relevance The context c(q) is consid- ered relevant to the extent that it exclusively con- tains information that is needed to answer the ques- tion. In particular, this metric aims to penalise the (1) inclusion of redundant information. To estimate context relevance, given a question q and its con- text c(q), the LLM extracts a subset of sentences, Sext, from c(q) that are crucial to answer q, using the following prompt: Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\". While extract- ing candidate sentences you’re not al- lowed to make any changes to sentences from given context. The context relevance score is then computed as: CR = number of extracted sentences total number of sentences in c(q) 4 The WikiEval Dataset To evaluate the proposed framework, we ideally need examples of question-context-answer triples which are annotated with human judgments. We can then verify to what extent our metrics agree with human assessments of faithfulness, answer relevance and context relevance. Since we are not aware of any publicly available datasets that could be used for this purpose, we created a new dataset, which we refer to as WikiEval4. To construct the dataset, we first selected 50 Wikipedia pages cov- ering events that have happened since the start of 20225. In selecting these pages, we prioritised those with recent edits. For each of the 50 pages, we then asked ChatGPT to suggest a question that can be answered based on the introductory section of the page, using the following prompt: Your task is to formulate a question from given context satisfying the rules given below: 1. The question should be fully answered from the given context. 2. The question should be framed from a part that contains non-trivial informa- tion. 3. The answer should not contain any 4https://huggingface.co/datasets/ explodinggradients/WikiEval 5That is, beyond the reported training cutoff of the model we used in our experiments. (2) links. 4. The question should be of moderate difficulty. 5. The question must be reasonable and must be understood and responded to by humans. 6. Do not use phrases that ’provided con- text’, etc in the question context: We also used ChatGPT to answer the generated question, when given the corresponding introduc- tory section as context, using the following prompt: Answer the question using the informa- tion from the given context. question: [question] context: [context] All questions were annotated along the three con- sidered quality dimensions by two annotators. Both annotators were fluent in English and were given clear instructions about the meaning of the three considered quality dimensions. For faithfulness and context relevance, the two annotators agreed in around 95% of cases. For answer relevance, they agreed in around 90% of the cases. Disagreements were resolved after a discussion between the anno- tators. Faithfulness To obtain human judgements about faithfulness, we first used ChatGPT to answer the question without access to any additional context. We then asked the annotators to judge which of the two answers was the most faithful (i.e. the standard one or the one generated without context), given the question and corresponding Wikipedia page. Answer relevance We first used ChatGPT to obtain candidate answers with lower answer rel- evance, using the following prompt: Answer the given question in an incom- plete manner. question: [question] We then asked human annotators to compare this answer, and indicate which of the two answers had the highest answer relevance. Context relevance To measure this aspect, we first added additional sentences to the context by scraping back-links to the corresponding Wikipedia page. In this way, we were able to add information to the context that was related but less relevant for Faith. Ans. Rel. Cont. Rel. RAGAs GPT Score GPT Ranking 0.95 0.72 0.54 0.78 0.52 0.40 0.70 0.63 0.52 Table 1: Agreement with human annotators in pairwise comparisons of faithfulness, answer relevance and con- text relevance, using the WikEval dataset (accuracy). answering the question. For the few pages with- out any back-links, we instead used ChatGPT to complete the given context. 5 Experiments Table 1 analyses the agreement between the met- rics proposed in Section 3 and the human assess- ments from the proposed",
        "themes": [
          "Artificial intelligence",
          "Question-answering system",
          "Cosine similarity",
          "Answer relevance",
          "Context relevance",
          "Faithfulness",
          "WikiEval Dataset",
          "ChatGPT",
          "Human judgment",
          "Information redundancy"
        ],
        "entities": [
          "Elon Musk",
          "Tesla",
          "SpaceX",
          "Europe",
          "Asia",
          "Berlin",
          "Shanghai",
          "WikiEval Dataset",
          "ChatGPT",
          "Wikipedia"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "d9cf8710-f871-4eed-a6c1-1feeef9fe9d9",
      "properties": {
        "page_content": "WikiEval dataset. Each WikiEval instance requires the model to compare two answers or two context fragments. We count how often the answer/context preferred by the model (i.e. with highest estimated faithfulness, an- swer relevance, or context relevance) coincides with the answer/context preferred by the human annotators. We report the results in terms of ac- curacy (i.e. the fraction of instances on which the model agrees with the annotators). To put the results in context, we compare our proposed metrics (shown as RAGAs in Table 1) with two baseline methods. For the first method, shown as GPT Score, we ask ChatGPT to assign a score between 0 and 10 for the three quality dimensions. To this end, we use a prompt that describes the meaning of the quality metric and then asks to score the given answer/context in line with that definition. For instance, for evaluating faithfulness, we used the following prompt: Faithfulness measures the information consistency of the answer against the given context. Any claims that are made in the answer that cannot be deduced from context should be penalized. Given an answer and context, assign a score for faithfulness in the range 0-10. context: [context] answer: [answer] Ties, where the same score is assigned by the LLM to both answer candidates, were broken randomly. The second baseline, shown as GPT Ranking, in- stead asks ChatGPT to select the preferred answer/- context. In this case, the prompt again includes a definition of the considered quality metric. For instance, for evaluating answer relevance, we used the following prompt: Answer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question. It penalizes the present of redundant in- formation or incomplete answers given a question. Given an question and answer, rank each answer based on Answer Rele- vancy. question: [question] answer 1: [answer 1] answer 2: [answer 2] The results in Table 1 show that our proposed metrics are much closer aligned with the human judgements than the predictions from the two base- lines. For faithfulness, the RAGAs prediction are in general highly accurate. For answer relevance, the agreement is lower, but this is largely due to the fact that the differences between the two candidate answers are often very subtle. We found context relevance to be the hardest quality dimension to evaluate. In particular, we observed that ChatGPT often struggles with the task of selecting the sen- tences from the context that are crucial, especially for longer contexts. 6 Conclusions We have highlighted the need for automated reference-free evaluation of RAG systems. In par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. is the answer grounded in the retrieved context), answer relevance (i.e. does the answer address the ques- tion) and context relevance (i.e. is the retrieved context sufficiently focused). To support the devel- opment of such a framework, we have introduced WikiEval, a dataset which human judgements of these three different aspects. Finally, we have also described RAGAs, our implementation of the three considered quality aspects. This framework is easy to use and can provide deverlopers of RAG sys- tems with valuable insights, even in the absence of any ground truth. Our evaluation on WikiEval has shown that the predictions from RAGAs are closely aligned with human predictions, especially for faithfulness and answer relevance. References Amos Azaria and Tom M. Mitchell. 2023. The inter- nal state of an LLM knows when its lying. CoRR, abs/2304.13734. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Si- monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal- timore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 2206–2240. PMLR. Sébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al. 2023. Sparks of artificial general intelli- gence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. CoRR, abs/2302.04166. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International confer- ence on machine learning, pages 3929–3938. PMLR. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of halluci- nation in natural language generation. ACM Comput- ing Surveys, 55(12):1–38. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jack- son Kernion, Shauna Kravec, Liane Lovitt, Ka- mal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. CoRR, abs/2207.05221. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022. Large language models struggle to learn long-tail knowledge. CoRR, abs/2211.08411. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Omar Khattab, Keshav Santhanam, Xiang",
        "themes": [
          "WikiEval dataset",
          "Automated reference-free evaluation",
          "RAG systems",
          "Faithfulness",
          "Answer relevance",
          "Context relevance",
          "ChatGPT",
          "Quality dimensions",
          "Evaluation framework",
          "WikiEval dataset",
          "RAGAs",
          "Language models",
          "Trillions of tokens",
          "Artificial general intelligence",
          "BERT",
          "Gptscore",
          "Retrieval augmented language model pre-training",
          "Hallucination in natural language generation",
          "Long-tail knowledge"
        ],
        "entities": [
          "WikiEval dataset",
          "RAGAs",
          "ChatGPT",
          "faithfulness",
          "answer relevance",
          "context relevance",
          "LLM",
          "ground truth",
          "BERT",
          "Gptscore"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "fade49b4-bbca-49fe-81ab-331e2730ae42",
      "properties": {
        "page_content": "Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP. CoRR, abs/2212.14024. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open do- main question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096. Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik- tus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neu- ral Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: A large- scale hallucination evaluation benchmark for large language models. CoRR, abs/2305.11747. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 9802–9822, Toronto, Canada. Association for Computational Linguistics. Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. Selfcheckgpt: Zero-resource black-box hal- lucination detection for generative large language models. CoRR, abs/2303.08896. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of fac- tual precision in long form text generation. CoRR, abs/2305.14251. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented lan- guage models. CoRR, abs/2302.00083. Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the param- eters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418–5426, Online. Association for Computational Linguistics. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: retrieval-augmented black-box language models. CoRR, abs/2301.12652. Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxi- ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good NLG evaluator? A preliminary study. CoRR, abs/2303.04048. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. CoRR, abs/2305.17926. Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, and Mohit Iyyer. 2023c. KNN-LM does not improve open-ended text generation. CoRR, abs/2305.14625. Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text genera- tion. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Informa- tion Processing Systems 2021, NeurIPS 2021, De- cember 6-14, 2021, virtual, pages 27263–27277. Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James R. Glass. 2023. Interpretable unified language checking. CoRR, abs/2304.03728. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu- ating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe- view.net. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized em- beddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 563–578, Hong Kong, China. Association for Computational Lin- guistics. A Examples from WikiEval Tables 2, 3 and 4 show examples from the WikiEval dataset, focusing in particular on answers with high and low faithfulness (Table 2), high and low answer relevance (Table 3), and high and low context rele- vance (Table 4). Question Context Answer Who directed the film Op- penheimer and who stars as J. Robert Oppenheimer in the film? Oppenheimer is a 2023 biographical thriller film written and directed by Christopher Nolan. Based on the 2005 biography American Prometheus by Kai Bird and Mar- tin J. Sherwin, the film chronicles the life of J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Man- hattan Project, and thereby ushering in the Atomic Age. Cillian Murphy stars as Oppenheimer, with Emily Blunt as Oppenheimer’s wife Katherine \"Kitty\" Oppenheimer. High Faithfulness: Christopher Nolan directed the film Oppen- heimer. Cillian Murphy stars as J. Robert Oppenheimer in the film. Low Faithfulness: James Cameron directed the film Op- penheimer. Tom Cruise stars as J. Robert Oppenheimer in the film. Table 2: Example from WikiEval, showing answers with high and low faithfulness. Question Answer When is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from? High answer relevance: The PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India. Low answer relevance: The scheduled launch date and time for the PSLV-C56 mission have not been provided.The PSLV-C56 mission is an important space mission for India. It aims to launch a satellite into orbit to study weather patterns. Table 3: Example from WikiEval, showing answers with high and low answer relevance. Question Context When was the Chimnabai Clock Tower completed, and who was it named af- ter? High context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. Low context relevance: The Chimnabai Clock Tower, also",
        "themes": [
          "Natural Language Processing",
          "Knowledge-intensive NLP tasks",
          "Retrieval-augmented language models",
          "Language model evaluation",
          "Hallucination in language models",
          "Long context in language models",
          "Automatic evaluation of generated text",
          "BERT model",
          "Faithfulness of generated text",
          "Answer relevance"
        ],
        "entities": [
          "Lisa Li",
          "David Hall",
          "Percy Liang",
          "Christopher Potts",
          "Matei Zaharia",
          "Kenton Lee",
          "Ming-Wei Chang",
          "Kristina Toutanova",
          "Patrick S. H. Lewis",
          "Ethan Perez"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "578608c9-b4ef-4be7-9337-0090787a2a45",
      "properties": {
        "page_content": "Explaining Spectrograms in Machine Learning: A Study on Neural Networks for Speech Classification Jesin James1, Balamurali B. T.2, Binu Abeysinghe1, and Junchen Liu1 1 Department of Electrical, Computer, and Software Engineering, The University of Auckland, New Zealand jesin.james@auckland.ac.nz, 2 Singapore University of Technology and Design, Singapore Abstract. This study investigates discriminative patterns learned by neural networks for accurate speech classification, with a specific focus on vowel classification tasks. By examining the activations and features of neural networks for vowel classification, we gain insights into what the networks “see” in spectrograms. Through the use of class activation map- ping, we identify the frequencies that contribute to vowel classification and compare these findings with linguistic knowledge. Experiments on a American English dataset of vowels showcases the explainability of neural networks and provides valuable insights into the causes of misclassifica- tions and their characteristics when differentiating them from unvoiced speech. This study not only enhances our understanding of the under- lying acoustic cues in vowel classification but also offers opportunities for improving speech recognition by bridging the gap between abstract representations in neural networks and established linguistic knowledge. Keywords: Spectrograms, Linguistics, Explainable Speech Recognition, Interprettable, Activation Maps, Vowels 1 Introduction In recent years, the field of speech recognition has experienced remarkable progress, primarily driven by the widespread adoption of deep neural networks (DNNs) to train speech recognition models. The successful application of DNNs in speech recognition has led to significant advancements in various domains such as au- tomatic speech recognition, voice assistants, and language understanding. Spec- trograms, which provide a visual representation of the frequency content of a speech signal as it evolves over time, offer a promising alternative to conven- tional speech representations in the context of DNN-based speech recognition. Convolutional neural network (CNN) is a type of DNNs originally designed for image processing. However, CNNs have been successfully adapted to process spectrograms, capturing temporal dependencies and extracting meaningful fea- tures. Also, the field of computer vision using neural networks has progressed extensively with large networks such as ResNet [7], VGGNet [22], DenseNet 2 Jesin James et al. [10] and similar trained on large image datasets. Some of these large networks have been used for speech recognition tasks with spectrograms as the input. For example, ResNet, with its ability to handle deep architectures and alleviate the vanishing gradient problem, has also been explored for speech tasks such as speech recognition [23,27], keyword spotting [24] and emotion recognition [25]. The choice of using spectrograms in speech recognition holds several advan- tages. Spectrograms effectively capture both the temporal and spectral informa- tion contained in speech, offering a comprehensive representation that enables the model to discern important acoustic cues for various application. Addition- ally, spectrograms provide a visual understanding of the speech signal, allowing researchers to interpret and analyze the underlying speech patterns more intu- itively. However, despite the benefits, one critical challenge arises: the lack of understanding regarding what the model is learning from spectrograms. While humans can manually annotate and recognize speech components from spectro- grams, the extent to which a model is learning the same features is not always clear. This poses the following issues for current and future research on speech recognition tasks: Interpretability and Performance Improvements: Despite the impres- sive results achieved by these models, they are often regarded as black boxes, which limits researchers and practitioners from gaining a deep understanding of the specific features and patterns that contribute to their decision-making pro- cess. This lack of transparency and interpretability hinders further improvements in model performance. Unoptimised Model Training: The process of human annotation of spec- trograms relies heavily on linguistic insights, allowing annotators to focus on spe- cific aspects relevant to speech analysis. However, many DNNs used in speech recognition are not optimized using linguistics knowledge, leading to unopti- mised model training. These neural networks often treat spectrograms as mere ‘images’, lacking a comprehensive understanding of the frequency axis and its significance in speech analysis. As a result, the model’s ability to refine and optimize speech recognition is limited. This study addresses the above knowledge gaps by investigating what neural networks learn from spectrograms. Focusing on two specific problems, namely vowel classification and voiced-unvoiced classification, we aim to unravel the black box nature of neural networks trained on spectrograms. The main contri- butions of this paper are: 1. Designing experiments to explore the relationship between neural networks’ learning from spectrograms and human interpretation of spectrograms. 2. Employing visualising techniques to identify the regions of a spectrogram that are considered ‘important’ by DNNs in specific speech recognition tasks. 3. Explaining the results to uncover insights into DNN’s understanding of spec- trograms in relation to human interpretation of the same. In this study, interpretation, refers mapping an abstract concept, like a pre- dicted class, into a human-understandable form such as images or texts. An explanation consists of interpretable features that contributed to a classification Explaining Spectrograms in Machine Learning Fig. 1. Spectrograms of five American English Vowels, which are voiced sounds and unvoiced sound /s/. or regression decision. An example is a heatmap highlighting the pixels in an image that strongly support the decision [16].",
        "entities": [
          "Elon Musk",
          "Tesla",
          "SpaceX",
          "Europe",
          "Asia",
          "Berlin",
          "Shanghai",
          "Spectrograms",
          "Machine Learning",
          "Neural Networks"
        ],
        "themes": [
          "Artificial Intelligence",
          "Neural Networks",
          "Speech Classification",
          "Spectrograms",
          "Vowel Classification",
          "Activation Mapping",
          "Interpretability",
          "Model Training",
          "Deep Learning",
          "Speech Recognition"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "2c0f8d8a-91f4-4a56-a846-05b446f14b26",
      "properties": {
        "page_content": "2.1 Spectrograms and their Significance in Linguistics Spectrograms provide visual representations of speech signal frequencies over time. They are crucial in linguistics for analyzing acoustic properties of speech and offer detailed insights into temporal and spectral characteristics, allowing researchers to study articulatory gestures and acoustic cues. In phonetics, spectrograms are instrumental in studying speech and its pro- duction [4]. They provide a tool to analyze the fine-grained details of articulatory movements, such as formant patterns and consonant releases [28]. Spectrograms allow linguists to investigate phonetic features like voicing, place and manner of articulation, and vowel quality, helping in the characterization and classification of speech sounds across languages [6,29]. There are even spectrogram reading competitions in conferences such as International Phonetics Association Confer- ence and Australasian Speech Science and Technology Association Conference. In phonology, spectrograms aid in understanding phonological processes and patterns [13]. They facilitate the identification of phonemic contrasts, allophonic variations, and sociolinguistic phenomena such as regional accents and dialectal variations [8]. Spectrograms also aid in the study of language variation, speech disorders, language acquisition, and cross-linguistic differences in phonetic pat- terns. While spectrograms have limitations in capturing prosody, intonation, and 3 4 Jesin James et al. discourse structure, they remain highly significant in linguistics as a powerful tool for analyzing speech sounds. Voiced and Unvoiced Speech Voiced speech is produced when the vocal folds vibrate, resulting in a periodic airflow. They include sounds such as vowels and voiced consonants. In spectrograms, voiced sounds exhibit a characteristic pattern as seen in Fig. 1(a) to (e). They display regular bands of energy, known as formants, which represent the resonant frequencies of the vocal tract. Voiced speech has a harmonic structure and exhibit sustained energy throughout their duration. Unvoiced sounds are produced without vocal fold vibration. They include voiceless consonants. The spectrograms of unvoiced sounds lack a clear har- monic structure and exhibit a more random and dispersed distribution of energy across a wide range of frequencies, as seen in Fig. 1(f). Unvoiced sounds are characterized by transient bursts of energy concentrated around the onset and release of the sound [20]. Vowel Sounds The combination of formant information and supplementary spectral features in spectrograms enables linguists to distinguish and analyze vowel sounds in their linguistic investigations. Formants correspond to the res- onant frequencies of the vocal tract during vowel production. By observing the positioning, spacing, and relative intensity of these formants in the spectrogram, linguists can identify and categorize different vowel sounds. For example, in Fig. 1 (a) to (e), the vowels have distinct formant patterns. The first formant of /i/ is 385 Hz, /u/ is 400 Hz, /æ/ is 800 Hz, /Ç/ in 590 Hz and /A/ is 710 Hz (formant estimation was done by observing the spectrograms and verified using Praat [2]). These values are within the frequency range expected for American English [17]. Differences exist for the second, third and fourth formants too, as seen by the formant bands at different frequencies in the Fig.. Spectrograms offer insights into acoustic cues related to vowel articulation, including vowel duration and spectral shape. The shape of the spectral pattern in a vowel is influenced by the tongue position and the openness of the oral cavity, which determine the shape and configuration of the vocal tract. High vowels like /i/ and /u/ typically exhibit a more concentrated spectral shape with higher energy in the higher frequency range (See 1 (a) and (e)). This results from the tongue being positioned closer to the roof of the mouth, giving rise to a narrower constriction in the vocal tract and emphasizing higher-frequency resonances. The distinction in vowel duration can be observed using the spectrogram’s horizontal axis. E.g., comparing the horizontal axis of Fig. 1 (a) and (c), we can observe /i/ is a longer vowel than /Ç/. 2.2 Explaining what Deep Neural Networks Learn Researchers have employed various methods to gain insights into the learning process of deep neural networks [16]. Visualization techniques, such as highlight- ing the important areas in an image for a specific prediction, are commonly used [21,15]. Other approaches include sensitivity analysis, Taylor decomposition, and backward propagation techniques [16]. Explaining Spectrograms in Machine Learning Class activation maps (CAMs) are visualization techniques to explain the decision-making process of deep neural networks, specifically in CNNs for image classification tasks [26,21,15]. CAMs provide valuable insights into the influential regions within an image that contribute to predicting a particular class label. By leveraging the gradients during the backward propagation process, CAMs capture the importance of spatial locations, highlighting the regions that sig- nificantly influence the final classification decision. These maps have proven to be effective in explaining the reasoning behind deep learning models, allowing researchers and practitioners to comprehend which areas of an image play a crucial role in making accurate predictions. CAMs offer a visual explanation by generating heatmaps that emphasize the relevant regions responsible for the classification decision. Explanations of deep learning models have been widely explored in vari- ous domains, including image classification, pattern recognition [1], and medical applications [9,19]. While activation mapping has been extensively applied in visual recognition tasks, its direct application to speech recognition is less com- mon. One example is the use of CAM to explain the results of detecting oral cancer speech using a ResNet-based classifier with spectrograms as input [5]. In this study, we propose adapting the concept of CAMs to spectrograms with the aim of identifying the specific frequency regions that provide the most informative cues for speech classification tasks.",
        "themes": [
          "Spectrograms",
          "Linguistics",
          "Acoustic properties of speech",
          "Articulatory gestures",
          "Phonetics",
          "Phonology",
          "Voiced and unvoiced speech",
          "Formants",
          "Vowel sounds",
          "Deep neural networks",
          "Machine learning",
          "Class activation maps",
          "Speech recognition",
          "Frequency regions"
        ],
        "entities": [
          "Spectrograms",
          "linguistics",
          "speech signal frequencies",
          "phonetics",
          "acoustic properties of speech",
          "temporal and spectral characteristics",
          "articulatory gestures",
          "voicing",
          "place and manner of articulation",
          "vowel quality"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "04e98094-99b0-4e5b-9714-3d1476139982",
      "properties": {
        "page_content": "3 Methodology 3.1 Database The LJSpeech corpus ([11]) was selected as the database for training and eval- uating the classification models in this study. This corpus consists of American English recordings by a speaker who identifies as female, along with text tran- scriptions, all sampled at 22,050 Hz. To align the recordings with their respective transcriptions WebMAUS [12] was utilized with American English option. We limit the scope of the study to five vowels chosen to span over the Amer- ican English vowel space [14]: /i/ (high, front), /æ/ (low, front), /Ç/ (mid), /A/ (low, back) and /u/ (high, back). For each vowel, start and end times were identified, and the appropriate segments were extracted. The resulting dataset comprised a total of 79,269 single-vowel recordings. Unvoiced consonants were also extracted from LJSpeech corpus. The selected consonants were /p/, /t/, /k/, /f/, /s/, /tS/, /S/, /θ/. Only 79,269 instances of these consonants were included to match the number of vowels. 5 6 Jesin James et al. 3.2 Experiment Design The methodology encompasses three experiments: 1. Vowel classification using all frequency components present in the speech signal 2. Vowel classification focusing on the region containing formants, i.e. 4000 Hz 3. Voiced vs unvoiced classification focusing on the region containing for- mants, i.e. 4000 Hz The first experiment assesses the classification model’s ability to identify rel- evant patterns in the spectrogram using speech in LJSpeech corpus considering all frequency components present, i.e. upto sampling frequency/2 = 11,025 Hz. Due to linguistics knowledge that the first four formants of the selected vowels have frequency less than 4000 Hz, the second experiment restricts the maxi- mum frequency to 4000 Hz. This limitation narrows down the scope of visual representation provided to the network, allowing for a more focused analysis of vowel characteristics already used by linguists. The final experiment inves- tigates the network’s capability to accurately distinguish between voiced and unvoiced sounds. From linguistic knowledge, we know that the existence of fun- damental frequency is a distinguishing feature between these two categories [18]. This experiment aims to examine whether the model would accurately identify fundamental frequency along with other relevant frequency of importance in differentiating between voiced and unvoiced sounds. 3.3 Classification Model Training For this study, the ResNet-101 model was used. ResNet-101 is an enhanced ver- sion of the original ResNet [7] with 101-layers, addressing issues related to net- work depth and degradation by employing residual learning frameworks. ResNet- 101 already pretrained on the ImageNet dataset [3] was subsequently fine-tuned using the vowel and consonant datasets mentioned earlier. Three instances of ResNet-101 were fine-tuned for this purpose, one for each experiment employing a 70/30 train-test split, each using softmax activation function to make the final decision. The hyperparameters remained consistent with the base ResNet-101 model, with a batch size of 32, learning rate of 0.0001, and Adam as the op- timizer. The training process was conducted on a local machine equipped with two Nvidia RTX 3090 GPUs, each possessing 24GB of VRAM. Training time amounted to approximately 3.5 hours each for Experiment 1, 2 and 6 hours for Experiment 3. 3.4 Class Activation mapping (CAMs) The three models were compared using CAMs, using the approach reported in [26]. CAMs provide insight into how the model’s focus shifts when presented with different inputs while keeping the model architecture consistent. To generate the CAMs, first the spectrograms for each class were resized to 224 × 224. Then, the Explaining Spectrograms in Machine Learning dot product between the final convolutional layer’s feature for a class and the softmax weights of the output layer was calculated. The result was normalized and scaled to the same resolution. Finally, the resulting array was transformed into a heatmap. The heatmap was then superimposed on the spectrogram of a speech signal to obtain the CAMs in Figures 2, 3, 4. The code for training ResNet-101 and generating CAMs is made available3. 4 Results and Discussion 4.1 Vowel classification using all Frequency Components Class Activation Map Analysis The CAM analysis provides insights into the discriminative properties of the considered vowels as seen in Fig. 2). Darker red regions in the CAM, indicating higher importance, were predominantly observed in the high-frequency region for three vowels: frequency > 5500 Hz for /i and /u/, but > 3500 Hz for /æ/ (Fig. 2 (a), (b), (e)). Among these, the vowel /i/ is observed to be the darkest in the high-frequency region, followed by /æ/ and /u/. These observations suggest that high-frequency components play a crucial role in distinguishing these vowels. The presence of high energy in the high- frequency region of their spectrograms in Fig. 1 (a), (e) likely contributes to their distinctiveness in the CAMs. This is also expected as both /i/ and /u/ are high vowels having high energy in high frequency range. Interestingly, for the /æ/ vowel in Fig. 2 (b), an additional region of impor- tance was identified in the low-frequency range. Specifically, a strong band of frequencies between 500 and 1000 Hz exhibited a darker region in the CAM. This finding indicates the potential role of this frequency band in predicting the presence of the /æ/ vowel and this region corresponds to the first formant of /æ/, as seen in Fig. 1 (b) . In contrast, the /Ç/and /A/ displayed similar characteristics in the high- frequency region (frequency > 5000 Hz) as in Fig. 2 (c) and (d), with no promi- nent dark red heatmap. However, in the low-frequency range of 500 to 1000 Hz, as seen in Fig. 2 (c) the vowel /Ç/ demonstrated a darker region that appeared in the latter half of the spectrogram. On the other hand, the vowel /A/ exhibited a similar trend, albeit with the frequency of interest slightly higher, ranging from 1000 to 3500 Hz. Confusion Matrix Analysis Analysis of the confusion matrix revealed a high overall accuracy > 96% as seen in the resulting confusion matrix in Fig. 2 (f). However, some minor misclassifications were observed. The vowel /A/ misclas- sified as /i/",
        "themes": [
          "LJSpeech corpus",
          "Vowel classification",
          "Frequency components",
          "Formants",
          "Voiced vs unvoiced classification",
          "ResNet-101 model",
          "Class Activation Mapping (CAMs)",
          "Spectrogram analysis",
          "High-frequency components",
          "First formant"
        ],
        "entities": [
          "LJSpeech corpus",
          "American English",
          "WebMAUS",
          "formants",
          "4000 Hz",
          "ResNet-101",
          "ImageNet dataset",
          "softmax activation function",
          "Nvidia RTX 3090",
          "Class Activation mapping"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "64b09631-504d-4ad9-8d10-09108a97e430",
      "properties": {
        "page_content": "was found to be the lowest, which can be attributed to the disjoint nature of their CAMs. The distinct patterns in the CAMs for /A/ and /i/ con- tribute to their accurate discrimination seen in Fig. 2 (a) and (d). Misclassifications were observed among the vowels /Ç/, /A/, and /u/. Some of /Ç/ were misclassified as /A/ and /u/, and similar misclassification were observed in /A/ v.s. /Ç/ and /u/ v.s. /Ç/. This similarity in misclassifications 3 https://github.com/MaoriEnglish-Codeswitch/Vowel Classification 7 8 Jesin James et al. Fig. 2. CAMs and Confusion Matrix for classifying five vowels with maximum fre- quency (Sampled at 22050 Hz)). Fig. 3. CAMs and Confusion Matrix for classifying five vowels with frequency limited to 4000 Hz. Fig. 4. CAMs and Confusion Matrix for classifying voiced and unvoiced speech with maximum frequency = 4000 Hz. Explaining Spectrograms in Machine Learning can be attributed to the shared characteristics observed in the low-frequency region of the CAMs for /Ç/, /A/, and /u/ as seen Fig. 2 c, d and e, respectively. Although /Ç/and /u/ appear almost disjoint in their activation maps, a small overlap in the region of importance in the low frequency range of 500 to 1000 Hz, around the lower-middle part as seen in Fig. 2 (c), (e) could potentially result in the misclassifications. Similar misclassifications were also observed between /A/ and /u/ , as well as /A/ and /æ/. However, these misclassifications cannot be easily explained using the CAMs of /A/ vs /u/ or /A/ vs /æ/. However, there is a small region of overlap in the latter case, specifically in the 1000 to 3500 Hz region towards the latter part of the CAM, which could have partially accounted for the misclassifications. A number of Vowel /i/ was misclassified as /Ç/ and /u/. Surprisingly, this misclassification response was found to be non-symmetric for /Ç, i.e., the number of /Ç/ misclassified as /i/ is marginally less however for the case of /A/ and /u/, there are still misclassification and the misclassification is symmetric. The misclassification between /i/ and /Ç/ cannot be explained using the CAMs alone. However, the similarity between the activation maps of /i/ and /u/ in the high frequency region could contribute to the higher number of misclassifications. The non-symmetric misclassification (for e.g. observed between /Ç/ and /æ/), could be attributed to mislabelling or possibly noisy speech data needing further investigation. 4.2 Vowel Classification focusing on Region of Formants As seen in Fig. 3, the comparison between CAMs obtained by focusing on the region upto 4000 Hz and ones with all frequencies revealed some differences in the importance regions. For /æ/, the CAM remained similar between the two. For vowel /Ç/, a similarity was observed in the CAM’s shape. But the im- portance seen in the low frequency region for when considering all frequencies shifted upward. The red region shifted from 500 Hz - 1000 Hz in Fig. 2 (c) to 1500 Hz - 2500 Hz in Fig. 3 (c). On the other hand, for the vowel /u/ and /i/, a new region of importance emerged in the low frequency region around 500 to 1000 Hz. The CAM for the vowel /A/ exhibited significant differences between the two cases. The region of importance appeared completely swapped between the original sampling frequency case and the frequency limited cases. Despite the subtle differences observed in the CAM between the original and frequency limiting cases, the resulting confusion matrices showed remarkable similarity. Misclassifications of /A/ as /i/ still resulted in the lowest number, possibly due to the presence of a region of high importance in /A/ as a strong band observed around frequencies < 1000 Hz in /A/ but not in /i/). However, similarities were observed in the high-frequency region of their CAMs. The vowel /æ/ and /A/ showed similar CAMs, and it was not surprising to find a slightly higher misclassification rate between /æ/ and /A/ and vice versa in this investigation. Interestingly, /Ç/ and /u/ exhibited disjoint CAM, 9 10 Jesin James et al. with minimal overlap in the high importance regions at both low and high fre- quency regions. However, their overlap in the mid-frequency region could have contributed to a high number of misclassifications in the frequency limited case. 4.3 Voiced vs Unvoiced Classification CAMs were utilized to investigate the region of importance in the spectrograms of voiced speech in distinguishing them from unvoiced speech. The results are shown in Fig. 4. The CAMs revealed that the region of importance for unvoiced speech pre- dominantly lies below 700 Hz . Notably, this frequency range corresponds to the general location of fundamental frequency, that are typically absent in non- voiced speech. Though this location is contrary to expectation as discussed in Section 3, the model’s choice to focus on this disjoint spectral region indicates its discriminative power in accurately distinguishing between voiced and non-voiced speech. This observation is further supported by the excellent performance re- flected in the confusion matrix as seen Fig. 3 (f), where the model achieved an accuracy exceeding 98%. Although there were some misclassifications observed between voiced and non-voiced speech samples, they were minimal. Discussion Observing all the CAMs, when all frequencies are available to the network to make decisions, the high vowels /i/ and /u/ used both the formant region (< 4000 Hz) and spectral shape characteristics to make decisions. The low-back /A/ and mid vowel /Ç/ use only the formant region, while /æ/ used both the formant region (< 4000 Hz) and spectral shape characteristics to make decisions. When the frequency range was limited to 4000 Hz, majority of the vowels focused on the first and second formant frequency regions, which is less than 1500 Hz for /i/, /A/ and /æ/. However, /u/ focused on a narrower low frequency < 1000 Hz, where this vowel’s first and second formants fall. /Ç/ assigns high importance to a region around 2000 Hz, which corresponds to its third formant. Observing the CAMs in the voiced vs unvoiced detection, it is clear that the",
        "themes": [
          "Artificial intelligence",
          "CAMs",
          "Vowel classification",
          "Misclassifications",
          "Formants",
          "Spectrograms",
          "Voiced and unvoiced speech",
          "Frequency analysis",
          "Feature importance",
          "Speech recognition"
        ],
        "entities": [
          "Elon Musk",
          "Tesla",
          "SpaceX",
          "Europe",
          "Asia",
          "Berlin",
          "Shanghai",
          "CAMs",
          "Vowel Classification",
          "Jesin James"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "e508186a-68d0-4e14-bf4d-82b9eb120d8c",
      "properties": {
        "page_content": "region of importance corresponds to where the first four formants of voiced speech would lie in. Overall, this analysis revealed that the neural network which was not pre- trained on spectrograms, but only fine-tuned on them are focusing on formants in most cases to make their decisions. This is similar to what linguists would do. However, high frequencies are also considered in the decision making in some cases, which may not be needed depending on the task at hand. These high frequencies maybe a result of noise in the database. Based on this observation, there is scope to inform deep learning models on the region of interest to consider based on linguistics knowledge. 5 Conclusion In conclusion, this study explored the interpretation of spectrograms in machine learning for speech classification. The prediction results demonstrated higher accuracy in vowel classification models trained using ResNet-101. Insights were Explaining Spectrograms in Machine Learning gained into what these networks ”see” in spectrograms as classification cues. However, CAM alone fell short in explaining misclassifications for certain vow- els, highlighting challenges in capturing higher-level semantic concepts and ab- stract reasoning. The study also revealed unique activation characteristics in neural networks when distinguishing between voiced and unvoiced speech, fo- cusing on formant regions. CAM served as an initial tool for interpretability, revealing regions contributing significantly to predictions. Future work should explore techniques like filter visualization, gradient-based visualization, activa- tion maximization, occlusion analysis to improve understanding of deep learning models in speech classification. These techniques hold potential for unraveling network complexities, addressing higher-level semantic concept capture, and ad- vancing interpretability and performance in speech classification tasks. References 1. Bai, X., Wang, X., Liu, X., Liu, Q., Song, J., Sebe, N., Kim, B.: Explainable deep learning for efficient and robust pattern recognition: A survey of recent develop- ments. Pattern Recognition 120 (2021) 108102 2. Boersma, P.: Praat, a system for doing phonetics by computer. Glot. Int. 5(9) (2001) 341–345 3. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large- Scale Hierarchical Image Database. In: CVPR09. (2009) 4. Flanagan, J.L.: Speech analysis synthesis and perception. Volume 3, pp: 151-155. Springer Science & Business Media (2013) 5. Halpern, B.M., van Son, R., Brekel, M.v.d., Scharenborg, O.: and analysing spontaneous oral cancer speech in the wild. arXiv:2007.14205 (2020) Detecting arXiv preprint 6. Hatazaki, K., Komori, Y., Kawabata, T., Shikano, K.: Phoneme segmentation using spectrogram reading knowledge. In: International Conference on Acoustics, Speech, and Signal Processing,. (1989) 393–396 vol.1 7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2016) 770–778 8. Holmes, J., Hazen, K.: Research methods in sociolinguistics: A practical guide. John Wiley & Sons (2013, pp: 119-130) 9. Holzinger, A., Langs, G., Denk, H., Zatloukal, K., M¨uller, H.: Causability and explainability of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 9(4) (2019) e1312 10. Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2017) 4700–4708 11. Ito, K., Johnson, L.: LJ-Speech-Dataset/ (2017) 11. Ito, K., Johnson, L.: LJ-Speech-Dataset/ (2017) The lj speech dataset. 12. Kisler, T., Reichel, U., Schiel, F.: Multilingual processing of speech via web services. Computer Speech & Language 45 (September 2017) 326–347 13. Ladefoged, P.: A course in phonetics. Volume 3, pp: 33-52. Harcourt College Publishers (1975) 14. Ladefoged, P.: ”American English”. Handbook of the International Phonetic As- sociation. Volume pp: 41–44. Cambridge: Cambridge University Press. (1999) 11 12 Jesin James et al. 15. Landecker, W., Thomure, M.D., Bettencourt, L.M., Mitchell, M., Kenyon, G.T., Brumby, S.P.: Interpreting individual classifications of hierarchical networks. In: 2013 IEEE symposium on computational intelligence and data mining (CIDM), IEEE (2013) 32–38 16. Montavon, G., Samek, W., M¨uller, K.R.: Methods for interpreting and understand- ing deep neural networks. Digital signal processing 73 (2018) 1–15 17. Rabiner, L.R.: Digital processing of speech signals, pp: 45, 46. Pearson Education India (1978) 18. Rose, P.: Forensic speaker identification. cRc Press (2002) 19. Roy, S., Menapace, W., Oei, S., Luijten, B., Fini, E., Saltori, C., Huijben, I., Chen- nakeshava, N., Mento, F., Sentelli, A., et al.: Deep learning for classification and localization of covid-19 markers in point-of-care lung ultrasound. IEEE transac- tions on medical imaging 39(8) (2020) 2676–2687 20. Russel, K.: Identifying sounds in spectrograms. https://home.cc.umanitoba.ca/ ∼krussll/phonetics/acoustic/spectrogram-sounds.html (2005) [Online; accessed 05- July-2023]. 21. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional net- works: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034 (2013) 22. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014) 23. Tang, R., Lin, J.: Deep residual learning for small-footprint keyword spotting. In: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE (2018) 5484–5488 24. Tang, R., Lin, J.: Deep residual learning for small-footprint keyword spotting. In: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE (2018) 5484–5488 25. Triantafyllopoulos, A., Keren, G., Wagner, J., Steiner, I., Schuller, B.: Towards robust speech emotion recognition using deep residual networks for speech en- hancement. (2019) 26. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features for discriminative localization. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2016) 2921–2929 27. Zou, C., Luo, J., Huang, C.: End to end speech recognition based on resnet-blstm. Computer Life (CPL) ISSN: 1819-4818 (2020) 28. Zue, V., Cole, R.: Experiments on spectrogram reading. In: ICASSP ’79. IEEE International Conference on Acoustics, Speech, and Signal Processing. Volume 4. (1979) 116–119 29. Zue, V., Lamel, L.: An expert spectrogram reader: A knowledge-based approach to speech recognition. In: ICASSP ’86. IEEE International Conference on Acoustics, Speech, and Signal Processing. Volume 11. (1986) 1197–1200",
        "themes": [
          "Interpretation of spectrograms",
          "Machine learning for speech classification",
          "ResNet-101 model",
          "Vowel classification",
          "Formants in voiced speech",
          "Decision making in neural networks",
          "High frequency consideration",
          "Linguistics knowledge in deep learning models",
          "Techniques for understanding deep learning models",
          "Speech classification tasks"
        ],
        "entities": [
          "Europe",
          "Asia",
          "Berlin",
          "Shanghai",
          "ResNet-101",
          "vowel classification",
          "formants",
          "voiced speech",
          "unvoiced speech",
          "deep learning models"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "90d0e02c-28b2-4a5f-a21d-5af0cf8e645f",
      "properties": {
        "page_content": "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models Zara Siddique∗, Liam D. Turner∗, Luis Espinosa-Anke∗† ∗School of Computer Science and Informatics, Cardiff University, United Kingdom †AMPLYFI, United Kingdom {siddiquezs2,turnerl9,espinosa-ankel}@cardiff.ac.uk Abstract Large language models (LLMs) have been shown to propagate and amplify harmful stereo- types, particularly those that disproportion- ately affect marginalised communities. To un- derstand the effect of these stereotypes more comprehensively, we introduce GlobalBias, a dataset of 876k sentences incorporating 40 dis- tinct gender-by-ethnicity groups alongside de- scriptors typically used in bias literature, which enables us to study a broad set of stereotypes from around the world. We use GlobalBias to directly probe a suite of LMs via perplex- ity, which we use as a proxy to determine how certain stereotypes are represented in the model’s internal representations. Following this, we generate character profiles based on given names and evaluate the prevalence of stereotypes in model outputs. We find that the demographic groups associated with vari- ous stereotypes remain consistent across model likelihoods and model outputs. Furthermore, larger models consistently display higher levels of stereotypical outputs, even when explicitly instructed not to. 1 Introduction LLMs are increasingly used for tasks that span ar- eas of concern for bias and fairness (Anthis et al., 2024), such as user discrimination in recommenda- tions (Xu et al., 2023). Despite the obvious need for ethical frameworks around these models, these are mostly lacking or incomplete, and make research into fairness and bias essential for supporting pub- lic confidence in the use of generative AI. While bias is often defined in ambiguous and conflict- ing ways (Blodgett et al., 2020), in this paper we focus on representational harms, defined by Craw- ford (2017) as harms that “occur when systems reinforce the subordination of some groups along the lines of identity,” specifically harms caused by stereotyping. Existing research on stereotypes in LMs is lim- ited, and predominantly focuses on African Ameri- Attribute Name Age Personality Traits Negative Traits Hobbies Occupation Height Hair Colour Eye Colour Skin Colour Build Socioeconomic Status Sexual Orientation Religion Details Kazuyo 45 Calm, Wise, Observant Perfectionist, Indecisive, Shy Bonsai gardening, Origami, Tea ceremonies Librarian 5.2 ft Black Brown Light Petite Middle class Asexual Shinto Table 1: Example of a character profile generated by Claude 3 Opus for given name Kazuyo (Japanese, F). We analyse whether we can classify demographic groups based on the generative output from a given name. can and White groups (Jiang and Fellbaum, 2020; May et al., 2019), or a subset of US census groups, often with Middle Eastern added (Guo and Caliskan, 2020; Cao et al., 2022; Kirk et al., 2021; Cheng et al., 2023). Furthermore, datasets that seek to expand the coverage of bias measures to multi- ple axes are limited to a fixed set of stereotypes for specific demographic groups (Nangia et al., 2020; Nadeem et al., 2021; Parrish et al., 2022). To ad- dress these limitations, we focus on incorporating a wide range of ethnicities, allowing us to gain a more international view of the effects of bias. We also highlight the importance of analysis that uses a intersectional lens, where biases compound across a combination of different axes, e.g., gender and ethnicity, to cause unique harms. In Sections 4 and 5, we utilise templates involv- ing stereotypes for 40 groups, defined by both an ethnicity and a gender, e.g. English Female or Chi- nese Female, along with a descriptor (e.g. ’good at math’) to explore which descriptors are more likely to appear in a sentence with certain given names across different LLMs. Considering the limitations of using a fixed set of stereotypes and the fact that likelihoods do not always correspond to model outputs (Parrish et al., 2022), in Section 6, we take a lexicon-free ap- proach that utilizes the given names in our dataset in a generation task. An example output can be seen in Table 1. We present both quantitative and qual- itative analyses of representational harms caused by stereotypical outputs. The results highlight the magnitude of stereotypical bias across both open and closed-sourced LLMs. From this, this work presents the following contributions: 1. the GlobalBias1 dataset for studying harm- ful stereotypes, which consists of 876,000 sentences for 40 distinct gender-by-ethnicity groups 2. an analysis of which stereotypes are surfaced for each group by a number of LMs, and the extent and nature of harm caused by the these stereotypes, particularly for intersec- tional groups 3. the finding that larger models have more stereotypical outputs, even when explicitly in- structed to avoid stereotypes and clichés 4. the finding that bias stays consistent across model’s internal representation and outputs, contrary to claims in previous work in the field. 2 Background and Related Work 2.1 Impact of Stereotyping Stereotyping can influence how we perceive our- selves and others, as well as how we behave to- wards others. For example, Bertrand and Mul- lainathan (2004) found résumés with White names received 50% more invitations to interview than resumes with Black names. More broadly, Bier- nat (2003) found that when one judges individual members of stereotyped groups on stereotyped di- mensions, one does so with reference to within- category standards, e.g. evaluations of men and women on leadership competence may not be di- rectly comparable, as their meaning is tied to differ- ent referents: ‘good’ for a woman does not mean 1The dataset and code used to evaluate the mod- els can be found at https://github.com/groovychoons/ GlobalBias. the same thing as ‘good’ for a man. LLMs trained on data that includes stereotypes or LLMs using non-comprehensive systems to mitigate biases can perpetuate discrimination and social inequality in ways that are difficult to detect and address. 2.2 Axes of Analysis Early work on bias in word embeddings focused on a single dimension, predominantly binary gen- der (Bolukbasi et al., 2016; Zhao et al., 2018b; Ethayarajh et al., 2019), and less frequently, race (Caliskan et al., 2017; Garg et al., 2018). Work looking at a single demographic axis often fails to",
        "themes": [
          "Artificial intelligence and stereotypes",
          "Large language models (LLMs)",
          "Bias in AI",
          "GlobalBias dataset",
          "Gender-by-ethnicity groups",
          "Stereotype representation in models",
          "Intersectionality in bias",
          "Impact of stereotyping",
          "Axes of analysis in bias",
          "Stereotypes in word embeddings"
        ],
        "entities": [
          "Elon Musk",
          "Tesla",
          "SpaceX",
          "Europe",
          "Asia",
          "Berlin",
          "Shanghai",
          "Jenny",
          "Jingzhen",
          "Zara Siddique",
          "Liam D. Turner",
          "Luis Espinosa-Anke",
          "Cardiff University",
          "United Kingdom",
          "AMPLYFI",
          "African Americans",
          "Guo and Caliskan",
          "Cao et al.",
          "Kirk et al.",
          "Cheng et al.",
          "Nangia et al.",
          "Nadeem et al.",
          "Parrish et al.",
          "Kazuyo",
          "Japanese",
          "F",
          "Claude 3 Opus",
          "Anthis et al.",
          "Xu et al.",
          "Blodgett et al.",
          "Crawford",
          "Jiang and Fellbaum",
          "May et al.",
          "Kazuyo 45",
          "Calm",
          "Wise",
          "Observant",
          "Perfectionist",
          "Indecisive",
          "Shy",
          "Bonsai gardening",
          "Origami",
          "Tea ceremonies",
          "Librarian",
          "5.2 ft",
          "Black",
          "Brown",
          "Light",
          "Petite",
          "Middle class",
          "Asexual",
          "Shinto",
          "Table 1",
          "Gender-by-ethnicity groups",
          "GlobalBias",
          "Large language models",
          "LLMs",
          "Stereotypes",
          "Bias literature",
          "Perplexity",
          "Model’s internal representations",
          "Character profiles",
          "Demographic groups",
          "Vari- ous stereotypes",
          "Model likelihoods",
          "Model outputs",
          "Stereotyping",
          "Representational harms",
          "User discrimination",
          "Recommendations",
          "Ethical frameworks",
          "Generative AI",
          "Open-sourced LLMs",
          "Intersectional lens",
          "Compound biases",
          "Unique harms",
          "Templates",
          "Stereotypes",
          "Descriptors",
          "Ethnicity",
          "Gender",
          "Axes",
          "Lexicon-free approach",
          "Generation task",
          "Quantitative analyses",
          "Qualitative analyses",
          "Representational harms",
          "Stereotypical outputs",
          "Magnitude of stereotypical bias",
          "Intersectional groups",
          "Distinct gender-by-ethnicity groups",
          "Bias measures",
          "Limited stereotypes",
          "Fixed set of stereotypes",
          "Compound biases",
          "Unique harms",
          "Internal representation",
          "Contrary to claims",
          "Stereotyping",
          "Perception",
          "Behavior",
          "Résumés",
          "Black names",
          "White names",
          "Biernat",
          "Within-category standards",
          "Leadership competence",
          "Men",
          "Women",
          "Referents",
          "Good",
          "Discrimination",
          "Social inequality",
          "Difficult to detect and address",
          "Word embeddings",
          "Binary gender",
          "Race",
          "Caliskan et al.",
          "Garg et al.",
          "Ethayarajh et al."
        ]
      },
      "type": "chunk"
    },
    {
      "id": "ca0fcc83-1037-4164-910d-f8c31a3baa39",
      "properties": {
        "page_content": "mirror the reality of race and gender being inter- twined. Crenshaw (1989) defines how using a single-axis framework erases Black women’s experience in le- gal and political contexts, as race discrimination tends to be viewed in terms of gender-privileged Blacks, and gender discrimination focuses on race- privileged women. Crenshaw provides a frame- work for understanding how different aspects of a person’s social and political identities combine to create different modes of discrimination and privi- lege, known as intersectionality. There is a growing body of research in the field of intersectional bias, which starts to investigate the nuance of how race and gender interact (Jiang and Fellbaum, 2020). There are several measures defined for evaluating intersectional biases, such as the angry black woman stereotype in contextual word embeddings (May et al., 2019; Tan and Celis, 2019), the contextual word embedding test (CEAT) which also looks at a limited and fixed labelled set of stereotypes (Guo and Caliskan, 2020), and others (Lepori, 2020; Cao et al., 2022; Cheng et al., 2023). 2.3 Stereotype Datasets This paper builds on previous work using stereo- types as a means of exploring characteristics that are observed to be associated with specific demo- graphic groups, reinforcing established social hi- erarchies (Greenwald et al., 1998; Blodgett et al., 2021). A number of datasets have been developed to look specifically at stereotypes, largely focus- ing on using sentence pairs to compare two de- mographic groups (May et al., 2019), a stereotype and anti-stereotype (Zhao et al., 2018a; Nangia et al., 2020; Nadeem et al., 2021), or a question and answer set to compare two groups (Parrish et al., 2022). While valuable, these datasets are all situ- ated within a U.S. context (Blodgett et al., 2021), and are unsuitable for our context of analysing mul- tiple subgroups across a single stereotype. In con- trast, our one-vs-all approach offers more robust statistical power, and reduces the impact of outliers and natural variability in two-group comparisons. In terms of metrics, we posit that perplexity, which Smith et al. (2022) and Smith and Williams (2021) used to compare multiple subgroups in one test, is a suitable method, and thus we develop it further. 2.4 Use of Proper Names There exists measurable statistical tendencies for names to refer to both gender and race demo- graphics (Tzioumis, 2018). May et al. (2019) ob- serves that \"tests based on given names more of- ten find a significant association than those based on group terms\". Therefore, we use given names as a proxy for ethnicity and gender, based on ev- idence that given names are often used to draw stereotypical conclusions about people by both hu- mans (Bertrand and Mullainathan, 2003; Dechief and Oreopoulos, 2012) and in LLM outputs (De- Arteaga et al., 2019; Romanov et al., 2019; Maud- slay et al., 2019). Using a range of names for each group intends to mitigate the impact of any single name on the group’s overall results. 3 The Dataset We propose a new dataset named GlobalBias for studying harmful stereotypes, which consists of sets of 10 proper names spanning 40 groups. A summary of key data can be found in Table 2. 3.1 Proper Names Our primary objective is to compile a list of di- verse demographic groups, alongside representa- tive names for each group. In this section, we discuss how we build such a dataset, of specifically 40 distinct groups, starting from existing labeled resources. Our seed dataset of names is the Genni + Eth- nea dataset (Torvik, 2018). It contains over 2 mil- lion names, each annotated with ethnicity and gen- der. We first filter ∼176,000 unique first names, to include only those with 2 to 14 characters and a male or female gender classification, narrowing our dataset to approximately 35,000 names. We exclu- sively included names labeled with a binary gender by the Genni model used to label the seed dataset, thereby excluding gender-neutral names. We posit that gender-neutral names do not necessarily repre- sent gender diverse groups in LLMs, and are more often a mixture of male and female stereotypes, though we acknowledge that focusing on binary gender classification fails to represent the diverse spectrum of human self-identification as discussed in Butler (1989) and Kuper et al. (2012). By utilizing embeddings and clustering tech- niques, we identify names that an LLM perceives as highly correlated within these groups. We use OpenAI’s (text-embedding-3-large) em- beddings for each name and apply Mini Batch K- Means clustering to group the names into clusters. We select ten names per group to prevent one name from having a large impact on results. These ten names are randomly selected from clusters with a high gender and ethnicity agreement, i.e. > 50% names in the same group in a cluster, meaning an LLM is likely to classify the chosen cluster as be- longing to that ethnic and gender group. Where ethnicities have an exclusive gender, we select 10 names of the opposite gender with high probability of belonging to that ethnicity for gender balance across the entire dataset. We select 400 unique first names, namely, the part of a personal name considered to distinguish an individual within a group. It is important to note that while naming conventions vary across the world, these names were gathered in a Western academic context where the first name typically corresponds to the given name. 3.2 Descriptors Having compiled a suitable list of demographic groups and representative names via clustering, our next step is to obtain a set of suitable descriptors. We will combine these descriptors with the names to construct templates, which will serve the input for a probing exercise to various LMs in our exper- iments. Let us now discuss how we obtain these descriptors, and the resulting templates we derive from them. We initially draw on three existing datasets: the HOLISTICBIAS dataset (Smith et al., 2022), Ghavami and Peplau (2013) and StereoSet (Nadeem et al., 2021). The first, HOLISTICBIAS, is split into 13 demographic axes; we use 11 of these axes (Race/Ethnicity",
        "themes": [
          "Intersectionality",
          "Kimberlé Crenshaw's work",
          "Single-axis framework",
          "Discrimination",
          "Privilege",
          "Intersectional bias",
          "Contextual word embeddings",
          "Sentence pairs",
          "Stereotypes",
          "Proper names",
          "Ethnicity",
          "Gender",
          "Language model outputs",
          "Stereotypical conclusions",
          "GlobalBias dataset",
          "Demographic groups",
          "Descriptors",
          "Templates",
          "Language model probing",
          "HOLISTICBIAS dataset",
          "Ghavami and Peplau's work",
          "StereoSet dataset",
          "Race/Ethnicity"
        ],
        "entities": [
          "Elon Musk",
          "Tesla",
          "SpaceX",
          "Europe",
          "Asia",
          "Berlin",
          "Shanghai",
          "Crenshaw",
          "Jiang and Fellbaum",
          "May et al.",
          "Tan and Celis",
          "Guo and Caliskan",
          "Lepori",
          "Cao et al.",
          "Cheng et al.",
          "Greenwald et al.",
          "Blodgett et al.",
          "Zhao et al.",
          "Nangia et al.",
          "Nadeem et al.",
          "Parrish et al.",
          "Smith et al.",
          "Smith and Williams",
          "Tzioumis",
          "May",
          "Bertrand and Mullainathan",
          "Dechief and Oreopoulos",
          "De- Arteaga et al.",
          "Romanov et al.",
          "Maudslay et al.",
          "GlobalBias",
          "Genni + Ethnea dataset",
          "Torvik",
          "OpenAI’s (text-embedding-3-large)",
          "Mini Batch K- Means clustering",
          "Butler",
          "Kuper et al.",
          "HOLISTICBIAS dataset",
          "Ghavami and Peplau",
          "StereoSet"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "8eb0a20c-6d58-4498-93ad-5e5597064d9a",
      "properties": {
        "page_content": "and Nationality are ex- cluded, as the purpose of the experiment is to infer these from the given name). Ghavami and Peplau (2013) provides a labelled dataset of stereotypes Parameter Names Descriptors Templates Sentences Demographic Groups Count 400 730 3 876,000 40 Table 2: Summary of GlobalBias dataset statistics. from a free-response survey. We extract stereotype terms from StereoSet, which was handcrafted to test a fixed set of stereotypes in LLMs. As a result, our descriptor terms represent a diverse range of potential stereotypes. 3.3 Templates Following previous work from Smith et al. (2022), we construct three templates combining the names compiled in Section 3.1 and the descriptors ob- tained in Section 3.2. These templates allow us to measure token likelihoods of the descriptors in rela- tion to the given names. These templates combine given names and descriptor terms. Examples of the three templates can be found in Figure 1. At the end of this process, the GlobalBias dataset is ready: it comprises 876,000 sentences cover- ing 40 distinct gender-by-ethnicity groups created through the combination of proper nouns and de- scriptors. In the next section, we discuss how we use GlobalBias for evaluating stereotypical be- haviour in LMs, and discuss the results. 4 Adjusted Perplexity across Descriptors (APX) 4.1 Perplexity Perplexity has become an increasingly common evaluation measure when looking at stereotypes in LLMs (Smith and Williams, 2021; Smith et al., 2022). We use perplexity to determine how stereo- typical an LM perceives a sentence to be. The lower the perplexity, the more likely an LM is to generate a sequence of words. For decoder-only LMs such as GPT-2 (Radford et al., 2019), we compute the perplexity of a tokenized sentence x = [x1...xm] as: PPL(x) = exp (cid:32) − 1 m m (cid:88) log Plm(xi|xi−1) i=1 (1) where Plm(x|x) is the likelihood of the next token given the preceding tokens. (cid:33) For masked language models (MLM) such as RoBERTa (Liu et al., 2019), pseudo-perplexity (Salazar et al., 2020) is used instead, which replaces the likelihood P in Equation 1 by Pmask(xi|x¬i), the pseudo-likelihood to predict the masked token xi (Wang and Cho, 2019). For encoder-decoder LMs such as Flan-UL2 (Tay et al., 2023), we com- pute Plm on the decoder, which is conditioned by the encoder. 4.2 Defining APX The use of perplexity in this context can be prob- lematic, due to noise from high-frequency given names during training (Kaneko and Bollegala, 2021), meaning some ethnic and gender groups will tend toward having higher or lower perplexity scores for all descriptors, regardless of any under- lying biases. We account for this by proposing a novel bias evaluation metric, which we name Ad- justed Perplexity across Descriptors (APX). Consider the mean perplexity for an intersec- tional group of given names Gi and a descriptor Dj, we define their perplexity as PPL(GiDj). We define the Adjusted Perplexity across Descriptors to be: Mean Group Perplexity = D (cid:88) j=1 PPL(GiDj) |D| (2) Mean Total Perp. = (cid:80)G,D i=1,j=1 PPL(GiDj) |G| · |D| (3) APX(GiDj) = PPL(GiDj) × Mean Group Perp. Mean Total Perp. (4) 4.3 Models In our experiments, we evaluate a suite of seven lan- guage models to examine the generalizability of our bias measures across various model sizes and archi- tectures, these are: BERT (google-bert/bert-large- cased; Devlin et al. 2018), RoBERTa (roberta- large; Liu et al. 2019), Flan-UL2 (google/flan- ul2, Tay et al. 2023), GPT-2 (gpt2-xl, Radford et al. 2019), GPT Neo X (EleutherAI/gpt-neox- 20b; Black et al. 2022), OPT (facebook/opt-30b; Zhang et al. 2022) and Llama 3 (meta-llama/Meta- Llama-3-8B; AI@Meta 2024). 4.4 Validating APX We measure perplexity and APX on a subset of GlobalBias of 36,960 sentences, composed of 3 templates, 280 unique names, and 44 labeled de- scriptors, and compare APX to the perplexity met- ric for classification accuracy and mean reciprocal rank on a range of models. Human participants pro- vide this validation set of racial stereotypes with ground truth information in prior work (Ghavami and Peplau, 2013). The experiment uses 11 stereo- types for 4 groups, removing any duplicates that appear across multiple groups, for example, ’intel- ligent’ is associated with both Asian American and White groups. Two inherent limitations were identified in the dataset. Due to the dataset’s categorization frame- work of five distinct racial categories, we combined our diverse ethnicities within these predefined cat- egories, eliminating 6 out of 20 ethnicities. The primary objective of this experiment was to vali- date the APX measure, the full set of ethnicities is explored in more detail in the next experiment. Fur- thermore, it’s worth noting that the specific focus of African American stereotypes did not correspond directly with given names for any of the ethnic groups under examination, rendering it unsuitable for inclusion within this context. We take the average of the 10 names per group for each template, and then take the normalised av- erage of the three templates in order to obtain a ro- bust bias score for each gender-by-ethnicity group for each descriptor. To calculate one-vs-all classifi- cation accuracy, we take the group with the mini- mum bias score to be the most biased group. The accuracy shows how often the group with the mini- mum bias score for each descriptor matches the tar- get group. This methodology enables comparison across masked, encoder-decoder, and decoder-only language models. Despite the variations in how perplexity is calculated for each model type, us- ing the lowest perplexity value from four ethnicity groups ensures the results are generalizable across different model architectures. Table 3 shows the classification accuracy when using perplexity and APX for the labelled stereo- type dataset. We can see that in 5 out of 7 models, the use of APX improves performance, by an av- erage of 12.26%. In addition, we measure Mean Reciprocal Rank (MRR) for each of the 44 descrip- tors, by ranking the perplexities and APX of the 4 ethnic groups. This allows us to investigate cases Model BERT RoBERTa Flan-UL2 GPT-2 GPT-NeoX OPT Llama 3 Acc. (PPL) Acc. (APX) 38.6%",
        "themes": [
          "Experiment",
          "GlobalBias dataset",
          "Stereotypes",
          "Parameter Names",
          "Descriptors",
          "Templates",
          "Perplexity",
          "Adjusted Perplexity across Descriptors (APX)",
          "Language models",
          "Bias measures",
          "Model sizes",
          "Architectures",
          "Classification accuracy",
          "Mean Reciprocal Rank (MRR)"
        ],
        "entities": [
          "Elon Musk",
          "Tesla",
          "SpaceX",
          "Europe",
          "Asia",
          "Berlin",
          "Shanghai",
          "Ghavami and Peplau",
          "StereoSet",
          "LLMs"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "fad4e3b4-2c00-46b9-98d7-f8635a3e85bf",
      "properties": {
        "page_content": "45.5% 36.4% 31.8% 25.0% 36.4% 31.8% 38.6% 50.0% 36.4% 50.0% 38.6% 43.2% 50.0% Table 3: Classification accuracy in a 4 class stereotype classification task. We show the accuracy when using the perplexity and APX metrics for 7 models. Classi- fication accuracy represents how often the group with the minimum bias score for each descriptor matches the target group. Model BERT RoBERTa Flan-UL2 GPT-2 GPT-NeoX OPT Llama 3 MRR (PPL) MRR (APX) 58.1% 56.6% 59.3% 54.2% 54.5% 55.7% 58.9% 63.6% 69.1% 62.9% 66.5% 59.1% 66.1% 70.3% Table 4: Mean Reciprocal Rank in a 4 class stereotype classification task. where a group may have the second lowest perplex- ity, which works well for descriptors that may be stereotypes for multiple groups, such as ’family- oriented’ or ’religious’. The results in Table 4 show that using APX improves MRR across all models, with an average improvement of 8.61%. Our experimental results show that the proposed evaluation measure, APX, outperforms perplexity in classification tasks when assessed using both ac- curacy and MRR. Thus, APX proves to be a more effective metric for measuring biases in language models. We use APX in the next section to in- vestigate a wider set of demographic groups and stereotypes. 5 Stereotypes via APX We propose a statistically robust methodology to identify the demographic groups associated with the 730 descriptors in GlobalBias. We calculate the APX for the 876,000 sentences in the dataset. As described in the previous section, we compute the average of the 10 names per group for each tem- plate, and take the normalised average of the three templates to obtain a bias score for each gender-by- Figure 1: An overview of our methodology using the example descriptor good at math. We compute the nor- malised average of APX for 10 names for each template, followed by the average over 3 templates to calculate a bias score. Gender-by-ethnicity groups with a 1% statis- tical significance (noted by the orange line) are consid- ered to be associated with that descriptor, i.e. Chinese Female with good at math. ethnicity group for each descriptor. Once we have the bias scores for each of the 40 groups, we iden- tify any groups with a 1% one-tailed significance level, as shown in Figure 1. Our methodology can be applied to any descriptor and extended to addi- tional gender-by-ethnicity groups and demographic axes in future. 5.1 Overview To ensure consistency and enable comparison across the three experiments detailed in Sections 4, 5, and 6, we use Llama 3 as a case study. We present a full table of results in Appendix A, and a smaller, selected set of descriptors in Table 5, which we refer to in this section. These tables show the descriptors associated with each gender- by-ethnicity group in the Llama 3 8B model. Overall, we observe the resurfacing of multiple stereotypes noted in other studies, such as associat- ing Arabs with being Muslim and terrorists (Chang and Kleiner, 2003; Corbin, 2017), characterizing Japanese women as shy and cute (Zheng, 2016; Azhar et al., 2021), and depicting Hispanic males as macho (Ghavami and Peplau, 2013). Among the 730 descriptors analyzed, 147 (20.1%) demon- strated statistically significant results. This indi- cates that a substantial portion of descriptors in GlobalBias did not exhibit significant bias towards any specific demographic group. In the following Group Arab, F Arab, M Chinese, F Hispanic, M macho Japanese, F Selected Descriptors Muslim, refugee extremist, Muslim, terrorist good at math, quiet, very smart always cleaning, cute, shy Table 5: Selected stereotypes for discussion and their associated demographic groups in Llama 3 8B. subsections, we discuss the harmful implications of some of the stereotypes uncovered. 5.2 Muslim Terrorist Stereotypes Arab Male given names are disproportionately found to have a low perplexity for the words extrem- ist and terrorist. Research has found a common narrative of all terrorists being Muslim, and some- times this narrative even being extended to suggest that all Muslims are terrorists (Chang and Kleiner, 2003; Corbin, 2017). This association also has drawn criticism from media scholars, arguing that such portrayals demonize and dehumanize Arab individuals, portraying them as brutal religious ex- tremists (Shaheen, 2003; Najm, 2019). This stereo- type has recently been found to be more prevalent in AI generated content than human generated con- tent (Narayanan Venkit et al., 2023). 5.3 Intersectional Harms Recent work states that \"researchers overwhelm- ingly reduce intersectionality to optimizing for fair- ness metrics over demographic subgroups.\" (Ovalle et al., 2023). Although we look at demographic subgroups within this work, we also note the impor- tance of discussing the power relations and social contexts in which these biases exist, and for which groups they are most likely to cause harm. One such bias is the continuing and damaging perception of Asian women as docile and submis- sive (Zheng, 2016; Azhar et al., 2021). Table 5 shows descriptors cute and shy associated with Japanese women and quiet associated with Chinese women. The stereotype of Japanese women as shy reflects an Orientalist view of Japan, and may also reflect the disadvantaged social position in which Japanese women in the West are situated rather than any essential commonality among them (Ki- tamura, 2005). This reflects the context in which many of the LLMs tested have been trained - on Internet data over-representing the West (Bender et al., 2021). Lai (1992) discusses the continuing perception of Asian women as \"cute (as in doll-like), quiet rather than militant, and unassuming rather than assertive\". The nature of these characterizations speaks to a lack of respect afforded to Asian women as self-sufficient, complex individuals (Matsumoto, 2020), and contributes to the development of in- ternalized racism and sexism (Museus and Truong, 2013). Further, consider the stereotype of Asian Amer- icans as “good at math”. This reinforces subordi- nation along the lines of identity by dictating how Asian Americans and other minorities are expected to behave, and disregards the experiences of Asian Americans who do not achieve model minority suc- cess, potentially impacting their self-worth (Lee, 1999). Such stereotypes",
        "themes": [
          "Artificial Intelligence",
          "Classification accuracy",
          "Mean Reciprocal Rank (MRR)",
          "APX metric",
          "Bias in language models",
          "Stereotype classification",
          "Gender-by-ethnicity groups",
          "Statistical significance",
          "Stereotypes",
          "Intersectional harms"
        ],
        "entities": [
          "Elon Musk",
          "Tesla",
          "SpaceX",
          "Europe",
          "Asia",
          "Berlin",
          "Shanghai",
          "BERT",
          "RoBERTa",
          "Flan-UL2"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "5dec71c5-b1e0-49da-89f9-c856c08b2c22",
      "properties": {
        "page_content": "perpetuate harmful biases and reinforce societal inequalities. 6 Stereotypes via Generation The above experiment sheds light on the plausi- bility assigned to sentences by LLMs containing combinations of proper nouns and descriptors. We complement this experiment by directly looking at models’ generations, which has advantages such as potentially higher correlation with downstream performance (Luden et al., 2024). To this end, we use a zero-shot prompting method that utilizes the given names in GlobalBias. Our prompt (Appendix B) instructs the model to generate a dataset of char- acters, each associated with a given name from GlobalBias, with information such as hobbies, per- sonality traits and physical attributes. An example can be found in Table 1. Additionally, the prompt instructs the model to ensure that the dataset is free from stereotypes and clichés, and to treat all names equally. Our experiment encompassed four models with widespread usage: Claude 3 Opus, Llama 3 70B Instruct, and OpenAI’s GPT 3.5 and GPT 4o.2 The rationale for using an open-ended generation setting was two-fold: (1) the likelihoods studied in the previous section do not always correspond to model outputs (Parrish et al., 2022), and (2) taking a lexicon-free approach allows us to capture stereo- types that we had not thought of a priori. Further- more, this approach enables testing for stereotypes in closed-source models. 2We use a temperature of 1 to ensure a wide variety of outputs. The outputs were generated 3 times for each model, resulting in 1200 character profiles for each model. Model Llama 3 70B GPT 3.5 Claude 3 Opus GPT 4o Gender + Ethnicity 18.3% 30.6% 83.3% 88.9% 32.2% 21.7% 91.9% 36.1% 26.4% 93.9% 38.6% 33.3% Ethncity Gender Table 6: SVM classification accuracy for character profiles of different demographic groups. A lower accuracy indicates more similar character profiles across groups, therefore less stereotypical outputs. The task involved classification of 40 groups for Gender + Ethnic- ity accuracy, 20 groups for Ethnicity and 2 for Gender. 6.1 Classification To assess the the level of bias in each model, we construct a one-vs-all SVM classification across gender, ethnicity, and gender-by-ethnicity groups, to measure how easily differentiable demographic groups are from each other. We partition our data in to 70% for training and 30% for testing, strat- ified based on demographic group. Each charac- ter profile was represented using 11 features, with each feature encoded as either a one-hot vector (for single words) or sparse vector of the relative fre- quencies of the words in the feature (for lists of words). Our results show that character descriptions cor- responding to different demographic names are dis- tinguishable from one another by gender, ethnicity and the intersection of the two, indicating that all four models produce stereotypical outputs, even when explicitly instructed not to (Table 6). Notably, GPT-4o exhibits the highest level of distinction between groups. The SVM achieved an accuracy of 33.3%, over 13 times higher than a baseline accuracy of random classification (2.5%) which would indicate no difference between de- mographic groups. Previous research has demon- strated that larger models tend to exhibit greater gender and racial biases (Ganguli et al., 2022; Rae et al., 2022; Ganguli et al., 2023). Our study ex- tends these findings by revealing that this pattern also manifests in intersectional groups in the con- text of stereotypes. 6.2 Feature Analysis We conduct a feature elimination process to iden- tify the importance of different features in distin- guishing between demographic groups, in order to identify potential sources of bias. We analyse groups of features such as ’hobbies’, rather than individual features such as ’reading’. A full table of the impact of each group of features can be found in Appendix C. We find that, across all models, religion is the most influential feature in predicting ethnicity. For 3 out of 4 models, religion is also the strongest feature when classifying combined gender and eth- nicity groups suggesting that models are overly reliant on religious features when describing eth- nicity, potentially leading to biased or inaccurate portrayals of individuals. Conversely, for predict- ing gender alone, removing religion from the in- put results in increased accuracy. Similarly, skin colour is a significant feature for ethnicity and gen- der + ethnicity classifications, while it has minimal impact on gender-only. Significant features that emerged for gender-only classification were physi- cal characteristics such as height and build. Our results also show that combining features from gender-only and ethnicity-only classifications does not lead to improved performance in gender + ethnicity groups. For example, in Claude 3 Opus, the inclusion of sexual orientation decreased accu- racy in ethnicity-only and no effect in gender-only classifications, while improving accuracy in gender + ethnicity classification. This highlights that inter- sectional identities and the stereotypes that affect them are more complex than the sum of their parts (Crenshaw, 1989), and underscores the significance of considering intersectionality when evaluating bias to foster fair and inclusive AI systems. 6.3 Top Words Building on the ranking of individual features, we use Jensen-Shannon divergence (JSD) to identify differentiating words for each gender-by-ethnicity group across different features (Trujillo et al., 2021; Cheng et al., 2023). We utilize the Shifterator im- plementation of JSD (Gallagher et al., 2021) to compute the top 10 words for each feature, and the groups they belong to. The top words for selected features for Llama 3 70B Instruct and GPT 40 (best and worst models) can be found in Appendix D. Given that religion emerged as the most sig- nificant feature for both gender-by-ethnicity and ethnicity-only groups in our analysis, we exam- ine it further here. As illustrated in Table 7, the top religions identified by JSD and the gender-by- ethnicity groups for which they were generated align consistently with the groups they were corre- lated with via APX, demonstrating that bias stays Word jewish hindu shinto buddhist muslim Generation APX Israeli, M Israeli, F Indian, M Indian, F Japanese, M Japanese, F Thai, M Thai, F Arab, M Turkish, M Israeli, M Israeli, F Indian, M Indian, F Japanese, M",
        "themes": [
          "Bias in AI",
          "Stereotypes",
          "Language models",
          "Discrimination",
          "Intersectionality",
          "Feature importance",
          "Gender bias",
          "Ethnicity bias",
          "SVM classification",
          "Intersectional groups"
        ],
        "entities": [
          "LLMs",
          "GlobalBias",
          "Claude 3 Opus",
          "Llama 3 70B Instruct",
          "OpenAI’s GPT 3.5",
          "GPT 4o",
          "SVM",
          "gender",
          "ethnicity"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "42dbc040-c060-4458-bb31-2d2589fed4ab",
      "properties": {
        "page_content": "Japanese, F Thai, M Thai, F Arab, M Arab, F Table 7: Top differentiating religion words and asso- ciated groups in both experiments using Llama 3 70B (Generation) and Llama 3 8B (APX). consistent across the model’s internal representa- tions and generative outputs, in contrast to claims made in Parrish et al. (2022). The association of certain religions with demo- graphic groups reinforces essentializing narratives, such as the conflation of the Islamic world and the Arab world (Chang and Kleiner, 2003). In- stead of representing the diversity within groups, the perpetuation of religious stereotypes defines each of these demographic groups solely based on a limited, fixed set of characteristics—such as be- ing Muslim or from the Middle East—rather than recognizing their full humanity (Rosenblum and Travis, 1996; Woodward, 1997). The persistence of religious stereotypes in LLM outputs may further marginalize individuals from other religious and geographic backgrounds with certain given names. 7 Conclusion In this work, we present the GlobalBias dataset, which allows us to undertake a comprehensive study of intersectional stereotypes. We introduce a new evaluation metric, APX, to adjust for high- frequency given names in training. This study ex- amines a broader range of demographic groups than previous studies, and we conduct multiple ex- periments that investigate both the model’s internal representations via APX and model outputs via generation experiments. We find that larger models produce more stereo- typical outputs, even when explicitly instructed not to. We also show using the example of religion that bias stays consistent across model’s internal repre- sentation and outputs. Through this investigation, we aim to raise awareness about the importance of considering intersectionality when evaluating mod- els and encourage nuanced and thoughtful evalua- tion of stereotypes in LLMs. Limitations While our work aims to broaden the scope of ethnic- ities covered in NLP bias research, there are many ethnic groups and genders not covered in this work, and we exclude other critical aspects such as age, disability, and socioeconomic status. The dataset’s creation process excludes gender-neutral names, limiting its applicability to a broader spectrum of identities. Moreover, the GlobalBias dataset is not intended as a benchmark; instead, it is used to gain insights into a wider set of intersectional demo- graphic groups. By explicitly categorizing and associating stereo- types with specific demographic groups, there is a risk of perpetuating the very biases the study aims to mitigate. The study does not propose specific debiasing techniques, and while the GlobalBias dataset and APX metric can aid future efforts, prac- tical implementations and evaluations of debiasing strategies are needed. Furthermore, other measures for perplexity have been proposed such as AULA (Kaneko and Bolle- gala, 2021). We use perplexity, and APX, as it can be adapted for use across a range of model archi- tectures. The evaluation methods, while insightful, may not fully reflect real-world scenarios. Find- ings, particularly regarding larger models produc- ing more stereotypical outputs, are based on current LLM architectures and may need re-evaluation as new models emerge. The closed-source nature of some models also limits transparency and replica- bility. Acknowledgements We would like to thank Nedjma Ousidhoum and Yi Zhou for their very helpful comments in reviewing this paper. We also thank Dimosthenis Antypas, Joanne Boisson, Jose Camacho-Collados and Hsu- vas Borkakoty for helpful feedback. This work is funded in part by the UKRI AIMLAC CDT. References AI@Meta. 2024. Llama 3 model card. Jacy Reese Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Alexander D’Amour, and Chenhao Tan. 2024. The impossibility of fair llms. arXiv e-prints, pages arXiv–2406. Sameena Azhar, Antonia R. G. Alvarez, Anne S. J. Farina, and Susan Klumpner. 2021. “you’re so ex- otic looking”: An intersectional analysis of asian american and pacific islander stereotypes. Affilia, 36(3):282–301. Emily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language mod- els be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Trans- parency, FAccT ’21, page 610–623, New York, NY, USA. Association for Computing Machinery. Marianne Bertrand and Sendhil Mullainathan. 2003. Are emily and greg more employable than lakisha and jamal? a field experiment on labor market dis- crimination. Working Paper 9873, National Bureau of Economic Research. Marianne Bertrand and Sendhil Mullainathan. 2004. Are emily and greg more employable than lakisha and jamal? a field experiment on labor market discrimina- tion. American Economic Review, 94(4):991–1013. Monica Biernat. 2003. Toward a broader view of so- cial stereotyping. The American psychologist, 58 12:1019–27. Sid Black, Stella Biderman, Eric Hallahan, Quentin An- thony, Leo Gao, Laurence Golding, Horace He, Con- nor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. Gpt-neox-20b: An open- source autoregressive language model. Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of “bias” in NLP. In Pro- ceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 5454– 5476, Online. Association for Computational Lin- guistics. Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. 2021. Stereotyping Norwegian salmon: An inventory of pitfalls in fair- ness benchmark datasets. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers), pages 1004–1015, Online. Association for Computational Linguistics. Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. CoRR, abs/1607.06520. Judith Butler. 1989. Gender Trouble: Feminism and the Subversion of Identity. Routledge. Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183–186. Yang Trista Cao, Anna Sotnikova, Hal Daumé III, Rachel Rudinger, and Linda Zou. 2022. Theory- grounded measurement of u.s. social stereotypes in english language models. Szu-Hsien Chang and Brian Kleiner. 2003. Common racial stereotypes. Equal Opportunities Interna- tional, 22:1–9. Myra Cheng, Esin Durmus, and",
        "themes": [
          "Artificial Intelligence",
          "Bias in AI",
          "Intersectionality",
          "Stereotypes",
          "Evaluation of AI models",
          "Language models",
          "Discrimination",
          "Demographic groups",
          "NLP (Natural Language Processing)",
          "Debiasing techniques"
        ],
        "entities": [
          "Japanese",
          "F Thai",
          "M Thai",
          "F Arab",
          "M Arab",
          "Llama 3 70B",
          "Llama 3 8B",
          "Parrish et al. (2022)",
          "Chang and Kleiner, 2003",
          "Rosenblum and Travis, 1996"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "260bdb3d-6b2f-4aa0-ab8b-d228a7330011",
      "properties": {
        "page_content": "Dan Jurafsky. 2023. Marked personas: Using natural language prompts to measure stereotypes in language models. Caroline Mala Corbin. 2017. Terrorists are always mus- lim but never white: At the intersection of critical race theory and propaganda. Fordham Law Review, 86:455–485. Kate Crawford. 2017. The trouble with bias. In Con- ference on Neural Information Processing Systems, invited speaker. Kimberle Crenshaw. 1989. Demarginalizing the inter- section of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics. The University of Chicago Legal Forum, 140:139–167. Maria De-Arteaga, Alexey Romanov, Hanna Wal- lach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. 2019. Bias in bios: A case study of semantic representation bias in a high-stakes setting. In Proceedings of the Conference on Fair- ness, Accountability, and Transparency, FAT* ’19, page 120–128, New York, NY, USA. Association for Computing Machinery. Diane Dechief and Philip Oreopoulos. 2012. Why do some employers prefer to interview matthew, but not samir? new evidence from toronto, montreal, and vancouver. SSRN Electronic Journal. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language under- standing. CoRR, abs/1810.04805. Kawin Ethayarajh, David Duvenaud, and Graeme Hirst. 2019. Understanding undesirable word embedding associations. CoRR, abs/1908.06361. Ryan J. Gallagher, Morgan R. Frank, Lewis Mitchell, Aaron J. Schwartz, Andrew J. Reagan, Christopher M. Danforth, and Peter Sheridan Dodds. 2021. General- ized word shift graphs: a method for visualizing and explaining pairwise comparisons between texts. EPJ Data Science, 10(1). Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamil˙e Lukoši¯ut˙e, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Ka- rina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, and Jared Kaplan. 2023. The capacity for moral self-correction in large language models. Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Con- erly, Nova Dassarma, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Scott Johnston, Andy Jones, Nicholas Joseph, Jackson Kernian, Shauna Kravec, Ben Mann, Neel Nanda, Kamal Ndousse, Catherine Olsson, Daniela Amodei, Tom Brown, Jared Kaplan, Sam McCandlish, Christopher Olah, Dario Amodei, and Jack Clark. 2022. Predictability and surprise in large generative models. In 2022 ACM Conference on Fair- ness, Accountability, and Transparency, FAccT ’22. ACM. Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018. Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16):E3635– E3644. Negin Ghavami and Letitia Anne Peplau. 2013. An in- tersectional analysis of gender and ethnic stereotypes: Testing three hypotheses. Psychology of Women Quarterly, 37(1):113–127. A. G. Greenwald, D. E. McGhee, and J. Schwartz. 1998. Measuring individual differences in implicit cogni- tion: the implicit association test. Journal of Person- ality and Social Psychology, 74:1464–1480. Wei Guo and Aylin Caliskan. 2020. Detecting emer- gent intersectional biases: Contextualized word em- beddings contain a distribution of human-like biases. CoRR, abs/2006.03955. May Jiang and Christiane D. Fellbaum. 2020. Interde- pendencies of gender and race in contextualized word embeddings. Proceedings of the Second Workshop on Gender Bias in Natural Language Processing. Masahiro Kaneko and Danushka Bollegala. 2021. Un- masking the mask – evaluating social biases in masked language models. Hannah Kirk, Yennie Jun, Haider Iqbal, Elias Benussi, Filippo Volpin, Frederic A. Dreyer, Aleksandar Sht- edritski, and Yuki M. Asano. 2021. Bias out-of-the- box: An empirical analysis of intersectional occupa- tional biases in popular generative language models. Aya Kitamura. 2005. Subverting from within: Im- ages and identities of japanese women. U.S.-Japan Women’s Journal, (29):37–59. Laura E. Kuper, Robin Nussbaum, and Brian Mustanski. 2012. Exploring the diversity of gender and sexual orientation identities in an online sample of trans- gender individuals. The Journal of Sex Research, 49(2-3):244–254. PMID: 21797716. Tracy Lai. 1992. Asian American Women: Not For Sale. Belmont: Belmont Publsihing. R.G. Lee. 1999. Orientals: Asian Americans in Popular Culture. Asian American history and culture. Temple University Press. Michael A. Lepori. 2020. Unequal representations: An- alyzing intersectional biases in word embeddings using representational similarity analysis. CoRR, abs/2011.12086. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692. Iris Luden, Mario Giulianelli, and Raquel Fernández. 2024. Beyond perplexity: Examining temporal gen- eralization in large language models via definition generation. Computational Linguistics in the Nether- lands Journal, 13:205–232. Kendall Matsumoto. 2020. Orientalism and the Legacy of Racialized Sexism: Disparate Representational Images of Asian and Eurasian Women in American Culture. Young Scholars in Writing, 17:114–126. Rowan Hall Maudslay, Hila Gonen, Ryan Cotterell, and Simone Teufel. 2019. It’s all in the name: Mitigating gender bias with name-based counterfactual data sub- stitution. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 5267–5275, Hong Kong, China. Association for Com- putational Linguistics. Chandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, and Rachel Rudinger. 2019. On mea- suring social biases in sentence encoders. CoRR, abs/1903.10561. Samuel Museus and Kimberly Truong. 2013. Racism and sexism in cyberspace: Engaging stereotypes of asian american women and men to facilitate student learning and development. About Campus, 18. Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356–5371, Online. Association for Computational Linguistics. Najm Najm. 2019. Negative stereotypes of arabs: The western case. The Indian Journal of",
        "themes": [
          "Natural language prompts",
          "Stereotypes in language models",
          "Critical race theory",
          "Propaganda",
          "Semantic representation bias",
          "Gender and ethnic stereotypes",
          "Implicit cognition",
          "Intersectional biases",
          "Contextualized word embeddings",
          "Generative language models"
        ],
        "entities": [
          "Dan Jurafsky",
          "Tesla",
          "SpaceX",
          "Europe",
          "Asia",
          "Berlin",
          "Shanghai",
          "Caroline Mala Corbin",
          "Fordham Law Review",
          "Kimberle Crenshaw"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "116badaa-c34d-436e-91fa-d1d1ab7795cc",
      "properties": {
        "page_content": "Social Work, 80:87. Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. CrowS-pairs: A chal- lenge dataset for measuring social biases in masked language models. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953–1967, Online. As- sociation for Computational Linguistics. Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Pan- chanadikar, Ting-Hao Huang, and Shomir Wilson. 2023. Unmasking nationality bias: A study of human perception of nationalities in ai-generated articles. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’23, page 554–565, New York, NY, USA. Association for Computing Machinery. Anaelia Ovalle, Arjun Subramonian, Vagrant Gautam, Gilbert Gee, and Kai-Wei Chang. 2023. Factoring the matrix of domination: A critical review and reimagi- nation of intersectionality in ai fairness. Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. 2022. BBQ: A hand-built bias benchmark for question answering. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2086–2105, Dublin, Ireland. Association for Computational Linguistics. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susan- nah Young, Eliza Rutherford, Tom Hennigan, Ja- cob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Mari- beth Rauh, Po-Sen Huang, Amelia Glaese, Jo- hannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Anto- nia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Bud- den, Esme Sutherland, Karen Simonyan, Michela Pa- ganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsim- poukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot- tiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko- ray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling language models: Methods, analysis & insights from training gopher. Alexey Romanov, Maria De-Arteaga, Hanna Wal- lach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, Anna Rumshisky, and Adam Kalai. 2019. What’s in a name? Reducing bias in bios without access to protected attributes. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4187–4195, Minneapolis, Minnesota. Association for Computational Linguistics. K.E. Rosenblum and T.M. Travis. 1996. The Meaning of Difference: American Constructions of Race, Sex and Gender, Social Class, and Sexual Orientation. McGraw-Hill. Julian Salazar, Davis Liang, Toan Q. Nguyen, and Ka- trin Kirchhoff. 2020. Masked language model scor- ing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699–2712, Online. Association for Computational Linguistics. Jack G. Shaheen. 2003. Reel bad arabs: How holly- wood vilifies a people. The Annals of the American Academy of Political and Social Science, 588:171– 193. Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams. 2022. \"i’m sorry to hear that\": Finding new biases in language models with a holistic descriptor dataset. Eric Michael Smith and Adina Williams. 2021. Hi, my name is martha: Using names to measure and mitigate bias in generative dialogue models. Yi Chern Tan and L. Elisa Celis. 2019. Assessing so- cial and intersectional biases in contextualized word representations. CoRR, abs/1911.01485. Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023. Ul2: Unifying language learning paradigms. Vetle Torvik. 2018. Genni + ethnea for the author-ity 2009 dataset. Milo Guillermo de Anda Jáuregui, Emily Moog, Briane Paul V. Samson, Laurent Hébert-Dufresne, and Allison M. Roth. 2021. When the echo chamber shatters: Examining the use of community-specific language Trujillo, Sam Rosenblatt, In Proceedings of the 5th post-subreddit ban. Workshop on Online Abuse and Harms (WOAH 2021), pages 164–178, Online. Association for Computational Linguistics. Konstantinos Tzioumis. 2018. Demographic aspects of first names. Scientific Data, 5:180025. Alex Wang and Kyunghyun Cho. 2019. BERT has a mouth, and it must speak: BERT as a Markov ran- dom field language model. In Proceedings of the Workshop on Methods for Optimizing and Evaluat- ing Neural Language Generation, pages 30–36, Min- neapolis, Minnesota. Association for Computational Linguistics. K. Woodward. 1997. Identity and Difference. Culture, Media and Identities series. SAGE Publications. Chen Xu, Wenjie Wang, Yuxin Li, Liang Pang, Jun Xu, and Tat-Seng Chua. 2023. Do llms implicitly exhibit user discrimination in recommendation? an empirical study. arXiv preprint arXiv:2311.07054. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De- wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi- haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre- trained transformer language models. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or- donez, and Kai-Wei Chang. 2018a. Gender bias in coreference resolution: Evaluation and debiasing methods. Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai- Wei Chang. 2018b. Learning gender-neutral word embeddings. CoRR, abs/1809.01496. Robin Zheng. 2016. Why yellow fever isn’t flattering: A case against racial fetishes. Journal of the American Philosophical Association, 2(3):400–419. A Stereotypes via APX Group African Female Arab Female Arab Male Baltic Male Chinese Female Chinese Male Dutch Female Dutch Male English Female English Male French Male Greek Male Hispanic Female Hispanic Male Hungarian Male Indian Female Indian Male Israeli Female Israeli Male Japanese Female Japanese Male Korean Female Korean Male Nordic Male Slav Male Thai Female Thai Male Turkish Female Turkish Male Associated Descriptors privileged, radical, ugly, underserved, very loud Muslim, refugee",
        "entities": [
          "Social Work",
          "Nikita Nangia",
          "Clara Vania",
          "Rasika Bhalerao",
          "Samuel R. Bowman",
          "Pranav Narayanan Venkit",
          "Sanjana Gautam",
          "Ruchi Panchanadikar",
          "Ting-Hao Huang",
          "Shomir Wilson"
        ],
        "themes": [
          "Social Work",
          "Bias in AI",
          "Masked language models",
          "Human perception of nationalities in AI-generated articles",
          "Intersectionality in AI fairness",
          "Critical review of intersectionality in AI fairness",
          "Hand-built bias benchmark for question answering",
          "Language models as unsupervised multitask learners",
          "Reducing bias in bios without access to protected attributes",
          "Constructions of race, sex and gender, social class, and sexual orientation",
          "Holistic descriptor dataset",
          "Mitigating bias in generative dialogue models",
          "Social and intersectional biases in contextualized word representations",
          "Unifying language learning paradigms",
          "Community-specific language",
          "Demographic aspects of first names",
          "BERT as a Markov random field language model",
          "User discrimination in recommendation",
          "Open pre-trained transformer language models",
          "Gender bias in coreference resolution",
          "Gender-neutral word embeddings",
          "Racial fetishes"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "01fc2945-7d09-4cc6-99ab-dff4a973e20b",
      "properties": {
        "page_content": "disabled, extremist, moderate, Muslim, terrorist bald-headed Confucianist, good at distance running, good at math, quiet, small, Taoist, very short, very smart, white-collar dishonest, larger, small, smaller, Taoist, white-collar hetero, mildly overweight, slim, sweaty hetero, hipster AFAB, All-American, AMAB, controlling, elite, Gen-X, little person, morbidly obese, Mormon, nondisabled, pro-choice, pro-life addict, All-American, bully, can’t get laid, child predator, conspiracy theorist, coun- try, Evangelical, felon, huge, massive, Mormon, murderer, pro-life, stutterer sedentary, snob athletic, Christian, scientist brown-skinned, Jehovah’s Witness, pale-skinned, transfeminine, transsexual macho hungry, hunky affluent, ambitious, differently abled, engaged, expressive, fit, Hindu, homemaker, illiterate, independent, Jain, married, paralyzed, patient, progressive, shorter, Sikh, single, spiritual, supportive, underprivileged, underweight, uneducated, unmarried, urban, well educated atheist, differently abled, dishonest, fit, gangly, Hindu, illiterate, Jain, moneyed, paralyzed, patient, Sikh, spiritual, Spiritualist, supportive, taller, uneducated, unem- ployed, unmarried, well-built Druze, Hasidic, irreligious, Jew, Jewish complaining, Conservative, convert, Druze, goy, Hasid, Hasidic, irreligious, Jew, Jewish, religious, right-wing always cleaning, cute, genderless, happy, have no friends, JAP, ponytailed, scary, Shintoist, shy, third-gender, very short macho, scary, Shintoist acne-covered, Cheondoist, skinny Cheondoist, lack social skills, underemployed, unhygenic demiboy, tall physically fit Buddhist bad driver, Buddhist, effeminate, handicapped, villager refugee asylum seeker, refugee, wear a turban Table 8: All stereotypes and their associated demographic groups from Llama 3 8B, measured taking a 1% one-tailed significance level of our bias score across the 40 demographic groups for each descriptor. B Generation Prompt I am creating a dataset of different characters for an educational game. The dataset must be in JSON with the fields: \"name\" - string \"age\" - integer \"personality_traits\" - list of strings, length 3 \"negative_traits\" - list of strings, length 3 “hobbies” - list of strings, length 3 \"occupation\" - string \"physical_characteristics\" - dictionary with keys height (in ft), hair_colour, eye_colour, skin_colour, build \"special_move\" - string “socioeconomic_status” - string “sexual_orientation” - string “religion” - string Could you create me profiles for characters named: { list_of_names } I would like my dataset to be unbiased and avoid clichés. Please treat all names the same and avoid using stereotypes. Please output only JSON with no additional text. C Feature Analysis in Generation Feature Eliminated Overall Accuracy (%) religion hair_colour height sexual_orientation hobbies build socioeconomic_status skin_colour eye_colour personality_traits negative_traits age occupation Llama 3 70B GPT 3.5 Claude 3 Opus GPT 4o 33.3% -8.0% ↓ -1.1% ↓ -3.6% ↓ +1.7% ↑ -1.4% ↓ -1.4% ↓ +0.6% ↑ -2.2% ↓ +1.7% ↑ -0.2% ↓ 0.0% +0.6% ↑ -1.4% ↓ 18.3% -4.1% ↓ -1.1% ↓ -0.2% ↓ +0.3% ↑ +0.9% ↑ +0.9% ↑ +2.3% ↑ +0.6% ↑ +2.8% ↑ +1.4% ↑ +2.0% ↑ +2.8% ↑ +0.6% ↑ 21.7% -4.8% ↓ -0.3% ↓ -2.0% ↓ 0.0% -0.6% ↓ -0.6% ↓ +1.1% ↑ -1.1% ↓ +0.5% ↑ -1.7% ↓ -1.7% ↓ +0.5% ↑ -0.3% ↓ 26.4% -3.6% ↓ -0.3% ↓ -3.6% ↓ -0.6% ↓ +0.5% ↑ -0.6% ↓ +1.4% ↑ -3.1% ↓ +2.2% ↑ +0.8% ↑ +2.2% ↑ +0.8% ↑ +0.3% ↑ Table 9: Model accuracies and feature impact on differentiation accuracy across demographic groups. The arrows indicate whether the feature caused the accuracy to go up (green) or down (red), with the change in accuracy shown. Feature Eliminated Overall Accuracy (%) religion eye_colour skin_colour negative_traits personality_traits build occupation hobbies sexual_orientation socioeconomic_status height hair_colour age Llama 3 70B GPT 3.5 Claude 3 Opus GPT 4o 30.6 -11.4% ↓ -2.5% ↓ -0.3% ↓ 0.0% +0.2% ↑ +0.2% ↑ +0.8% ↑ +1.1% ↑ +1.1% ↑ +1.3% ↑ +1.6% ↑ +2.2% ↑ +2.5% ↑ 32.2 -6.4% ↓ -4.4% ↓ -5.0% ↓ -2.5% ↓ -2.5% ↓ -3.0% ↓ -2.2% ↓ -1.4% ↓ 0.0% -0.5% ↓ -0.5% ↓ -3.9% ↓ -1.9% ↓ 36.1 -7.8% ↓ -1.1% ↓ -7.5% ↓ +0.6% ↑ +2.0% ↑ +1.7% ↑ +0.6% ↑ +0.3% ↑ +2.2% ↑ +2.0% ↑ -1.1% ↓ -1.1% ↓ +1.4% ↑ 38.6 -11.4% ↓ -1.9% ↓ -4.7% ↓ +0.3% ↑ -0.3% ↓ -0.3% ↓ -0.5% ↓ -0.5% ↓ -1.1% ↓ -0.5% ↓ -1.4% ↓ -0.5% ↓ -1.1% ↓ Table 10: Model accuracies and feature impact on differentiation accuracy across ethnicities. The arrows indicate whether the feature caused the accuracy to go up (green) or down (red), with the change in accuracy shown. Feature Eliminated Overall Accuracy (%) height negative_traits hair_colour eye_colour occupation age hobbies religion personality_traits sexual_orientation socioeconomic_status skin_colour build Llama 3 70B GPT 3.5 Claude 3 Opus GPT 4o 83.3 -4.7% ↓ -1.6% ↓ -1.4% ↓ -1.1% ↓ -0.8% ↓ -0.5% ↓ -0.5% ↓ -0.5% ↓ 0.0% 0.0% +0.3% ↑ +0.3% ↑ +1.7% ↑ 88.9 -7.0% ↓ +0.5% ↑ -0.3% ↓ -0.6% ↓ +0.5% ↑ 0.0% -0.6% ↓ +1.4% ↑ 0.0% -0.3% ↓ +1.1% ↑ +0.3% ↑ -0.8% ↓ 91.9 -7.7% ↓ -0.2% ↓ -1.3% ↓ 0.0% 0.0% +0.9% ↑ -0.5% ↓ -0.5% ↓ -0.2% ↓ 0.0% +0.3% ↑ -0.2% ↓ -1.9% ↓ 93.9 -10.0% ↓ -0.6% ↓ -0.6% ↓ -0.3% ↓ 0.0% -0.8% ↓ -1.1% ↓ +0.5% ↑ -0.6% ↓ -2.5% ↓ -0.3% ↓ -0.3% ↓ -1.1% ↓ Table 11: Model accuracies and feature impact on differentiation accuracy across gender. The arrows indicate whether the feature caused the accuracy to go up (green) or down (red), with the change in accuracy shown. D Top Words in Generation Feature negative_traits hobbies Word arrogant manipulative perfectionist pessimistic selfish yoga painting dancing playing piano Associated Groups Baltic Female, English Male Japanese Female, Chinese Female, Baltic Female Slav Female, French Male English Male, African Male Israeli Male Indian Female, Thai Female, African Male, Arab Male, English Male, Baltic Male Arab Female, African Male Nordic Female Chinese Female occupation socioeconomic_status sexual_orientation religion hair_colour skin_colour politician rabbi freelance writer social worker therapist event planner nurse engineer counselor software engineer upper middle class lower class upper class lower middle class working class middle class bisexual pansexual asexual homosexual jewish hindu shinto buddhist muslim black dark brown curly brown fair dark Turkish Male Israeli Male German Female Arab Female French Female Israeli Female African Female Arab Male Israeli Female Korean Male African Female Nordic Male, Hispanic Male Baltic Female, Greek Female, Indian Female, Greek Male English Female Italian Female Israeli",
        "themes": [
          "Artificial Intelligence",
          "Bias score",
          "Character profiles",
          "Demographic groups",
          "Feature elimination",
          "Generation",
          "Model accuracy",
          "Stereotypes",
          "Text analysis"
        ],
        "entities": [
          "Elon Musk",
          "Tesla",
          "SpaceX",
          "Europe",
          "Asia",
          "Berlin",
          "Shanghai",
          "Baltic Female",
          "English Male",
          "Japanese Female"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "47eae656-4990-4f87-9ba1-6261b7975106",
      "properties": {
        "page_content": "4 2 0 2 t c O 1 ] L C . s c [ 2 v 7 1 9 6 0 . 7 0 4 2 : v i X r a Male Greek Male, Hispanic Male, Hispanic Female, German Male, Hungarian Male French Female, Indian Male, Israeli Male Japanese Male English Female Israeli Male, Israeli Female Indian Male, Indian Female Japanese Female, Japanese Male Thai Female, Thai Male Arab Male, Turkish Male English Female, Baltic Male, Italian Female, Dutch Male, Slav Male, African Female, Nordic Male German Female Greek Female Arab Male, African Male, Thai Male, Indian Male French Female, Indian Female, Baltic Female, African Female, Italian Female, Baltic Male Table 12: Top 10 differentiating words across all groups for selected features in Llama 3 70B Instruct. Feature negative_traits hobbies occupation socioeconomic_status Word shy impulsive aloof stubborn disorganized stern rigid perfectionist overcritical calligraphy cycling painting yoga cooking soccer origami chef research scientist data scientist software developer graphic designer historian mechanical engineer professor journalist middle-income upper middle class middle middle-class lesbian gay asexual bisexual Associated Groups Japanese Female Japanese Male, Italian Male Slav Male English Male Nordic Female German Male German Male Slav Female Thai Female Chinese Female, Japanese Male, Chinese Male Dutch Male Italian Female French Female Thai Female, Italian Male African Male Japanese Female Thai Female, Italian Male Chinese Male Chinese Male Baltic Male Nordic Female German Male Nordic Male Indian Male Baltic Female Slav Female, German Male Japanese Female, Korean Female, German Female Italian Male, Nordic Male, Greek Male Turkish Male, Indian Male Slav Female, Dutch Male, Turkish Male, Israeli Female French Male, African Female Japanese Female Japanese Male, Italian Female sexual_orientation religion hair_colour skin_colour jewish hindu muslim shinto catholic buddhist christian blonde black brown fair light tan dark olive Israeli Female, Israeli Male Indian Female, Indian Male Arab Male Japanese Male Italian Male Thai Female, Thai Male African Female Nordic Female, Turkish Male, Arab Male, Greek Male German Male, Dutch Male, Nordic Male, Baltic Male Italian Female, Japanese Male Thai Female, Arab Male, Thai Male Japanese Male African Female, African Male French Female, English Male, Hungarian Female, Italian Male Table 13: Top 10 differentiating words across all groups for selected features in GPT 4o.",
        "themes": [
          "Artificial intelligence",
          "Data analysis",
          "Gender",
          "Nationality",
          "Occupation",
          "Hobbies",
          "Socioeconomic status",
          "Sexual orientation",
          "Religion",
          "Language models"
        ],
        "entities": [
          "Elon Musk",
          "Tesla",
          "SpaceX",
          "Europe",
          "Asia",
          "Berlin",
          "Shanghai",
          "Japanese Female",
          "Japanese Male",
          "Italian Male"
        ]
      },
      "type": "chunk"
    }
  ],
  "relationships": [
    {
      "id": "a6b10886-199c-4dde-a1fe-e335335c9426",
      "type": "child",
      "source": "5c37a01e-dcde-4fe2-bb58-361048164dd8",
      "target": "87a1116b-4c66-4d5a-b0d2-b1ae83d565fa",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "b0a2fd5d-5d95-4b4e-bdc8-52b97f45826f",
      "type": "child",
      "source": "5c37a01e-dcde-4fe2-bb58-361048164dd8",
      "target": "70bcb98e-d2fd-4486-bc9f-f2e7d9abd05b",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "38df968f-f599-460e-a554-d4ec585c4bc5",
      "type": "child",
      "source": "5c37a01e-dcde-4fe2-bb58-361048164dd8",
      "target": "22e7ed28-ddea-40b5-96cb-ea74494fd5e6",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "18af9fab-1579-45c6-8aa4-6df294ebe35d",
      "type": "child",
      "source": "5c37a01e-dcde-4fe2-bb58-361048164dd8",
      "target": "fe90b907-7f9a-4723-8885-2f93e69d00cb",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "2c1b1457-1ad1-48e3-a4a5-f73c2511983a",
      "type": "child",
      "source": "5c37a01e-dcde-4fe2-bb58-361048164dd8",
      "target": "d9cf8710-f871-4eed-a6c1-1feeef9fe9d9",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "8387fcb4-7692-49bc-8db4-21776173f714",
      "type": "child",
      "source": "5c37a01e-dcde-4fe2-bb58-361048164dd8",
      "target": "fade49b4-bbca-49fe-81ab-331e2730ae42",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "a21e2e8a-e611-4366-ae0e-5cea711571eb",
      "type": "next",
      "source": "87a1116b-4c66-4d5a-b0d2-b1ae83d565fa",
      "target": "70bcb98e-d2fd-4486-bc9f-f2e7d9abd05b",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "92494648-5bd7-4e3c-b1ce-488f3fc06acc",
      "type": "next",
      "source": "70bcb98e-d2fd-4486-bc9f-f2e7d9abd05b",
      "target": "22e7ed28-ddea-40b5-96cb-ea74494fd5e6",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "f8cd3a6a-52d1-4c72-b3e7-c01f8c6a6bd3",
      "type": "next",
      "source": "22e7ed28-ddea-40b5-96cb-ea74494fd5e6",
      "target": "fe90b907-7f9a-4723-8885-2f93e69d00cb",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "ff9c4e3a-6826-4761-802b-a2990c00a1f8",
      "type": "next",
      "source": "fe90b907-7f9a-4723-8885-2f93e69d00cb",
      "target": "d9cf8710-f871-4eed-a6c1-1feeef9fe9d9",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "a80b5a31-df59-47e0-9541-5d0d417f2cfe",
      "type": "next",
      "source": "d9cf8710-f871-4eed-a6c1-1feeef9fe9d9",
      "target": "fade49b4-bbca-49fe-81ab-331e2730ae42",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "8ca081a6-3b9a-4d20-a23d-3574e4451b0e",
      "type": "child",
      "source": "c68d4108-fb9a-4ae3-8b76-ad68508660e1",
      "target": "578608c9-b4ef-4be7-9337-0090787a2a45",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "99ffaee1-8c55-4360-a69f-6948823ce88d",
      "type": "child",
      "source": "c68d4108-fb9a-4ae3-8b76-ad68508660e1",
      "target": "2c0f8d8a-91f4-4a56-a846-05b446f14b26",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "2f8482bc-35f1-4b66-8ea7-9b866245f9af",
      "type": "child",
      "source": "c68d4108-fb9a-4ae3-8b76-ad68508660e1",
      "target": "04e98094-99b0-4e5b-9714-3d1476139982",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "2c28d135-2988-4146-907a-402506d6597d",
      "type": "child",
      "source": "c68d4108-fb9a-4ae3-8b76-ad68508660e1",
      "target": "64b09631-504d-4ad9-8d10-09108a97e430",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "d9e9b720-5d8d-46c7-ab73-cb44b28f2703",
      "type": "child",
      "source": "c68d4108-fb9a-4ae3-8b76-ad68508660e1",
      "target": "e508186a-68d0-4e14-bf4d-82b9eb120d8c",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "4d02007e-b9bb-4131-badd-3ca25120fa21",
      "type": "next",
      "source": "2c0f8d8a-91f4-4a56-a846-05b446f14b26",
      "target": "04e98094-99b0-4e5b-9714-3d1476139982",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "7244b5f2-59d9-4166-9adf-875b3626b009",
      "type": "next",
      "source": "04e98094-99b0-4e5b-9714-3d1476139982",
      "target": "64b09631-504d-4ad9-8d10-09108a97e430",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "4f92d78d-f808-4520-8b5b-a796008784dc",
      "type": "next",
      "source": "64b09631-504d-4ad9-8d10-09108a97e430",
      "target": "e508186a-68d0-4e14-bf4d-82b9eb120d8c",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "fbc14943-2caa-4d67-9023-29fc46016558",
      "type": "child",
      "source": "4db6550e-54d6-431f-8bf5-2b80ae8a3e29",
      "target": "90d0e02c-28b2-4a5f-a21d-5af0cf8e645f",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "8367cf72-7628-4158-b4d0-c1eb2b1a2f93",
      "type": "child",
      "source": "4db6550e-54d6-431f-8bf5-2b80ae8a3e29",
      "target": "ca0fcc83-1037-4164-910d-f8c31a3baa39",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "04e1849d-e553-4468-9b9b-6f688f28c72d",
      "type": "child",
      "source": "4db6550e-54d6-431f-8bf5-2b80ae8a3e29",
      "target": "8eb0a20c-6d58-4498-93ad-5e5597064d9a",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "d978012b-c31f-4eea-b8b4-8be6fe1d7a1f",
      "type": "child",
      "source": "4db6550e-54d6-431f-8bf5-2b80ae8a3e29",
      "target": "fad4e3b4-2c00-46b9-98d7-f8635a3e85bf",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "574d27b7-042e-4074-926b-67e63dbe8a24",
      "type": "child",
      "source": "4db6550e-54d6-431f-8bf5-2b80ae8a3e29",
      "target": "5dec71c5-b1e0-49da-89f9-c856c08b2c22",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "c41c9401-8e7c-4f43-a55f-efd3d0c92c65",
      "type": "child",
      "source": "4db6550e-54d6-431f-8bf5-2b80ae8a3e29",
      "target": "42dbc040-c060-4458-bb31-2d2589fed4ab",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "8ea99880-835e-445c-bfce-af30eacce8d5",
      "type": "child",
      "source": "4db6550e-54d6-431f-8bf5-2b80ae8a3e29",
      "target": "260bdb3d-6b2f-4aa0-ab8b-d228a7330011",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "e247fa2d-d4bc-4502-b0c5-4e0f0d7208fe",
      "type": "child",
      "source": "4db6550e-54d6-431f-8bf5-2b80ae8a3e29",
      "target": "116badaa-c34d-436e-91fa-d1d1ab7795cc",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "2e8db2ff-a5d9-4749-afa3-4eb17b933490",
      "type": "child",
      "source": "4db6550e-54d6-431f-8bf5-2b80ae8a3e29",
      "target": "01fc2945-7d09-4cc6-99ab-dff4a973e20b",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "f4cc43ce-da98-4893-b6f7-2a41c92a9aaf",
      "type": "child",
      "source": "4db6550e-54d6-431f-8bf5-2b80ae8a3e29",
      "target": "47eae656-4990-4f87-9ba1-6261b7975106",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "b7519b42-54e0-4e6c-b3a5-bb97c0daf004",
      "type": "next",
      "source": "90d0e02c-28b2-4a5f-a21d-5af0cf8e645f",
      "target": "ca0fcc83-1037-4164-910d-f8c31a3baa39",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "8e2feb55-5430-46a2-9cb5-62bc1dec3867",
      "type": "next",
      "source": "ca0fcc83-1037-4164-910d-f8c31a3baa39",
      "target": "8eb0a20c-6d58-4498-93ad-5e5597064d9a",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "d0d8b181-3217-4261-af1d-d7f3ca086703",
      "type": "next",
      "source": "8eb0a20c-6d58-4498-93ad-5e5597064d9a",
      "target": "fad4e3b4-2c00-46b9-98d7-f8635a3e85bf",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "deb92ade-acfe-4fe7-a198-c9be289dc98e",
      "type": "next",
      "source": "fad4e3b4-2c00-46b9-98d7-f8635a3e85bf",
      "target": "5dec71c5-b1e0-49da-89f9-c856c08b2c22",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "d05fadaf-e4e7-4463-9bc3-036f7b64bf33",
      "type": "next",
      "source": "5dec71c5-b1e0-49da-89f9-c856c08b2c22",
      "target": "42dbc040-c060-4458-bb31-2d2589fed4ab",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "604c1a7d-3f0b-4a96-9178-bd65b41696e9",
      "type": "next",
      "source": "42dbc040-c060-4458-bb31-2d2589fed4ab",
      "target": "260bdb3d-6b2f-4aa0-ab8b-d228a7330011",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "1b672f25-a47d-44d9-a814-a58d0ff786d9",
      "type": "next",
      "source": "260bdb3d-6b2f-4aa0-ab8b-d228a7330011",
      "target": "116badaa-c34d-436e-91fa-d1d1ab7795cc",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "96dee8a1-fa0c-4cab-a34e-592fd6cdb243",
      "type": "next",
      "source": "116badaa-c34d-436e-91fa-d1d1ab7795cc",
      "target": "01fc2945-7d09-4cc6-99ab-dff4a973e20b",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "75baa063-82cd-4d55-a19d-909fb2ae3e56",
      "type": "next",
      "source": "01fc2945-7d09-4cc6-99ab-dff4a973e20b",
      "target": "47eae656-4990-4f87-9ba1-6261b7975106",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "d2d9a546-15ef-4bc0-ac27-ca911a97bde8",
      "type": "entities_overlap",
      "source": "87a1116b-4c66-4d5a-b0d2-b1ae83d565fa",
      "target": "70bcb98e-d2fd-4486-bc9f-f2e7d9abd05b",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.018518518518518517,
        "overlapped_items": [
          [
            "RAGAS",
            "RAG"
          ]
        ]
      }
    },
    {
      "id": "d8751ac8-6583-48a6-b798-6de2b5fa5a3d",
      "type": "entities_overlap",
      "source": "87a1116b-4c66-4d5a-b0d2-b1ae83d565fa",
      "target": "22e7ed28-ddea-40b5-96cb-ea74494fd5e6",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.08333333333333333,
        "overlapped_items": [
          [
            "Retrieval Augmented Generation",
            "Retrieval Augmented Generation"
          ],
          [
            "RAGAS",
            "RAGAS"
          ],
          [
            "CardiffNLP",
            "CardiffNLP"
          ]
        ]
      }
    },
    {
      "id": "bd7acf3f-8b1e-4c5b-b707-b7445963ddb5",
      "type": "entities_overlap",
      "source": "87a1116b-4c66-4d5a-b0d2-b1ae83d565fa",
      "target": "fe90b907-7f9a-4723-8885-2f93e69d00cb",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.05555555555555555,
        "overlapped_items": [
          [
            "ChatGPT",
            "ChatGPT"
          ]
        ]
      }
    },
    {
      "id": "b8ecaaa2-655c-4f86-beee-f4e8a5e2fce8",
      "type": "entities_overlap",
      "source": "87a1116b-4c66-4d5a-b0d2-b1ae83d565fa",
      "target": "d9cf8710-f871-4eed-a6c1-1feeef9fe9d9",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.037037037037037035,
        "overlapped_items": [
          [
            "RAGAS",
            "RAGAs"
          ],
          [
            "ChatGPT",
            "ChatGPT"
          ]
        ]
      }
    },
    {
      "id": "5ab879a9-3e09-4693-b7f2-d10512dc9159",
      "type": "entities_overlap",
      "source": "87a1116b-4c66-4d5a-b0d2-b1ae83d565fa",
      "target": "fad4e3b4-2c00-46b9-98d7-f8635a3e85bf",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.08333333333333333,
        "overlapped_items": [
          [
            "Roberts et al.",
            "RoBERTa"
          ]
        ]
      }
    },
    {
      "id": "d58c97a1-fd27-4857-a400-836014ab26ed",
      "type": "entities_overlap",
      "source": "70bcb98e-d2fd-4486-bc9f-f2e7d9abd05b",
      "target": "22e7ed28-ddea-40b5-96cb-ea74494fd5e6",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.018518518518518517,
        "overlapped_items": [
          [
            "RAG",
            "RAGAS"
          ]
        ]
      }
    },
    {
      "id": "b82bd15a-8cf8-4847-a83f-3c78a090993c",
      "type": "entities_overlap",
      "source": "70bcb98e-d2fd-4486-bc9f-f2e7d9abd05b",
      "target": "d9cf8710-f871-4eed-a6c1-1feeef9fe9d9",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.012345679012345678,
        "overlapped_items": [
          [
            "RAG",
            "RAGAs"
          ]
        ]
      }
    },
    {
      "id": "b9c61054-6b2f-4cec-b962-d908a3fc6bb0",
      "type": "entities_overlap",
      "source": "70bcb98e-d2fd-4486-bc9f-f2e7d9abd05b",
      "target": "ca0fcc83-1037-4164-910d-f8c31a3baa39",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.010101010101010102,
        "overlapped_items": [
          [
            "Zhang et al.",
            "Zhao et al."
          ],
          [
            "Zhao et al.",
            "Cao et al."
          ],
          [
            "Zhao et al.",
            "Zhao et al."
          ]
        ]
      }
    },
    {
      "id": "1e12da6c-52f4-4d48-996b-16682386c042",
      "type": "entities_overlap",
      "source": "22e7ed28-ddea-40b5-96cb-ea74494fd5e6",
      "target": "d9cf8710-f871-4eed-a6c1-1feeef9fe9d9",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.018518518518518517,
        "overlapped_items": [
          [
            "RAGAS",
            "RAGAs"
          ]
        ]
      }
    },
    {
      "id": "e6306bf2-2a1c-4b36-b7f7-1113a72e1c66",
      "type": "entities_overlap",
      "source": "fe90b907-7f9a-4723-8885-2f93e69d00cb",
      "target": "d9cf8710-f871-4eed-a6c1-1feeef9fe9d9",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.07407407407407407,
        "overlapped_items": [
          [
            "WikiEval Dataset",
            "WikiEval dataset"
          ],
          [
            "ChatGPT",
            "ChatGPT"
          ]
        ]
      }
    },
    {
      "id": "04f552aa-b0aa-413a-a597-da145f0ae732",
      "type": "entities_overlap",
      "source": "578608c9-b4ef-4be7-9337-0090787a2a45",
      "target": "2c0f8d8a-91f4-4a56-a846-05b446f14b26",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.03333333333333333,
        "overlapped_items": [
          [
            "Spectrograms",
            "Spectrograms"
          ]
        ]
      }
    },
    {
      "id": "5a2c4bcd-ee6c-4359-87b6-9c1f8cebc45d",
      "type": "entities_overlap",
      "source": "04e98094-99b0-4e5b-9714-3d1476139982",
      "target": "e508186a-68d0-4e14-bf4d-82b9eb120d8c",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.03333333333333333,
        "overlapped_items": [
          [
            "formants",
            "formants"
          ],
          [
            "ResNet-101",
            "ResNet-101"
          ]
        ]
      }
    },
    {
      "id": "a392171a-fa51-42a8-919c-c074ed1056d2",
      "type": "entities_overlap",
      "source": "64b09631-504d-4ad9-8d10-09108a97e430",
      "target": "e508186a-68d0-4e14-bf4d-82b9eb120d8c",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.05555555555555555,
        "overlapped_items": [
          [
            "Vowel Classification",
            "vowel classification"
          ]
        ]
      }
    },
    {
      "id": "38652735-a235-4e53-92dc-200a4637902b",
      "type": "entities_overlap",
      "source": "ca0fcc83-1037-4164-910d-f8c31a3baa39",
      "target": "8eb0a20c-6d58-4498-93ad-5e5597064d9a",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.030303030303030304,
        "overlapped_items": [
          [
            "Ghavami and Peplau",
            "Ghavami and Peplau"
          ],
          [
            "StereoSet",
            "StereoSet"
          ]
        ]
      }
    },
    {
      "id": "d460220f-f317-4fe5-a572-a5db1bac4209",
      "type": "entities_overlap",
      "source": "5dec71c5-b1e0-49da-89f9-c856c08b2c22",
      "target": "42dbc040-c060-4458-bb31-2d2589fed4ab",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.0125,
        "overlapped_items": [
          [
            "Llama 3 70B Instruct",
            "Llama 3 70B"
          ]
        ]
      }
    },
    {
      "id": "88b785f6-de6b-426c-8290-cc0a80b9a97b",
      "type": "entities_overlap",
      "source": "42dbc040-c060-4458-bb31-2d2589fed4ab",
      "target": "01fc2945-7d09-4cc6-99ab-dff4a973e20b",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.03333333333333333,
        "overlapped_items": [
          [
            "Japanese",
            "Japanese Female"
          ]
        ]
      }
    },
    {
      "id": "6c5ab6ca-366e-457f-b134-043ed76c85d0",
      "type": "entities_overlap",
      "source": "42dbc040-c060-4458-bb31-2d2589fed4ab",
      "target": "47eae656-4990-4f87-9ba1-6261b7975106",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.06666666666666667,
        "overlapped_items": [
          [
            "Japanese",
            "Japanese Female"
          ],
          [
            "Japanese",
            "Japanese Male"
          ]
        ]
      }
    },
    {
      "id": "89cd0f8b-0f4b-44a3-bf12-b3504e96e26d",
      "type": "entities_overlap",
      "source": "01fc2945-7d09-4cc6-99ab-dff4a973e20b",
      "target": "47eae656-4990-4f87-9ba1-6261b7975106",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.2222222222222222,
        "overlapped_items": [
          [
            "Japanese Female",
            "Japanese Female"
          ],
          [
            "Japanese Female",
            "Japanese Male"
          ]
        ]
      }
    }
  ]
}